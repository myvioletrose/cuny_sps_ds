---
title: "data_608_final_project"
author: "Jimmy Ng"
date: "5/13/2020"
output: html_document
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(warning = FALSE)
knitr::opts_chunk$set(error = FALSE)
knitr::opts_chunk$set(message = FALSE)
```

# OVERVIEW

Fears of the coronavirus crashed the stock market back in February, precisely beginning on February 24, 2020. The pandemic sent a shockwave to the global market and it still continuously wreaked havoc to humanity. The fears spread quickly and globally, e.g. over 70% of the world population was under lockdown at some point in March. Recently in May, the US unemployment rate has climbed to historical high since the Great Depression, and the whole country is still not ready for reopen (as of this moment the report is generated on `r Sys.time()`). Nevertheless, the US stock market has remarkably bounced back since late March, early April and continuously rising in May. The inspiration of this final project is come from the decoupling of the stock market and reality. Can we predict and visualize any upward or downward trend of any stock during this significant crisis in our time? 

## (I) SET UP

We first load a list of packages in the session using `pacman::p_load`. We need to register for an API from <https://www.alphavantage.co/> in order to get real time data using the **quantmod** package. It's free to use this service as long as there's not more than 500 calls a day.

```{r set_up, collapse = TRUE}
# load packages
packages <- c('broom', 'caret', 'doParallel', 'e1071', 'foreach', 'glue', 'gridExtra', 'Hmisc', 'htmlwidgets', 'InformationValue', 'kableExtra', 'parallel', 'plotly', 'quantmod', 'sqldf', 'tictoc', 'tidyverse', 'TTR', 'wrapr')
pacman::p_load(char = packages)

# set environment
readRenviron("~/I/config/.env")

# set project home directory
PROJECT_HOME_DIRECTORY <- Sys.getenv("PROJECT_HOME_DIRECTORY")
setwd(PROJECT_HOME_DIRECTORY)

# get real time data if necessary
REAL_TIME = TRUE
ALPHA_VANTAGE_API <- Sys.getenv("ALPHA_VANTAGE_API")
```

## (II) GET DATA

For illustrative purpose, we pick only one stock to look at in this final project (but the script is built to handle multiple stocks at the same time). We decide to choose a very popular stock to look at, i.e. Apple Inc (**AAPL**). When downloading historical data for multiple stocks, it's wise to use multicores on one's machine and create a parallel processing to speed up the data retrieval processing. 

```{r get_data, collapse = TRUE}
# stock symbol
stock <- c("AAPL")

# detect, use multicores
numCores <- parallel::detectCores()

# create a simple cluster on the local machine using all available threads
cl <- parallel::makeCluster(detectCores(), methods = FALSE)

# register our cluster
doParallel::registerDoParallel(cl)

# loop through a list of stock tickers - super fast! 5x faster than the traditional for-loop approach!!
symbols <- foreach::foreach(i = 1:length(stock), .errorhandling = 'remove') %dopar% { quantmod::getSymbols(stock[i]) } %>% unlist  # change .errorhandling = 'pass' to see error

# return a single list of xts objects from the valid symbols
if(REAL_TIME){
        
        xtsList <- vector(mode = "list", length = length(symbols))
        
        for(i in 1:length(symbols)){
                
                xtsList[[i]] <- quantmod::getSymbols(
                        
                        symbols[i], 
                        env = NULL,  # set env = NULL and that is equivalent to auto.assign = FALSE
                        src = "av",
                        periodicity = "daily", 
                        output.size = "full", 
                        adjusted = TRUE,
                        api.key = ALPHA_VANTAGE_API
                        
                )
                
                Sys.sleep(12)
                
        }
        
} else {
        
        xtsList <- foreach::foreach(i = 1:length(symbols)) %dopar% { quantmod::getSymbols(symbols[i], env = NULL, adjusted = TRUE) }  # set env = NULL and that is equivalent to auto.assign = FALSE        
        
}

# set names for xtsList
names(xtsList) <- symbols
```

## (III) BASIC TECHNICAL ANALYSIS

Before diving into our prediction model, let's take a quick look of the time series data from "01-02-2019" to "05-13-2020". There are two obvious observations from the chart, i.e. 1) there's a steady upward trend presented in the data, and 2) there's a significant and rapid down turn (caused by the pandemic) starting on "02-24-2020" until a bounce-back on "03-23-2020". The chart includes multiple technical indicators, e.g. **Bollinger Bands, Width and %B Indicator**, **Average Directional Movement Index (ADX)**, **Moving Average Convergence Divergence (MACD)**, **Relative Strength Index (RSI)**, **Commodity Channel Index (CCI)**, **Chaikin Money Flow (CMF)**, and **Trading Volume**.

```{r technical_analysis, echo = FALSE, fig.width = 10, fig.height = 8}
x <- xts::xts(xtsList[[stock]])

quantmod::chartSeries( x["201901::202005"],
                       name = stock,
                       TA = c(addBBands(draw = 'bands'), 
                              addADX(), 
                              addMACD(), 
                              addRSI(),
                              addCCI(), 
                              addCMF(), 
                              addBBands(draw = 'width'), 
                              addBBands(draw = 'percent'), 
                              addVo()) )
```

## (IV) DATA TRANSFORMATION, BUILD MODELS

There are four simple concepts that we need to first go over, i.e. i) **nday** (or **tday** as they are interchangable throughout the script), ii) **lday**, iii) **trend**, and iv) **close_percentile_threshold**.

**nday** is referred to the number of future trading day(s). For example, we want to use today (t) closing price to predict tomorrow (t plus 1) closing price, and we want to know whether the difference **(closing tomorrow - closing today) / closing today** would signal an upward or downward trend. Therefore, we actually look at two **trends**, and we simply refer them as "bullish" and "bearish". 

How should we set up a threshold for triggering a "bullish" or "bearish" signal? We look at the change in percentage between today and future values. Say, we want to make a prediction for tomorrow (or the next trading window), we first calculate the ratio of **(closing tomorrow - closing today) / closing today**, and then we find the distribution and look up the percentile (see below density curves for better illustration). Anything that is roughly above 1 SD is considered to be an upward trend ("bullish") signal, whereas anything that is roughly below 1 SD ("bearish") is considered to be a downward trend signal. We decide to choose roughly "plus or minus one SD" as our **close_percentile_threshold**, i.e. above 85th percentile / below 15th perentile. Thus our target of prediction is always a moving target based on market volatility and the gap between current and future trading windows. We decide to go with a very liberal approach (roughly plus or minus 1 SD) because we do not want to miss any signal. We want to avoid any type II error.

In addition, we need to transform our data by getting values from the past to see if that would predict the future. We use **"5 ndays"** and **"5 ldays"** in this final project. The 5 ndays are 1, 2, 3, 4, and 5, whereas the ldays are 1, 5, 10, 15, and 20. What that means is that for each n day, we use 5 ldays and each lday would make an individual prediction for the nday. Subsequently, we would create a consensus out of the 5 models. For example, let's say we use trading volume to predict closing price. We would transform our data and get the trading volume of today minus 1 (previous trading day), today minus 5 (5 days ago), 10 days ago, 15 days ago, and 20 days ago. Each set of data would make a prediction of today plus 1 (next trading day), plus 2, 3, and so forth. We need to find a consensus out of these five models and that's our signal for **trend**. 

We write a custom function to transform and return values plus multiple indicators (such as RSI, MACD) from the time series object that we pull from quantmod. Please see the appendix for detail. 

After data transformation, we build separate classification models to identify whether future trading day will close at higher or lower end than the present day. Our **target** is actually a threshold, i.e. whether it is above or below 1 SD and we use it as our predicted or dependent variable ("Yes" or "No"). 

We build our classification/prediction model using different algorithms, i.e. glm (generalized linear model), nb (naive bayes) and gbm (gradient boosting machine). We build a base layer (using glm, nb and gbm) to predict the **target** from the train set. Second, we build a top layer (again using glm, nb and gbm) to predict the **target** based on the outcomes (OOF or Out-Of-Fold prediction) of our base layer. In other words, we make prediction (of the target variable) based on the predictions of our base models. Finally, we simply average the outcome probabilities of our top layer to get our final probability and decision using 0.5 as cut-off.

Instead of using entire historical record, let's just focus on the past 1000 trading days. We split the data by 80/20 for our train and test sets respectively. Finally, we use the past 90 trading days as our evaluation set for accuracy. 

Our focus is to see whether our data transformation (using **ldays** 1, 5, 10, 15, 20 to predict for **ndays** 1, 2, 3, 4, 5) and ensembling different algorithms would return any signal about stock price movement.

```{r data_transformation, echo = FALSE}
# custom function
historicalTransformation <- function(xts,        
                                     l = 1,
                                     n = 1,
                                     originalSymbol = FALSE,
                                     transformOnly = FALSE
){
        
        # transform() function
        transformation <- function(xts, l, n, originalSymbol){
                
                # symbol
                symbol <- stringr::str_extract_all(names(xts), pattern = "^[[:alpha:]].*\\.") %>% unlist %>% stringr::str_to_lower(.) %>% unique %>% gsub("\\.", "", .)
                
                # rename columns
                names(xts) <- gsub("^[[:alpha:]].*\\.", "", names(xts)) %>% stringr::str_to_lower(.)
                
                # turn it into data.frame
                xts <- as.data.frame(xts)
                
                # insert columns
                xts <- xts %>%
                        dplyr::mutate(
                                # date
                                date = row.names(xts) %>% lubridate::ymd(.),
                                year = lubridate::year(date),
                                quarter = lubridate::quarter(date),
                                month = lubridate::month(date),
                                weekday = base::weekdays(date),
                                day = lubridate::day(date),
                                # Lag
                                op_l = quantmod::Lag(xts[, "open"], l),
                                hi_l = quantmod::Lag(xts[, "high"], l),
                                lo_l = quantmod::Lag(xts[, "low"], l),
                                cl_l = quantmod::Lag(xts[, "close"], l),
                                vol_l = quantmod::Lag(xts[, "volume"], l),
                                adj_l = quantmod::Lag(xts[, "adjusted"], l),
                                # Next
                                op_n = lead(xts[, "open"], n),
                                hi_n = lead(xts[, "high"], n),
                                lo_n = lead(xts[, "low"], n),
                                cl_n = lead(xts[, "close"], n),
                                vol_n = lead(xts[, "volume"], n),
                                adj_n = lead(xts[, "adjusted"], n),
                                # daily "Close - Open", "Hi - Low"
                                cl_op_diff = close - open,
                                hi_lo_diff = high - low,
                                # Lag "diff"
                                op_l_diff = open - op_l,
                                hi_l_diff = high - hi_l,
                                lo_l_diff = low - lo_l,
                                cl_l_diff = close - cl_l,
                                vol_l_diff = volume - vol_l,
                                adj_l_diff = adjusted - adj_l,
                                # Next "diff"
                                op_n_diff = op_n - open,
                                hi_n_diff = hi_n - high,
                                lo_n_diff = lo_n - low,
                                cl_n_diff = cl_n - close,
                                vol_n_diff = vol_n - volume,
                                adj_n_diff = adj_n - adjusted,
                                # oscillator - RSI, CCI                                
                                rsi_close = TTR::RSI(xts[, "close"]),
                                cci_close = TTR::CCI(xts[, "close"])
                        )
                
                # oscillator - MACD
                macd <- MACD(xts$close) %>% as.data.frame %>% dplyr::select(macd)
                signal <- MACD(xts$close) %>% as.data.frame %>% dplyr::select(signal)
                
                # add MACD
                xts <- xts %>%
                        dplyr::mutate(macd_close = macd$macd,
                                      signal_close = signal$signal,
                                      macd_close_diff = macd_close - signal_close) %>%
                        dplyr::select(date, year, quarter, month, weekday, day,
                                      open, high, low, close, volume, adjusted,
                                      rsi_close, cci_close, macd_close, signal_close, macd_close_diff,
                                      everything()) %>%
                        dplyr::arrange(date)
                
                # rename columns back with original symbol
                if(originalSymbol){names(xts) <- paste(symbol, names(xts), sep = "_")}
                
                # return xts
                return(xts)
                
        }
        
        # return a df object
        if(transformOnly){
                
                # return a transformed xts object
                transformObj <- transformation(xts, l, n, originalSymbol)
                return(transformObj)
                
        } else {
                
                # return the min and max of different variables associated with dates based on a transformed xts object
                SYMBOL <- transformation(xts, l, n, originalSymbol)
                
                minVector <- SYMBOL[complete.cases(SYMBOL), 6:ncol(SYMBOL)] %>% lapply(., min) %>% unlist
                maxVector <- SYMBOL[complete.cases(SYMBOL), 6:ncol(SYMBOL)] %>% lapply(., max) %>% unlist
                
                minList <- vector(mode = "list", length = length(minVector))
                for(i in 1:length(minList)){
                        minList[[i]] <- sqldf(sprintf("select date, %s from SYMBOL where floor(%s) = floor(%f)", names(minVector)[i], names(minVector)[i], minVector[i]))
                }
                
                maxList <- vector(mode = "list", length = length(maxVector))
                for(i in 1:length(maxList)){
                        maxList[[i]] <- sqldf(sprintf("select date, %s from SYMBOL where ceil(%s) = ceil(%f)", names(maxVector)[i], names(maxVector)[i], maxVector[i]))
                }
                
                minDf <- minList %>%
                        bind_rows() %>%
                        tidyr::gather(., key, value, -date) %>%
                        dplyr::filter(!is.na(value)) %>%
                        dplyr::mutate(type = "min") %>%
                        arrange(key, date)
                
                maxDf <- maxList %>%
                        bind_rows() %>%
                        tidyr::gather(., key, value, -date) %>%
                        dplyr::filter(!is.na(value)) %>%
                        dplyr::mutate(type = "max") %>%
                        arrange(key, date)
                
                historyObj <- bind_rows(minDf, maxDf) %>%
                        dplyr::select(., type, everything()) %>%
                        arrange(type, key, date)
                
                return(historyObj)
        }
        
}
```

```{r set_parameters, echo = FALSE}
# trend
volatility = c("bullish", "bearish")

# target percentile threshold, e.g. top X% and bottom X% price change (cl_n_diff / close)
cl_percentile_threshold = 0.15

# xts subset
xts_data_subset = TRUE

# xts subset partition, i.e. number of most recent trading days
xts_data_subset_partition = 1000

# lag day(s)
l_period = c(1, 5, 10, 15, 20)

# next day(s)
n_period = c(1, 2, 3, 4, 5)

# baseTrainMethods
baseTrainMethods <- c("glm", "nb", "gbm")

# topTrainMethods
topTrainMethods <- c("glm", "nb", "gbm")

# tune length
tune.length = 3

# seed 
seed = 1234

# split data into train and test set by XX
size = 0.8

# Dependent Variable
DV <- "target"

# Features Selection Threshold
features_selection_threshold = .01

# dfEval, i.e. number of trading days for evaluation
dfEval_subset_partition = 90

# output
modelSummaryList <- vector(mode = "list")

evalList <- vector(mode = "list")
```

```{r density_curve, echo = FALSE, fig.width = 10, fig.height = 8}
# density curve
tempObj <- vector(mode = "list", length = length(n_period))

for(i in n_period){
        
        tempDf <- xtsList[[stock]] %>% 
                historicalTransformation(transformOnly = TRUE, n = i) %>%
                dplyr::mutate(percent_change = (cl_n_diff / close),
                              t_period = i,
                              tday = dplyr::case_when(t_period == 1 ~ "t plus 1 day",
                                                      t_period == 2 ~ "t plus 2 days",
                                                      t_period == 3 ~ "t plus 3 days",
                                                      t_period == 4 ~ "t plus 4 days",
                                                      t_period == 5 ~ "t plus 5 days"),
                              index = nrow(.):1) %>%
                dplyr::filter(index <= xts_data_subset_partition) %>%
                dplyr::select(date, close, cl_n_diff, percent_change, tday)
        
        tempList <- list(tempDf)
        tempObj <- c(tempObj, tempList)
        
}

tempDf <- tempObj %>% dplyr::bind_rows()

# gather data frame
dfGather <- tempDf %>%
        dplyr::select(-date) %>%
        dplyr::mutate(tday = as.factor(tday)) %>%
        tidyr::gather(key, value, -tday) 

# plot close ($) by tday
densityCurveClose <- ggplot(dfGather %>% dplyr::filter(key == "close") %>% distinct, 
                            aes(value)) +
        geom_density() +
        geom_vline(data = aggregate(value ~ tday, 
                                    dfGather %>% dplyr::filter(key == "close") %>% distinct, 
                                    median), 
                   aes(xintercept = value),
                   linetype = "dashed",
                   col = "red") +
        theme(legend.position = "none") +
        labs(x = "Close ($)") +
        theme_light() +
        scale_x_continuous(labels = scales::dollar) +
        scale_y_continuous(labels = scales::percent) +
        ggtitle(paste0(stock, " distribution of Close ($) for past ", xts_data_subset_partition, " trading days"))

# plot cl_n_diff
densityCurveCl_N_diff <- ggplot(dfGather %>% dplyr::filter(key == "cl_n_diff"), 
                                aes(value, fill = tday)) +
        geom_density(alpha = 0.5) +
        # median
        geom_vline(data = aggregate(value ~ tday, 
                                    dfGather %>% dplyr::filter(key == "cl_n_diff"), 
                                    median), 
                   aes(xintercept = value,
                       color = tday),
                   linetype = "dashed") +
        # cl_percentile_threshold
        geom_vline(data = aggregate(value ~ tday, 
                                    dfGather %>% dplyr::filter(key == "cl_n_diff"),
                                    quantile,
                                    probs = cl_percentile_threshold), 
                   aes(xintercept = value,
                       color = tday),
                   linetype = "dotted") +
        # 1 - cl_percentile_threshold
        geom_vline(data = aggregate(value ~ tday, 
                                    dfGather %>% dplyr::filter(key == "cl_n_diff"),
                                    quantile,
                                    probs = 1 -cl_percentile_threshold), 
                   aes(xintercept = value,
                       color = tday),
                   linetype = "dotted") +
        theme_minimal() +
        scale_x_continuous(labels = scales::dollar) +
        scale_y_continuous(labels = scales::percent) +
        theme(legend.position = "right") +
        labs(x = "Close ($) between t and t plus n day(s)") +
        ggtitle(paste0(stock, " difference in $ between today (t) and future (t plus n trading day) Close ($) \nfor past ", xts_data_subset_partition, " trading days")) +
        facet_wrap(~tday, nrow = 5)

# plot percent_change
densityCurveCl_percent_change <- ggplot(dfGather %>% dplyr::filter(key == "percent_change"), 
                                        aes(value, fill = tday)) +
        geom_density(alpha = 0.5) +
        # median
        geom_vline(data = aggregate(value ~ tday, 
                                    dfGather %>% dplyr::filter(key == "percent_change"), 
                                    median), 
                   aes(xintercept = value,
                       color = tday),
                   linetype = "dashed") +
        # cl_percentile_threshold
        geom_vline(data = aggregate(value ~ tday, 
                                    dfGather %>% dplyr::filter(key == "percent_change"),
                                    quantile,
                                    probs = cl_percentile_threshold), 
                   aes(xintercept = value,
                       color = tday),
                   linetype = "dotted") +
        # 1 - cl_percentile_threshold
        geom_vline(data = aggregate(value ~ tday, 
                                    dfGather %>% dplyr::filter(key == "percent_change"),
                                    quantile,
                                    probs = 1 -cl_percentile_threshold), 
                   aes(xintercept = value,
                       color = tday),
                   linetype = "dotted") +
        theme_bw() +
        scale_x_continuous(labels = scales::percent) +
        scale_y_continuous(labels = scales::percent) +
        theme(legend.position = "right") +
        labs(x = "% diff between t and t plus n day(s)") +
        ggtitle(paste0(stock, " change in % between today (t) and future (t plus n trading day) Close ($) \nfor past ", xts_data_subset_partition, " trading days")) +
        facet_wrap(~tday, nrow = 5)

# plots
densityCurveClose %>% ggplotly()
densityCurveCl_N_diff %>% ggplotly()
densityCurveCl_percent_change %>% ggplotly()
```

```{r build_models, echo = FALSE, collapse = TRUE, results = "hide"}
# start timer
tic()

# start nested for() loop        
for(b in volatility){
        
        marketTrend = b
        
        for(l in l_period){
                
                lday = l
                
                for(i in n_period){
                        
                        nday = i
                        
                        for(j in stock){
                                
                                # get data
                                x <- xtsList[[j]]
                                
                                # get target threshold
                                if(xts_data_subset){
                                        y.t <- historicalTransformation(x, l = lday, n = nday, transformOnly = TRUE) %>%
                                                dplyr::mutate(percent_diff = cl_n_diff / close,
                                                              partition = (nrow(.)-1):0) %>%
                                                dplyr::filter(partition <= xts_data_subset_partition)
                                } else {
                                        y.t <- historicalTransformation(x, l = lday, n = nday, transformOnly = TRUE) %>%
                                                dplyr::mutate(percent_diff = cl_n_diff / close)
                                }
                                y <- y.t$percent_diff
                                y <- y[!is.na(y)]
                                z <- quantile(y, c(cl_percentile_threshold, 1 - cl_percentile_threshold))
                                
                                # features
                                features <- c(                                
                                        'rsi_close', 
                                        'cci_close',
                                        'macd_close', 
                                        'signal_close',
                                        'vol_n_diff',
                                        'op_l_percent_diff', 
                                        'hi_l_percent_diff', 
                                        'lo_l_percent_diff', 
                                        'cl_l_percent_diff', 
                                        'vol_l_percent_diff',    
                                        'cl_op_diff_by_op',
                                        'hi_lo_diff_by_op',
                                        'cl_op_diff_by_cl_l',
                                        'hi_lo_diff_by_cl_l',
                                        'cl_cl_l_ratio',
                                        'op_cl_l_ratio',
                                        'vol_cl_l_ratio',
                                        'vol_op_l_ratio',  
                                        'vol_diff_ratio',
                                        'sma_5',                        
                                        'sma_10',
                                        'sma_15'
                                )
                                
                                # transform, get DV, set partition
                                x.t <- historicalTransformation(x, l = lday, n = nday, transformOnly = TRUE) %>%
                                        dplyr::mutate(
                                                trend = marketTrend,
                                                symbol = j,                        
                                                l_period = lday,
                                                n_period = nday,
                                                op_l_percent_diff = ((open - op_l) / op_l) %>% as.vector,
                                                hi_l_percent_diff = ((high - hi_l) / hi_l) %>% as.vector,
                                                lo_l_percent_diff = ((low - lo_l) / lo_l) %>% as.vector,
                                                cl_l_percent_diff = ((close - cl_l) / cl_l) %>% as.vector,
                                                vol_l_percent_diff = ((volume - vol_l) / vol_l) %>% as.vector,
                                                cl_op_diff_by_op = ((close - open) / open) %>% as.vector,
                                                hi_lo_diff_by_op = ((high - low) / open) %>% as.vector,
                                                cl_op_diff_by_cl_l = ((close - open) / cl_l) %>% as.vector,
                                                hi_lo_diff_by_cl_l = ((high - low) / cl_l) %>% as.vector,
                                                cl_cl_l_ratio = (close / cl_l) %>% as.vector,
                                                op_cl_l_ratio = (open / cl_l) %>% as.vector, 
                                                vol_cl_l_ratio = (vol_l_percent_diff / cl_cl_l_ratio) %>% as.vector, 
                                                vol_op_l_ratio = (vol_l_percent_diff / op_cl_l_ratio) %>% as.vector,  
                                                vol_diff_ratio = ((vol_n_diff / vol_l_diff) %>% as.vector) %>% as.vector, 
                                                sma_5 = TTR::SMA(close, 5),                                
                                                sma_10 = TTR::SMA(close, 10),
                                                sma_15 = TTR::SMA(close, 15),
                                                target = dplyr::case_when(trend == "bullish" ~ ifelse( (cl_n_diff / close) > z[2], "Y", "N" ),
                                                                          trend == "bearish" ~ ifelse( (cl_n_diff / close) < z[1], "Y", "N" )) %>%
                                                        factor(., levels = c("Y", "N"), labels = c("Yes", "No")),            
                                                partition = (nrow(.)-1):0) %>%  # 0 is referred to the most recent (or last) trading day
                                        dplyr::select(trend, symbol, l_period, n_period, partition, date, year, quarter, month, weekday, day, everything()) %>%
                                        arrange(desc(partition))                
                                
                                # whether to subset and use only the most recent trading days or entire historical record
                                if(xts_data_subset){x.t <- x.t %>% dplyr::filter(partition <= xts_data_subset_partition)}
                                
                                # scale features
                                x.t.scaled <- x.t %>% dplyr::mutate_at(vars(features), scale)
                                
                                # remove any nan column
                                x.t.scaled <- x.t.scaled[, which(unlist(lapply(x.t.scaled, function(x) !any(is.nan(x)))))]
                                
                                # complete.cases
                                x.t.scaled <- x.t.scaled %>% complete.cases() %.>% x.t.scaled[., ]
                                
                                # new set of features
                                features <- features[features %in% names(x.t.scaled)]
                                
                                # feature selections by t.test        
                                t.test.list <- lapply(x.t.scaled[, features], function(x) t.test(x ~ x.t.scaled$target) %>% broom::tidy())
                                t.test.df <- t.test.list %>%
                                        dplyr::bind_rows() %>%
                                        dplyr::mutate(variables = features) %>%
                                        arrange(p.value)
                                
                                IV <- t.test.df$variables[t.test.df$p.value < features_selection_threshold]
                                if(length(IV) <=2){IV = unique(c(IV, 'rsi_close', 'cci_close', 'macd_close', 'signal_close'))}
                                
                                # subset features
                                x.t.scaled2 <- x.t.scaled %>% 
                                        dplyr::select(partition, date, all_of(IV), DV) %>%
                                        dplyr::filter(partition > dfEval_subset_partition)
                                
                                # evaluation set (include all variables and TODAY values)
                                dfEval <- x.t %>% 
                                        dplyr::filter(partition <= dfEval_subset_partition) %>%
                                        dplyr::mutate_at(vars(features), scale) %>%
                                        dplyr::select(partition, date, all_of(IV), DV) %>%                
                                        dplyr::mutate_at(IV, as.vector)
                                
                                # impute dataset
                                preProcValues <- caret::preProcess(dfEval[, IV], method = c("medianImpute"))
                                dfEval <- predict(preProcValues, dfEval) %>%
                                        dplyr::mutate_at(IV, as.vector)
                                
                                # oversampling target == "Yes" by 50%
                                x.t.scaled2_Yes <- x.t.scaled2 %>% dplyr::filter(target == "Yes")
                                x.t.scaled2_IndexUp <- sample(1:nrow(x.t.scaled2_Yes), size = floor(nrow(x.t.scaled2_Yes) * 1.5), replace = TRUE)
                                x.t.scaled2_Yes <- x.t.scaled2_Yes[x.t.scaled2_IndexUp, ]
                                
                                # undersampling target == "No" by 50%
                                x.t.scaled2_No <- x.t.scaled2 %>% dplyr::filter(target == "No")
                                x.t.scaled2_IndexDown <- sample(1:nrow(x.t.scaled2_No), size = floor(nrow(x.t.scaled2_No) * 0.5), replace = FALSE)
                                x.t.scaled2_No <- x.t.scaled2_No[x.t.scaled2_IndexDown, ]
                                
                                # put them together to come up with new x.t.scaled
                                x.t.scaled2 <- dplyr::bind_rows(x.t.scaled2_Yes, x.t.scaled2_No)
                                
                                # split data into train, test sets
                                set.seed(seed + nday + lday)
                                index <- caret::createDataPartition(x.t.scaled2$target, p = size, list = FALSE)
                                trainSet <- x.t.scaled2[index, ]
                                testSet <- x.t.scaled2[-index, ] %>% distinct 
                                
                                # trainSetControl
                                trainSet.control <- caret::trainControl(method = "cv", number = 10, savePredictions = "final", classProbs = TRUE)
                                
                                # train base layer
                                for(baseLayer in baseTrainMethods){
                                        
                                        # set parameters
                                        ml = baseLayer
                                        model = paste0("model_", ml)
                                        OOF_prediction = paste0("OOF_pred_", ml)
                                        prediction = paste0("pred_", ml)
                                        result = paste0("result_", ml)
                                        
                                        # model
                                        assign(bquote(.(model)), caret::train(trainSet[, IV], trainSet[, DV], 
                                                                              method = ml, 
                                                                              trControl = trainSet.control, 
                                                                              tuneLength = tune.length))
                                        
                                        # Out-Of-Fold probability predictions - trainSet    
                                        if(ml == "glm"){trainSet$OOF_pred_glm = eval(sym(model))$pred$Y[order(eval(sym(model))$pred$rowIndex)]}
                                        if(ml == "nb"){trainSet$OOF_pred_nb = eval(sym(model))$pred$Y[order(eval(sym(model))$pred$rowIndex)]}
                                        if(ml == "gbm"){trainSet$OOF_pred_gbm = eval(sym(model))$pred$Y[order(eval(sym(model))$pred$rowIndex)]}
                                        
                                        # Out-Of-Fold probability predictions - testSet 
                                        assign(bquote(.(OOF_prediction)), predict(eval(sym(model)), testSet[, IV], type = "prob")$Y)
                                        if(ml == "glm"){testSet$OOF_pred_glm = eval(sym(OOF_prediction))}
                                        if(ml == "nb"){testSet$OOF_pred_nb = eval(sym(OOF_prediction))}
                                        if(ml == "gbm"){testSet$OOF_pred_gbm = eval(sym(OOF_prediction))}
                                        
                                        # Out-Of-Fold probability predictions - dfEval 
                                        assign(bquote(.(OOF_prediction)), predict(eval(sym(model)), dfEval[, IV], type = "prob")$Y)
                                        if(ml == "glm"){dfEval$OOF_pred_glm = eval(sym(OOF_prediction))}
                                        if(ml == "nb"){dfEval$OOF_pred_nb = eval(sym(OOF_prediction))}
                                        if(ml == "gbm"){dfEval$OOF_pred_gbm = eval(sym(OOF_prediction))}
                                        
                                        # Y/N predictions for Confusion Matrix - testSet    
                                        assign(bquote(.(prediction)), predict(eval(sym(model)), testSet[, IV]))
                                        if(ml == "glm"){testSet$pred_glm = eval(sym(prediction))}
                                        if(ml == "nb"){testSet$pred_nb = eval(sym(prediction))}
                                        if(ml == "gbm"){testSet$pred_gbm = eval(sym(prediction))}
                                        
                                        # output
                                        assign(bquote(.(result)), broom::tidy(caret::confusionMatrix(testSet[, prediction], testSet$target)) %>%
                                                       dplyr::mutate(trend = marketTrend, symbol = j, n_period = nday, l_period = lday, trainMethod = ml) %>%
                                                       dplyr::select(trend, symbol, n_period, l_period, trainMethod, everything()))
                                        
                                        # store output into a list
                                        tempModelList <- list(eval(sym(result)))
                                        modelSummaryList <<- c(modelSummaryList, tempModelList)
                                        
                                }
                                
                                # train top layer
                                for(topLayer in topTrainMethods){
                                        
                                        # set parameters
                                        ml = topLayer
                                        model = paste0("model_", ml)        
                                        OOF_predictors_top = c("OOF_pred_glm", "OOF_pred_nb", "OOF_pred_gbm")
                                        OOF_prediction_top = paste0("OOF_pred_top_", ml)
                                        prediction_top = paste0("pred_top_", ml)
                                        result = paste0("result_top_", ml)
                                        
                                        # model
                                        assign(bquote(.(model)), caret::train(trainSet[, OOF_predictors_top], trainSet[, DV], 
                                                                              method = ml, 
                                                                              trControl = trainSet.control, 
                                                                              tuneLength = tune.length))
                                        
                                        # Out-Of-Fold probability predictions - testSet 
                                        assign(bquote(.(OOF_prediction_top)), predict(eval(sym(model)), testSet[, OOF_predictors_top], type = "prob")$Y)
                                        if(ml == "glm"){testSet$OOF_pred_top_glm = eval(sym(OOF_prediction_top))}
                                        if(ml == "nb"){testSet$OOF_pred_top_nb = eval(sym(OOF_prediction_top))}
                                        if(ml == "gbm"){testSet$OOF_pred_top_gbm = eval(sym(OOF_prediction_top))}
                                        
                                        # Out-Of-Fold probability predictions - dfEval 
                                        assign(bquote(.(OOF_prediction_top)), predict(eval(sym(model)), dfEval[, OOF_predictors_top], type = "prob")$Y)
                                        if(ml == "glm"){dfEval$OOF_pred_top_glm = eval(sym(OOF_prediction_top))}
                                        if(ml == "nb"){dfEval$OOF_pred_top_nb = eval(sym(OOF_prediction_top))}
                                        if(ml == "gbm"){dfEval$OOF_pred_top_gbm = eval(sym(OOF_prediction_top))}
                                        
                                        # Y/N predictions for Confusion Matrix - testSet    
                                        assign(bquote(.(prediction_top)), predict(eval(sym(model)), testSet[, OOF_predictors_top]))
                                        if(ml == "glm"){testSet$pred_top_glm = eval(sym(prediction_top))}
                                        if(ml == "nb"){testSet$pred_top_nb = eval(sym(prediction_top))}
                                        if(ml == "gbm"){testSet$pred_top_gbm = eval(sym(prediction_top))}
                                        
                                        # output
                                        assign(bquote(.(result)), broom::tidy(caret::confusionMatrix(testSet[, prediction_top], testSet$target)) %>%
                                                       dplyr::mutate(trend = marketTrend, symbol = j, n_period = nday, l_period = lday, trainMethod = paste0(ml, " - top layer")) %>%
                                                       dplyr::select(trend, symbol, n_period, l_period, trainMethod, everything()))
                                        
                                        # store output into a list
                                        tempModelList <- list(eval(sym(result)))
                                        modelSummaryList <<- c(modelSummaryList, tempModelList)
                                        
                                }
                                
                                # put together - final averaging
                                testSet <- testSet %>%
                                        dplyr::mutate(pred_final_avg = (OOF_pred_top_glm + OOF_pred_top_nb + OOF_pred_top_gbm) / length(topTrainMethods),
                                                      pred_final = ifelse(pred_final_avg > 0.5, "Y", "N") %>%
                                                              factor(., levels = c("Y", "N"), labels = c("Yes", "No")))
                                
                                dfEval <- dfEval %>%
                                        dplyr::mutate(pred_final_avg = (OOF_pred_top_glm + OOF_pred_top_nb + OOF_pred_top_gbm) / length(topTrainMethods),
                                                      pred_final = ifelse(pred_final_avg > 0.5, 1, 0),
                                                      trend = marketTrend, symbol = j, n_period = nday, l_period = lday) %>%
                                        dplyr::select(trend, symbol, n_period, l_period, date, target, pred_final_avg, pred_final)
                                
                                finalResult <- broom::tidy(caret::confusionMatrix(testSet$pred_final, testSet$target)) %>%
                                        dplyr::mutate(trend = marketTrend, symbol = j, n_period = nday, l_period = lday, trainMethod = "final - averaging") %>%
                                        dplyr::select(trend, symbol, n_period, l_period, trainMethod, everything())
                                
                                # store output into a list
                                tempModelList <- list(finalResult)
                                modelSummaryList <<- c(modelSummaryList, tempModelList)
                                
                                tempEvalList <- list(dfEval)
                                evalList <<- c(evalList, tempEvalList)
                                
                        }
                        
                }
                
        }
        
}

# stop timer
toc()
```

## (V) RESULTS

Here we show a snippet of the original data, then the same piece of data after transformation with additional features, and subsequently, we scale and impute missing data and split it into train and test sets respectively. 

Down below is a summary of the model output. We care mainly about the model of "final - averaging" and that's the result of our top layer ensembling our models. We apply that specific model to our evaluation set using the most recent 90 trading days (including today closing price on `r Sys.Date()`) to generate prediction. The **target** field represents the actual, whereas the **pred_final** represents our prediction.

### original data
```{r output, echo = FALSE}
# original data sourced from quantmod
x %>%
        tail %>%
        kableExtra::kable() %>%
        kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"), 
                      full_width = FALSE, 
                      position = "left",
                      fixed_thead = TRUE)
```

### after transformation
```{r after_transformation, echo = FALSE}
# after transformation
x.t %>%
        tail %>%
        kableExtra::kable() %>%
        kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"), 
                      full_width = FALSE, 
                      position = "left",
                      fixed_thead = TRUE) %>%
        scroll_box(width = "100%", height = "200px")
```

### train set
```{r train_set, echo = FALSE}
# sample train set
trainSet %>%
        tail %>%
        kableExtra::kable() %>%
        kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"), 
                      full_width = FALSE, 
                      position = "left",
                      fixed_thead = TRUE) %>%
        scroll_box(width = "100%", height = "200px")
```

### evaluation (prediction) output
```{r evaluation_set, echo = FALSE}
# consolidate lists into df
modelSummaryDf <- modelSummaryList %>% dplyr::bind_rows()
evalDf <- evalList %>% dplyr::bind_rows()

# evaluation set
dfEval %>%
        kableExtra::kable() %>%
        kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"), 
                      full_width = FALSE, 
                      position = "left",
                      fixed_thead = TRUE) %>%
        scroll_box(width = "100%", height = "200px")
```

### model summary output
```{r model_summary_output, echo = FALSE}
# data transformation for model evaluation, spread table
modelSummarySpread <- modelSummaryDf %>% 
        dplyr::select(trend, symbol, n_period, l_period, trainMethod, term, estimate) %>%
        tidyr::spread(term, estimate) %>%
        arrange(trend, symbol, n_period, trainMethod)

# get measures
modelSummaryMeasure <- modelSummaryDf %>%
        dplyr::filter(term %in% c("recall", "precision", "f1") & trainMethod == "final - averaging") %>%
        dplyr::select(trend, symbol, n_period, l_period, measure = term, estimate) %>%
        dplyr::mutate(lday = dplyr::case_when(l_period == 1 ~ "lag 1 day",
                                              l_period == 5 ~ "lag 5 days",
                                              l_period == 10 ~ "lag 10 days",
                                              l_period == 15 ~ "lag 15 days",
                                              l_period == 20 ~ "lag 20 days") %>%
                              factor(., levels = c("lag 1 day", "lag 5 days", "lag 10 days", "lag 15 days", "lag 20 days")))

# get accuracy from eval set
accuracyDf <- evalDf %>%
        dplyr::mutate(flag = dplyr::case_when(pred_final == 0 ~ "No", TRUE ~ "Yes"),
                      accuracy = dplyr::case_when(target == flag ~ 1, TRUE ~ 0)) %>%
        dplyr::filter(!is.na(target)) %>%
        group_by(trend, symbol, n_period, l_period) %>%
        summarise(accuracy = mean(accuracy)) %>%
        dplyr::mutate(measure = "accuracy",
                      lday = dplyr::case_when(l_period == 1 ~ "lag 1 day",
                                              l_period == 5 ~ "lag 5 days",
                                              l_period == 10 ~ "lag 10 days",
                                              l_period == 15 ~ "lag 15 days",
                                              l_period == 20 ~ "lag 20 days") %>%
                              factor(., levels = c("lag 1 day", "lag 5 days", "lag 10 days", "lag 15 days", "lag 20 days"))) %>%
        dplyr::select(trend, symbol, n_period, l_period, measure, estimate = accuracy, lday) %>%
        arrange(trend, symbol, n_period, l_period)

# model summary output
modelSummarySpread %>% 
        kableExtra::kable() %>%
        kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"), 
                      full_width = FALSE, 
                      position = "left",
                      fixed_thead = TRUE) %>%
        scroll_box(width = "100%", height = "200px")
```

## (VI) EVALUATION

In this final project, we focus on four key **measures** to evaluate the success of our models, i.e. **accuracy**, **f1 score**, **precision**, and **recall** (aka **sensitivity**). Overall, the model seems to be more accurate in pointint out a downward trend ("bearish").

```{r model_evaluation, echo = FALSE, fig.width = 10, fig.height = 8}
# combine the key measures of model summary together
keyMeasures <- dplyr::bind_rows(modelSummaryMeasure, accuracyDf) %>%
        arrange(trend, symbol, n_period, l_period, measure)

# plot evaluation chart
evalChart <- keyMeasures %>%
        dplyr::filter(symbol == stock) %>%
        ggplot(aes(n_period, estimate, fill = factor(trend))) +
        geom_bar(stat = "identity", position = "dodge2") +
        geom_hline(yintercept = 0.5, linetype = "dashed", col = "black") +
        scale_y_continuous(labels = scales::percent, limits = c(0, 1)) +
        facet_grid(measure ~ lday) +
        labs(x = "Prediction for t plus n trading day(s)", y = "", fill = "trend") +
        theme_light() +
        theme(legend.position = "bottom") +
        ggtitle(paste0(stock, " Model Evaluation ", min(evalDf$date), " : ", max(evalDf$date)))

# evaluation chart
evalChart %>% ggplotly()
```

## (VII) PREDICTION

```{r signal_date, echo = FALSE}
# data transformation for prediction
predDf <- evalDf %>%
        group_by(trend, symbol, n_period, date) %>%
        summarise(prediction = round(mean(pred_final_avg), 3)) %>% 
        dplyr::mutate(tday = dplyr::case_when(n_period == 1 ~ "t plus 1 day",
                                              n_period == 2 ~ "t plus 2 days",
                                              n_period == 3 ~ "t plus 3 days",
                                              n_period == 4 ~ "t plus 4 days",
                                              n_period == 5 ~ "t plus 5 days")) %>%
        ungroup %>%
        dplyr::select(-n_period)

# get signal dates
signalDate <- predDf %>%
        dplyr::select(trend, symbol, date, prediction, tday) %>%
        tidyr::spread(tday, prediction) %>%
        dplyr::mutate(`t plus 1 day` = dplyr::case_when(`t plus 1 day` > 0.5 ~ 1, TRUE ~ 0),
                      `t plus 2 days` = dplyr::case_when(`t plus 2 days` > 0.5 ~ 1, TRUE ~ 0),
                      `t plus 3 days` = dplyr::case_when(`t plus 3 days` > 0.5 ~ 1, TRUE ~ 0),
                      `t plus 4 days` = dplyr::case_when(`t plus 4 days` > 0.5 ~ 1, TRUE ~ 0),
                      `t plus 5 days` = dplyr::case_when(`t plus 5 days` > 0.5 ~ 1, TRUE ~ 0),
                      flag = `t plus 1 day` + `t plus 2 days` + `t plus 3 days` + `t plus 4 days` + `t plus 5 days`) %>%
        dplyr::select(trend, symbol, date, flag) %>%
        tidyr::spread(trend, flag) %>%
        dplyr::mutate(signal = dplyr::case_when(bearish <= 2 & bullish == 5 ~ 1,
                                                bearish == 5 & bullish <= 2 ~ 1,
                                                TRUE ~ 0)) %>%
        dplyr::filter(signal == 1) %>% 
        dplyr::select(-signal) %>%
        tidyr::gather(trend, value, -symbol, -date) %>%
        dplyr::filter(value == 5) %>%
        dplyr::select(-value)
```

Here's the most important piece of this final project - prediction. Let's first display the **Moving Average** chart. We look at the moving average of 5- and 20-trading windows. We can use this chart as a reference when reading our prediction chart. 

The prediction chart displays the two trends, i.e. **bearish**, **bullish**, each is defined by the threshold that we set up earlier. The red line represents a downward ("bearish") trend, whereas the green line represents the upward ("bullish") trend. These lines move up and down along the trading dates. We should read it as, "on that specific trading date, we predict "t plus n trading day" for moving upward and crossing our threshold (above 85th percentile of our change in closing prices between t and t plus n day) would be XX%, whereas the probability of moving downward and crossing the threshold (below 15th percentile) would be YY%. 

The red vertical dashed lines represent very strong signal of trending downward, whereas a green vertical dashed line represents the opposite upward trend. The vertical lines would only appear if there's a consensus among the 5 ndays and they must cross above the 50% cut-off line. We would interpret that as a very strong signal that the coming future trading day(s) would see a very strong downward/upward trend. Looking at the prediction chart, we see red flags (bearish signals) for these dates, i.e. `r signalDate %>% dplyr::filter(trend == "bearish") %>% .$date %>% unlist %>% paste0(., collapse = ", ")`. On the other hand, we see a green flag (bullish signal) for a specific date, i.e. `r signalDate %>% dplyr::filter(trend == "bullish") %>% .$date %>% unlist %>% paste0(., collapse = ",")`.

The model successfully captures these significant dates that were heavily impacted by the pandemic and its subsequent reactions from the global market. Basically, the market crashed on "02-24-2020" and the Dow plunged 1000 points on coronavirus fears and AAPL droped by almost 5% from the previous trading day ("02-21-2020"). The market went into complete turmoil in the week of "02-24-2020". A month later, AAPL bounced back starting on "03-23-2020". The prediction chart captured all these downard and upward moments in late March and throughout April. The prediction chart highly corresponds with the moving average chart and both charts display clear signal about when market will likely rise or crash. 

```{r prediction, echo = FALSE, fig.width = 10, fig.height = 8}
###################################
# MA charts
MA_chart <- xtsList[[stock]] %>% 
        historicalTransformation(transformOnly = TRUE) %>%
        dplyr::mutate(`5 day MA` = TTR::SMA(close, 5),
                      `20 day MA` = TTR::SMA(close, 20),
                      index = nrow(.):1) %>%
        dplyr::filter(index <= dfEval_subset_partition) %>%
        dplyr::select(date, close, `5 day MA`, `20 day MA`) %>%
        tidyr::gather(key, value, -date) %>%
        ggplot(aes(date, value)) + 
        geom_line(aes(col = key)) +
        labs(x = "", y = "") +
        scale_x_date(labels = scales::date_format("%Y-%m-%d"), date_breaks = "3 day") +
        scale_color_manual(values = c("black", "blue", "red")) +
        theme_minimal() +
        theme(legend.position = "top", 
              plot.title = element_text(hjust = 0.5),
              axis.text.x = element_text(hjust = 1, angle = 60),
              legend.title = element_blank()) +
        scale_y_continuous(labels = scales::dollar) +
        guides(col = guide_legend(reverse = TRUE)) +
        ggtitle(paste0(stock, " Close ($) Moving Average(s) for past ", dfEval_subset_partition, " trading days"))

###################################
# prediction chart
predChart <- predDf %>%
        dplyr::filter(symbol == stock) %>%
        ggplot(aes(x = date, y = prediction, col = trend)) +
        geom_line() +
        geom_hline(yintercept = 0.5, linetype = "dashed", col = "black") +
        geom_vline(aes(xintercept = date, col = trend), 
                   data = signalDate %>% dplyr::filter(symbol == stock),
                   linetype = "dashed") +
        scale_y_continuous(labels = scales::percent, limits = c(0, 1)) +
        scale_x_date(labels = scales::date_format("%Y-%m-%d"), 
                     date_breaks = "3 day", 
                     limits = c(min(predDf$date), max(predDf$date))) +
        facet_wrap(~ tday, nrow = 5) +
        theme_minimal() +
        theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
        theme(legend.position = "bottom") +
        labs(x = "", y = "") +
        ggtitle(paste0(stock, " Trend Prediction ", min(predDf$date), " : ", max(predDf$date)))

# plot
MA_chart %>% ggplotly()
predChart
```

```{r end_cluster, echo = FALSE}
# stop the cluster
parallel::stopCluster(cl)
```

## CONCLUSION

Obviously there's a lot of room for improvement, such as feature selection, precision, accuracy of the model, etc. Another piece of challenge is how to speed up the time of processing multiple stocks at the same time (running for 10 stocks would require almost an hour on my personal laptop with 32 GB of RAM). In addition, how do we decide an appropriate threshold that is useful or meaningful to us? It can vary from situation to situation, and obviously stock price movement depends on market volatility and other factors (like Elon Musk tweeted about TSLA price was too high in the morning and subsequently the market responded by dragging the price down by 12% before closing. All it took was a single tweet in a single trading day). After all, maybe all the hard work is not as easily interpretable or more insightful than a simple moving average chart!

## APPENDIX

```{r appendix, echo = TRUE, eval = FALSE}
############################################################
### SET UP ###

# load packages
packages <- c('broom', 'caret', 'doParallel', 'e1071', 'foreach', 'glue', 'gridExtra', 'Hmisc', 'htmlwidgets', 'InformationValue', 'kableExtra', 'parallel', 'plotly', 'quantmod', 'sqldf', 'tictoc', 'tidyverse', 'TTR', 'wrapr')
pacman::p_load(char = packages)

# set environment
readRenviron("~/I/config/.env")

# set project home directory
PROJECT_HOME_DIRECTORY <- Sys.getenv("PROJECT_HOME_DIRECTORY")
setwd(PROJECT_HOME_DIRECTORY)

# get real time data
REAL_TIME = TRUE
ALPHA_VANTAGE_API <- Sys.getenv("ALPHA_VANTAGE_API")

### GET SYMBOLS ###

# stock symbol
stock <- c("AAPL")

# detect, use multicores
numCores <- parallel::detectCores()

# create a simple cluster on the local machine using all available threads
cl <- parallel::makeCluster(detectCores(), methods = FALSE)

# register our cluster
doParallel::registerDoParallel(cl)

# loop through a list of stock tickers - super fast! 5x faster than the traditional for-loop approach!!
symbols <- foreach::foreach(i = 1:length(stock), .errorhandling = 'remove') %dopar% { quantmod::getSymbols(stock[i]) } %>% unlist  # change .errorhandling = 'pass' to see error

# return a single list of xts objects from the valid symbols
if(REAL_TIME){
        
        xtsList <- vector(mode = "list", length = length(symbols))
        
        for(i in 1:length(symbols)){
                
                xtsList[[i]] <- quantmod::getSymbols(
                        
                        symbols[i], 
                        env = NULL,  # set env = NULL and that is equivalent to auto.assign = FALSE
                        src = "av",
                        periodicity = "daily", 
                        output.size = "full", 
                        adjusted = TRUE,
                        api.key = ALPHA_VANTAGE_API
                        
                )
                
                Sys.sleep(12)
                
        }
        
} else {
        
        xtsList <- foreach::foreach(i = 1:length(symbols)) %dopar% { quantmod::getSymbols(symbols[i], env = NULL, adjusted = TRUE) }  # set env = NULL and that is equivalent to auto.assign = FALSE        
        
}

# set names for xtsList
names(xtsList) <- symbols

############################################################
### CUSTOM FUNCTION ###
historicalTransformation <- function(xts,        
                                     l = 1,
                                     n = 1,
                                     originalSymbol = FALSE,
                                     transformOnly = FALSE
){
        
        # transform() function
        transformation <- function(xts, l, n, originalSymbol){
                
                # symbol
                symbol <- stringr::str_extract_all(names(xts), pattern = "^[[:alpha:]].*\\.") %>% unlist %>% stringr::str_to_lower(.) %>% unique %>% gsub("\\.", "", .)
                
                # rename columns
                names(xts) <- gsub("^[[:alpha:]].*\\.", "", names(xts)) %>% stringr::str_to_lower(.)
                
                # turn it into data.frame
                xts <- as.data.frame(xts)
                
                # insert columns
                xts <- xts %>%
                        dplyr::mutate(
                                # date
                                date = row.names(xts) %>% lubridate::ymd(.),
                                year = lubridate::year(date),
                                quarter = lubridate::quarter(date),
                                month = lubridate::month(date),
                                weekday = base::weekdays(date),
                                day = lubridate::day(date),
                                # Lag
                                op_l = quantmod::Lag(xts[, "open"], l),
                                hi_l = quantmod::Lag(xts[, "high"], l),
                                lo_l = quantmod::Lag(xts[, "low"], l),
                                cl_l = quantmod::Lag(xts[, "close"], l),
                                vol_l = quantmod::Lag(xts[, "volume"], l),
                                adj_l = quantmod::Lag(xts[, "adjusted"], l),
                                # Next
                                op_n = lead(xts[, "open"], n),
                                hi_n = lead(xts[, "high"], n),
                                lo_n = lead(xts[, "low"], n),
                                cl_n = lead(xts[, "close"], n),
                                vol_n = lead(xts[, "volume"], n),
                                adj_n = lead(xts[, "adjusted"], n),
                                # daily "Close - Open", "Hi - Low"
                                cl_op_diff = close - open,
                                hi_lo_diff = high - low,
                                # Lag "diff"
                                op_l_diff = open - op_l,
                                hi_l_diff = high - hi_l,
                                lo_l_diff = low - lo_l,
                                cl_l_diff = close - cl_l,
                                vol_l_diff = volume - vol_l,
                                adj_l_diff = adjusted - adj_l,
                                # Next "diff"
                                op_n_diff = op_n - open,
                                hi_n_diff = hi_n - high,
                                lo_n_diff = lo_n - low,
                                cl_n_diff = cl_n - close,
                                vol_n_diff = vol_n - volume,
                                adj_n_diff = adj_n - adjusted,
                                # oscillator - RSI, CCI                                
                                rsi_close = TTR::RSI(xts[, "close"]),
                                cci_close = TTR::CCI(xts[, "close"])
                        )
                
                # oscillator - MACD
                macd <- MACD(xts$close) %>% as.data.frame %>% dplyr::select(macd)
                signal <- MACD(xts$close) %>% as.data.frame %>% dplyr::select(signal)
                
                # add MACD
                xts <- xts %>%
                        dplyr::mutate(macd_close = macd$macd,
                                      signal_close = signal$signal,
                                      macd_close_diff = macd_close - signal_close) %>%
                        dplyr::select(date, year, quarter, month, weekday, day,
                                      open, high, low, close, volume, adjusted,
                                      rsi_close, cci_close, macd_close, signal_close, macd_close_diff,
                                      everything()) %>%
                        dplyr::arrange(date)
                
                # rename columns back with original symbol
                if(originalSymbol){names(xts) <- paste(symbol, names(xts), sep = "_")}
                
                # return xts
                return(xts)
                
        }
        
        # return a df object
        if(transformOnly){
                
                # return a transformed xts object
                transformObj <- transformation(xts, l, n, originalSymbol)
                return(transformObj)
                
        } else {
                
                # return the min and max of different variables associated with dates based on a transformed xts object
                SYMBOL <- transformation(xts, l, n, originalSymbol)
                
                minVector <- SYMBOL[complete.cases(SYMBOL), 6:ncol(SYMBOL)] %>% lapply(., min) %>% unlist
                maxVector <- SYMBOL[complete.cases(SYMBOL), 6:ncol(SYMBOL)] %>% lapply(., max) %>% unlist
                
                minList <- vector(mode = "list", length = length(minVector))
                for(i in 1:length(minList)){
                        minList[[i]] <- sqldf(sprintf("select date, %s from SYMBOL where floor(%s) = floor(%f)", names(minVector)[i], names(minVector)[i], minVector[i]))
                }
                
                maxList <- vector(mode = "list", length = length(maxVector))
                for(i in 1:length(maxList)){
                        maxList[[i]] <- sqldf(sprintf("select date, %s from SYMBOL where ceil(%s) = ceil(%f)", names(maxVector)[i], names(maxVector)[i], maxVector[i]))
                }
                
                minDf <- minList %>%
                        bind_rows() %>%
                        tidyr::gather(., key, value, -date) %>%
                        dplyr::filter(!is.na(value)) %>%
                        dplyr::mutate(type = "min") %>%
                        arrange(key, date)
                
                maxDf <- maxList %>%
                        bind_rows() %>%
                        tidyr::gather(., key, value, -date) %>%
                        dplyr::filter(!is.na(value)) %>%
                        dplyr::mutate(type = "max") %>%
                        arrange(key, date)
                
                historyObj <- bind_rows(minDf, maxDf) %>%
                        dplyr::select(., type, everything()) %>%
                        arrange(type, key, date)
                
                return(historyObj)
        }
        
}

############################################################
### SET PARAMETERS FOR MODELS ###

# trend
volatility = c("bullish", "bearish")

# target percentile threshold, e.g. top X% and bottom X% price change (cl_n_diff / close)
cl_percentile_threshold = 0.15

# xts subset
xts_data_subset = TRUE

# xts subset partition, i.e. number of most recent trading days
xts_data_subset_partition = 1000

# lag day(s)
l_period = c(1, 5, 10, 15, 20)

# next day(s)
n_period = c(1, 2, 3, 4, 5)

# baseTrainMethods
baseTrainMethods <- c("glm", "nb", "gbm")

# topTrainMethods
topTrainMethods <- c("glm", "nb", "gbm")

# tune length
tune.length = 3

# seed 
seed = 1234

# split data into train and test set by XX
size = 0.8

# Dependent Variable
DV <- "target"

# Features Selection Threshold
features_selection_threshold = .01

# dfEval, i.e. number of trading days for evaluation
dfEval_subset_partition = 90

# output
modelSummaryList <- vector(mode = "list")

evalList <- vector(mode = "list")

############################################################
### BUILD MODELS ###

# start timer
tic()

# start nested for() loop        
for(b in volatility){
        
        marketTrend = b
        
        for(l in l_period){
                
                lday = l
                
                for(i in n_period){
                        
                        nday = i
                        
                        for(j in stock){
                                
                                # get data
                                x <- xtsList[[j]]
                                
                                # get target threshold
                                if(xts_data_subset){
                                        y.t <- historicalTransformation(x, l = lday, n = nday, transformOnly = TRUE) %>%
                                                dplyr::mutate(percent_diff = cl_n_diff / close,
                                                              partition = (nrow(.)-1):0) %>%
                                                dplyr::filter(partition <= xts_data_subset_partition)
                                } else {
                                        y.t <- historicalTransformation(x, l = lday, n = nday, transformOnly = TRUE) %>%
                                                dplyr::mutate(percent_diff = cl_n_diff / close)
                                }
                                y <- y.t$percent_diff
                                y <- y[!is.na(y)]
                                z <- quantile(y, c(cl_percentile_threshold, 1 - cl_percentile_threshold))
                                
                                # features
                                features <- c(                                
                                        'rsi_close', 
                                        'cci_close',
                                        'macd_close', 
                                        'signal_close',
                                        'vol_n_diff',
                                        'op_l_percent_diff', 
                                        'hi_l_percent_diff', 
                                        'lo_l_percent_diff', 
                                        'cl_l_percent_diff', 
                                        'vol_l_percent_diff',    
                                        'cl_op_diff_by_op',
                                        'hi_lo_diff_by_op',
                                        'cl_op_diff_by_cl_l',
                                        'hi_lo_diff_by_cl_l',
                                        'cl_cl_l_ratio',
                                        'op_cl_l_ratio',
                                        'vol_cl_l_ratio',
                                        'vol_op_l_ratio',  
                                        'vol_diff_ratio',
                                        'sma_5',                        
                                        'sma_10',
                                        'sma_15'
                                )
                                
                                # transform, get DV, set partition
                                x.t <- historicalTransformation(x, l = lday, n = nday, transformOnly = TRUE) %>%
                                        dplyr::mutate(
                                                trend = marketTrend,
                                                symbol = j,                        
                                                l_period = lday,
                                                n_period = nday,
                                                op_l_percent_diff = ((open - op_l) / op_l) %>% as.vector,
                                                hi_l_percent_diff = ((high - hi_l) / hi_l) %>% as.vector,
                                                lo_l_percent_diff = ((low - lo_l) / lo_l) %>% as.vector,
                                                cl_l_percent_diff = ((close - cl_l) / cl_l) %>% as.vector,
                                                vol_l_percent_diff = ((volume - vol_l) / vol_l) %>% as.vector,
                                                cl_op_diff_by_op = ((close - open) / open) %>% as.vector,
                                                hi_lo_diff_by_op = ((high - low) / open) %>% as.vector,
                                                cl_op_diff_by_cl_l = ((close - open) / cl_l) %>% as.vector,
                                                hi_lo_diff_by_cl_l = ((high - low) / cl_l) %>% as.vector,
                                                cl_cl_l_ratio = (close / cl_l) %>% as.vector,
                                                op_cl_l_ratio = (open / cl_l) %>% as.vector, 
                                                vol_cl_l_ratio = (vol_l_percent_diff / cl_cl_l_ratio) %>% as.vector, 
                                                vol_op_l_ratio = (vol_l_percent_diff / op_cl_l_ratio) %>% as.vector,  
                                                vol_diff_ratio = ((vol_n_diff / vol_l_diff) %>% as.vector) %>% as.vector, 
                                                sma_5 = TTR::SMA(close, 5),                                
                                                sma_10 = TTR::SMA(close, 10),
                                                sma_15 = TTR::SMA(close, 15),
                                                target = dplyr::case_when(trend == "bullish" ~ ifelse( (cl_n_diff / close) > z[2], "Y", "N" ),
                                                                          trend == "bearish" ~ ifelse( (cl_n_diff / close) < z[1], "Y", "N" )) %>%
                                                        factor(., levels = c("Y", "N"), labels = c("Yes", "No")),            
                                                partition = (nrow(.)-1):0) %>%  # 0 is referred to the most recent (or last) trading day
                                        dplyr::select(trend, symbol, l_period, n_period, partition, date, year, quarter, month, weekday, day, everything()) %>%
                                        arrange(desc(partition))                
                                
                                # whether to subset and use only the most recent trading days or entire historical record
                                if(xts_data_subset){x.t <- x.t %>% dplyr::filter(partition <= xts_data_subset_partition)}
                                
                                # scale features
                                x.t.scaled <- x.t %>% dplyr::mutate_at(vars(features), scale)
                                
                                # remove any nan column
                                x.t.scaled <- x.t.scaled[, which(unlist(lapply(x.t.scaled, function(x) !any(is.nan(x)))))]
                                
                                # complete.cases
                                x.t.scaled <- x.t.scaled %>% complete.cases() %.>% x.t.scaled[., ]
                                
                                # new set of features
                                features <- features[features %in% names(x.t.scaled)]
                                
                                # feature selections by t.test        
                                t.test.list <- lapply(x.t.scaled[, features], function(x) t.test(x ~ x.t.scaled$target) %>% broom::tidy())
                                t.test.df <- t.test.list %>%
                                        dplyr::bind_rows() %>%
                                        dplyr::mutate(variables = features) %>%
                                        arrange(p.value)
                                
                                IV <- t.test.df$variables[t.test.df$p.value < features_selection_threshold]
                                if(length(IV) <=2){IV = unique(c(IV, 'rsi_close', 'cci_close', 'macd_close', 'signal_close'))}
                                
                                # subset features
                                x.t.scaled2 <- x.t.scaled %>% 
                                        dplyr::select(partition, date, all_of(IV), DV) %>%
                                        dplyr::filter(partition > dfEval_subset_partition)
                                
                                # evaluation set (include all variables and TODAY values)
                                dfEval <- x.t %>% 
                                        dplyr::filter(partition <= dfEval_subset_partition) %>%
                                        dplyr::mutate_at(vars(features), scale) %>%
                                        dplyr::select(partition, date, all_of(IV), DV) %>%                
                                        dplyr::mutate_at(IV, as.vector)
                                
                                # impute dataset
                                preProcValues <- caret::preProcess(dfEval[, IV], method = c("medianImpute"))
                                dfEval <- predict(preProcValues, dfEval) %>%
                                        dplyr::mutate_at(IV, as.vector)
                                
                                # oversampling target == "Yes" by 50%
                                x.t.scaled2_Yes <- x.t.scaled2 %>% dplyr::filter(target == "Yes")
                                x.t.scaled2_IndexUp <- sample(1:nrow(x.t.scaled2_Yes), size = floor(nrow(x.t.scaled2_Yes) * 1.5), replace = TRUE)
                                x.t.scaled2_Yes <- x.t.scaled2_Yes[x.t.scaled2_IndexUp, ]
                                
                                # undersampling target == "No" by 50%
                                x.t.scaled2_No <- x.t.scaled2 %>% dplyr::filter(target == "No")
                                x.t.scaled2_IndexDown <- sample(1:nrow(x.t.scaled2_No), size = floor(nrow(x.t.scaled2_No) * 0.5), replace = FALSE)
                                x.t.scaled2_No <- x.t.scaled2_No[x.t.scaled2_IndexDown, ]
                                
                                # put them together to come up with new x.t.scaled
                                x.t.scaled2 <- dplyr::bind_rows(x.t.scaled2_Yes, x.t.scaled2_No)
                                
                                # split data into train, test sets
                                set.seed(seed + nday + lday)
                                index <- caret::createDataPartition(x.t.scaled2$target, p = size, list = FALSE)
                                trainSet <- x.t.scaled2[index, ]
                                testSet <- x.t.scaled2[-index, ] %>% distinct 
                                
                                # trainSetControl
                                trainSet.control <- caret::trainControl(method = "cv", number = 10, savePredictions = "final", classProbs = TRUE)
                                
                                # train base layer
                                for(baseLayer in baseTrainMethods){
                                        
                                        # set parameters
                                        ml = baseLayer
                                        model = paste0("model_", ml)
                                        OOF_prediction = paste0("OOF_pred_", ml)
                                        prediction = paste0("pred_", ml)
                                        result = paste0("result_", ml)
                                        
                                        # model
                                        assign(bquote(.(model)), caret::train(trainSet[, IV], trainSet[, DV], 
                                                                              method = ml, 
                                                                              trControl = trainSet.control, 
                                                                              tuneLength = tune.length))
                                        
                                        # Out-Of-Fold probability predictions - trainSet    
                                        if(ml == "glm"){trainSet$OOF_pred_glm = eval(sym(model))$pred$Y[order(eval(sym(model))$pred$rowIndex)]}
                                        if(ml == "nb"){trainSet$OOF_pred_nb = eval(sym(model))$pred$Y[order(eval(sym(model))$pred$rowIndex)]}
                                        if(ml == "gbm"){trainSet$OOF_pred_gbm = eval(sym(model))$pred$Y[order(eval(sym(model))$pred$rowIndex)]}
                                        
                                        # Out-Of-Fold probability predictions - testSet 
                                        assign(bquote(.(OOF_prediction)), predict(eval(sym(model)), testSet[, IV], type = "prob")$Y)
                                        if(ml == "glm"){testSet$OOF_pred_glm = eval(sym(OOF_prediction))}
                                        if(ml == "nb"){testSet$OOF_pred_nb = eval(sym(OOF_prediction))}
                                        if(ml == "gbm"){testSet$OOF_pred_gbm = eval(sym(OOF_prediction))}
                                        
                                        # Out-Of-Fold probability predictions - dfEval 
                                        assign(bquote(.(OOF_prediction)), predict(eval(sym(model)), dfEval[, IV], type = "prob")$Y)
                                        if(ml == "glm"){dfEval$OOF_pred_glm = eval(sym(OOF_prediction))}
                                        if(ml == "nb"){dfEval$OOF_pred_nb = eval(sym(OOF_prediction))}
                                        if(ml == "gbm"){dfEval$OOF_pred_gbm = eval(sym(OOF_prediction))}
                                        
                                        # Y/N predictions for Confusion Matrix - testSet    
                                        assign(bquote(.(prediction)), predict(eval(sym(model)), testSet[, IV]))
                                        if(ml == "glm"){testSet$pred_glm = eval(sym(prediction))}
                                        if(ml == "nb"){testSet$pred_nb = eval(sym(prediction))}
                                        if(ml == "gbm"){testSet$pred_gbm = eval(sym(prediction))}
                                        
                                        # output
                                        assign(bquote(.(result)), broom::tidy(caret::confusionMatrix(testSet[, prediction], testSet$target)) %>%
                                                       dplyr::mutate(trend = marketTrend, symbol = j, n_period = nday, l_period = lday, trainMethod = ml) %>%
                                                       dplyr::select(trend, symbol, n_period, l_period, trainMethod, everything()))
                                        
                                        # store output into a list
                                        tempModelList <- list(eval(sym(result)))
                                        modelSummaryList <<- c(modelSummaryList, tempModelList)
                                        
                                }
                                
                                # train top layer
                                for(topLayer in topTrainMethods){
                                        
                                        # set parameters
                                        ml = topLayer
                                        model = paste0("model_", ml)        
                                        OOF_predictors_top = c("OOF_pred_glm", "OOF_pred_nb", "OOF_pred_gbm")
                                        OOF_prediction_top = paste0("OOF_pred_top_", ml)
                                        prediction_top = paste0("pred_top_", ml)
                                        result = paste0("result_top_", ml)
                                        
                                        # model
                                        assign(bquote(.(model)), caret::train(trainSet[, OOF_predictors_top], trainSet[, DV], 
                                                                              method = ml, 
                                                                              trControl = trainSet.control, 
                                                                              tuneLength = tune.length))
                                        
                                        # Out-Of-Fold probability predictions - testSet 
                                        assign(bquote(.(OOF_prediction_top)), predict(eval(sym(model)), testSet[, OOF_predictors_top], type = "prob")$Y)
                                        if(ml == "glm"){testSet$OOF_pred_top_glm = eval(sym(OOF_prediction_top))}
                                        if(ml == "nb"){testSet$OOF_pred_top_nb = eval(sym(OOF_prediction_top))}
                                        if(ml == "gbm"){testSet$OOF_pred_top_gbm = eval(sym(OOF_prediction_top))}
                                        
                                        # Out-Of-Fold probability predictions - dfEval 
                                        assign(bquote(.(OOF_prediction_top)), predict(eval(sym(model)), dfEval[, OOF_predictors_top], type = "prob")$Y)
                                        if(ml == "glm"){dfEval$OOF_pred_top_glm = eval(sym(OOF_prediction_top))}
                                        if(ml == "nb"){dfEval$OOF_pred_top_nb = eval(sym(OOF_prediction_top))}
                                        if(ml == "gbm"){dfEval$OOF_pred_top_gbm = eval(sym(OOF_prediction_top))}
                                        
                                        # Y/N predictions for Confusion Matrix - testSet    
                                        assign(bquote(.(prediction_top)), predict(eval(sym(model)), testSet[, OOF_predictors_top]))
                                        if(ml == "glm"){testSet$pred_top_glm = eval(sym(prediction_top))}
                                        if(ml == "nb"){testSet$pred_top_nb = eval(sym(prediction_top))}
                                        if(ml == "gbm"){testSet$pred_top_gbm = eval(sym(prediction_top))}
                                        
                                        # output
                                        assign(bquote(.(result)), broom::tidy(caret::confusionMatrix(testSet[, prediction_top], testSet$target)) %>%
                                                       dplyr::mutate(trend = marketTrend, symbol = j, n_period = nday, l_period = lday, trainMethod = paste0(ml, " - top layer")) %>%
                                                       dplyr::select(trend, symbol, n_period, l_period, trainMethod, everything()))
                                        
                                        # store output into a list
                                        tempModelList <- list(eval(sym(result)))
                                        modelSummaryList <<- c(modelSummaryList, tempModelList)
                                        
                                }
                                
                                # put together - final averaging
                                testSet <- testSet %>%
                                        dplyr::mutate(pred_final_avg = (OOF_pred_top_glm + OOF_pred_top_nb + OOF_pred_top_gbm) / length(topTrainMethods),
                                                      pred_final = ifelse(pred_final_avg > 0.5, "Y", "N") %>%
                                                              factor(., levels = c("Y", "N"), labels = c("Yes", "No")))
                                
                                dfEval <- dfEval %>%
                                        dplyr::mutate(pred_final_avg = (OOF_pred_top_glm + OOF_pred_top_nb + OOF_pred_top_gbm) / length(topTrainMethods),
                                                      pred_final = ifelse(pred_final_avg > 0.5, 1, 0),
                                                      trend = marketTrend, symbol = j, n_period = nday, l_period = lday) %>%
                                        dplyr::select(trend, symbol, n_period, l_period, date, target, pred_final_avg, pred_final)
                                
                                finalResult <- broom::tidy(caret::confusionMatrix(testSet$pred_final, testSet$target)) %>%
                                        dplyr::mutate(trend = marketTrend, symbol = j, n_period = nday, l_period = lday, trainMethod = "final - averaging") %>%
                                        dplyr::select(trend, symbol, n_period, l_period, trainMethod, everything())
                                
                                # store output into a list
                                tempModelList <- list(finalResult)
                                modelSummaryList <<- c(modelSummaryList, tempModelList)
                                
                                tempEvalList <- list(dfEval)
                                evalList <<- c(evalList, tempEvalList)
                                
                        }
                        
                }
                
        }
        
}

# stop timer
toc()

# consolidate lists into df
modelSummaryDf <- modelSummaryList %>% dplyr::bind_rows()
evalDf <- evalList %>% dplyr::bind_rows()

############################################################
### VISUALIZATION ###

###################################
### TECHNICAL ANALYSIS ###
x <- xts::xts(xtsList[[stock]])

quantmod::chartSeries( x["201801::202005"],
                       name = stock,
                       TA = c(addBBands(draw = 'bands'), 
                              addADX(), 
                              addMACD(), 
                              addRSI(), 
                              addCCI(), 
                              addCMF(), 
                              addBBands(draw = 'width'), 
                              addBBands(draw = 'percent'), 
                              addVo()) )

###################################
# density curve
tempObj <- vector(mode = "list", length = length(n_period))

for(i in n_period){
        
        tempDf <- xtsList[[stock]] %>% 
                historicalTransformation(transformOnly = TRUE, n = i) %>%
                dplyr::mutate(percent_change = (cl_n_diff / close),
                              t_period = i,
                              tday = dplyr::case_when(t_period == 1 ~ "t plus 1 day",
                                                      t_period == 2 ~ "t plus 2 days",
                                                      t_period == 3 ~ "t plus 3 days",
                                                      t_period == 4 ~ "t plus 4 days",
                                                      t_period == 5 ~ "t plus 5 days"),
                              index = nrow(.):1) %>%
                dplyr::filter(index <= xts_data_subset_partition) %>%
                dplyr::select(date, close, cl_n_diff, percent_change, tday)
        
        tempList <- list(tempDf)
        tempObj <- c(tempObj, tempList)
        
}

tempDf <- tempObj %>% dplyr::bind_rows()

# gather data frame
dfGather <- tempDf %>%
        dplyr::select(-date) %>%
        dplyr::mutate(tday = as.factor(tday)) %>%
        tidyr::gather(key, value, -tday) 

# plot close ($) by tday
densityCurveClose <- ggplot(dfGather %>% dplyr::filter(key == "close") %>% distinct, 
                            aes(value)) +
        geom_density() +
        geom_vline(data = aggregate(value ~ tday, 
                                    dfGather %>% dplyr::filter(key == "close") %>% distinct, 
                                    median), 
                   aes(xintercept = value),
                   linetype = "dashed",
                   col = "red") +
        theme(legend.position = "none") +
        labs(x = "Close ($)") +
        theme_light() +
        scale_x_continuous(labels = scales::dollar) +
        scale_y_continuous(labels = scales::percent) +
        ggtitle(paste0(stock, " distribution of Close ($) for past ", xts_data_subset_partition, " trading days"))

# plot cl_n_diff
densityCurveCl_N_diff <- ggplot(dfGather %>% dplyr::filter(key == "cl_n_diff"), 
                                aes(value, fill = tday)) +
        geom_density(alpha = 0.5) +
        # median
        geom_vline(data = aggregate(value ~ tday, 
                                    dfGather %>% dplyr::filter(key == "cl_n_diff"), 
                                    median), 
                   aes(xintercept = value,
                       color = tday),
                   linetype = "dashed") +
        # cl_percentile_threshold
        geom_vline(data = aggregate(value ~ tday, 
                                    dfGather %>% dplyr::filter(key == "cl_n_diff"),
                                    quantile,
                                    probs = cl_percentile_threshold), 
                   aes(xintercept = value,
                       color = tday),
                   linetype = "dotted") +
        # 1 - cl_percentile_threshold
        geom_vline(data = aggregate(value ~ tday, 
                                    dfGather %>% dplyr::filter(key == "cl_n_diff"),
                                    quantile,
                                    probs = 1 -cl_percentile_threshold), 
                   aes(xintercept = value,
                       color = tday),
                   linetype = "dotted") +
        theme_minimal() +
        scale_x_continuous(labels = scales::dollar) +
        scale_y_continuous(labels = scales::percent) +
        theme(legend.position = "right") +
        labs(x = "Close ($) between t and t plus n day(s)") +
        ggtitle(paste0(stock, " distributions of difference between today (t) and future (t plus n trading day) Close ($) for past ", xts_data_subset_partition, " trading days")) +
        facet_wrap(~tday, nrow = 5)

# plot percent_change
densityCurveCl_percent_change <- ggplot(dfGather %>% dplyr::filter(key == "percent_change"), 
                                        aes(value, fill = tday)) +
        geom_density(alpha = 0.5) +
        # median
        geom_vline(data = aggregate(value ~ tday, 
                                    dfGather %>% dplyr::filter(key == "percent_change"), 
                                    median), 
                   aes(xintercept = value,
                       color = tday),
                   linetype = "dashed") +
        # cl_percentile_threshold
        geom_vline(data = aggregate(value ~ tday, 
                                    dfGather %>% dplyr::filter(key == "percent_change"),
                                    quantile,
                                    probs = cl_percentile_threshold), 
                   aes(xintercept = value,
                       color = tday),
                   linetype = "dotted") +
        # 1 - cl_percentile_threshold
        geom_vline(data = aggregate(value ~ tday, 
                                    dfGather %>% dplyr::filter(key == "percent_change"),
                                    quantile,
                                    probs = 1 -cl_percentile_threshold), 
                   aes(xintercept = value,
                       color = tday),
                   linetype = "dotted") +
        theme_bw() +
        scale_x_continuous(labels = scales::percent) +
        scale_y_continuous(labels = scales::percent) +
        theme(legend.position = "right") +
        labs(x = "% diff between t and t plus n day(s)") +
        ggtitle(paste0(stock, " distributions of change in percentage between today (t) and future (t plus n trading day) Close ($) for past ", xts_data_subset_partition, " trading days")) +
        facet_wrap(~tday, nrow = 5)

###################################
# evaluation chart
# data transformation for model evaluation

# spread table
modelSummarySpread <- modelSummaryDf %>% 
        dplyr::select(trend, symbol, n_period, l_period, trainMethod, term, estimate) %>%
        tidyr::spread(term, estimate) %>%
        arrange(trend, symbol, n_period, trainMethod)

# get measures
modelSummaryMeasure <- modelSummaryDf %>%
        dplyr::filter(term %in% c("recall", "precision", "f1") & trainMethod == "final - averaging") %>%
        dplyr::select(trend, symbol, n_period, l_period, measure = term, estimate) %>%
        dplyr::mutate(lday = dplyr::case_when(l_period == 1 ~ "lag 1 day",
                                              l_period == 5 ~ "lag 5 days",
                                              l_period == 10 ~ "lag 10 days",
                                              l_period == 15 ~ "lag 15 days",
                                              l_period == 20 ~ "lag 20 days") %>%
                              factor(., levels = c("lag 1 day", "lag 5 days", "lag 10 days", "lag 15 days", "lag 20 days")))

# get accuracy from eval set
accuracyDf <- evalDf %>%
        dplyr::mutate(flag = dplyr::case_when(pred_final == 0 ~ "No", TRUE ~ "Yes"),
                      accuracy = dplyr::case_when(target == flag ~ 1, TRUE ~ 0)) %>%
        dplyr::filter(!is.na(target)) %>%
        group_by(trend, symbol, n_period, l_period) %>%
        summarise(accuracy = mean(accuracy)) %>%
        dplyr::mutate(measure = "accuracy",
                      lday = dplyr::case_when(l_period == 1 ~ "lag 1 day",
                                              l_period == 5 ~ "lag 5 days",
                                              l_period == 10 ~ "lag 10 days",
                                              l_period == 15 ~ "lag 15 days",
                                              l_period == 20 ~ "lag 20 days") %>%
                              factor(., levels = c("lag 1 day", "lag 5 days", "lag 10 days", "lag 15 days", "lag 20 days"))) %>%
        dplyr::select(trend, symbol, n_period, l_period, measure, estimate = accuracy, lday) %>%
        arrange(trend, symbol, n_period, l_period)

# combine the key measures of model summary together
keyMeasures <- dplyr::bind_rows(modelSummaryMeasure, accuracyDf) %>%
        arrange(trend, symbol, n_period, l_period, measure)

# plot evaluation chart
evalChart <- keyMeasures %>%
        dplyr::filter(symbol == stock) %>%
        ggplot(aes(n_period, estimate, fill = factor(trend))) +
        geom_bar(stat = "identity", position = "dodge2") +
        geom_hline(yintercept = 0.5, linetype = "dashed", col = "black") +
        scale_y_continuous(labels = scales::percent, limits = c(0, 1)) +
        facet_grid(measure ~ lday) +
        labs(x = "Prediction for t plus n trading day(s)", y = "", fill = "trend") +
        theme_light() +
        theme(legend.position = "bottom") +
        ggtitle(paste0(stock, " Model Evaluation ", min(evalDf$date), " : ", max(evalDf$date)))

###################################
# MA charts
MA_chart <- xtsList[[stock]] %>% 
        historicalTransformation(transformOnly = TRUE) %>%
        dplyr::mutate(`5 day MA` = TTR::SMA(close, 5),
                      `20 day MA` = TTR::SMA(close, 20),
                      index = nrow(.):1) %>%
        dplyr::filter(index <= dfEval_subset_partition) %>%
        dplyr::select(date, close, `5 day MA`, `20 day MA`) %>%
        tidyr::gather(key, value, -date) %>%
        ggplot(aes(date, value)) + 
        geom_line(aes(col = key)) +
        labs(x = "", y = "") +
        scale_x_date(labels = scales::date_format("%Y-%m-%d"), date_breaks = "3 day") +
        scale_color_manual(values = c("black", "blue", "red")) +
        theme_minimal() +
        theme(legend.position = "top", 
              plot.title = element_text(hjust = 0.5),
              axis.text.x = element_text(hjust = 1, angle = 60),
              legend.title = element_blank()) +
        scale_y_continuous(labels = scales::dollar) +
        guides(col = guide_legend(reverse = TRUE)) +
        ggtitle(paste0(stock, " Close ($) Moving Average(s) for past ", dfEval_subset_partition, " trading days"))


###################################
# prediction chart

# data transformation for prediction
predDf <- evalDf %>%
        group_by(trend, symbol, n_period, date) %>%
        summarise(prediction = round(mean(pred_final_avg), 3)) %>% 
        dplyr::mutate(tday = dplyr::case_when(n_period == 1 ~ "t plus 1 day",
                                              n_period == 2 ~ "t plus 2 days",
                                              n_period == 3 ~ "t plus 3 days",
                                              n_period == 4 ~ "t plus 4 days",
                                              n_period == 5 ~ "t plus 5 days")) %>%
        ungroup %>%
        dplyr::select(-n_period)

# get signal dates
signalDate <- predDf %>%
        dplyr::select(trend, symbol, date, prediction, tday) %>%
        tidyr::spread(tday, prediction) %>%
        dplyr::mutate(`t plus 1 day` = dplyr::case_when(`t plus 1 day` > 0.5 ~ 1, TRUE ~ 0),
                      `t plus 2 days` = dplyr::case_when(`t plus 2 days` > 0.5 ~ 1, TRUE ~ 0),
                      `t plus 3 days` = dplyr::case_when(`t plus 3 days` > 0.5 ~ 1, TRUE ~ 0),
                      `t plus 4 days` = dplyr::case_when(`t plus 4 days` > 0.5 ~ 1, TRUE ~ 0),
                      `t plus 5 days` = dplyr::case_when(`t plus 5 days` > 0.5 ~ 1, TRUE ~ 0),
                      flag = `t plus 1 day` + `t plus 2 days` + `t plus 3 days` + `t plus 4 days` + `t plus 5 days`) %>%
        dplyr::select(trend, symbol, date, flag) %>%
        tidyr::spread(trend, flag) %>%
        dplyr::mutate(signal = dplyr::case_when(bearish <= 2 & bullish == 5 ~ 1,
                                                bearish == 5 & bullish <= 2 ~ 1,
                                                TRUE ~ 0)) %>%
        dplyr::filter(signal == 1) %>% 
        dplyr::select(-signal) %>%
        tidyr::gather(trend, value, -symbol, -date) %>%
        dplyr::filter(value == 5) %>%
        dplyr::select(-value)

# plot prediction chart
predChart <- predDf %>%
        dplyr::filter(symbol == stock) %>%
        ggplot(aes(x = date, y = prediction, col = trend)) +
        geom_line() +
        geom_hline(yintercept = 0.5, linetype = "dashed", col = "black") +
        geom_vline(aes(xintercept = date, col = trend), 
                   data = signalDate %>% dplyr::filter(symbol == stock),
                   linetype = "dashed") +
        scale_y_continuous(labels = scales::percent, limits = c(0, 1)) +
        scale_x_date(labels = scales::date_format("%Y-%m-%d"), 
                     date_breaks = "3 day", 
                     limits = c(min(predDf$date), max(predDf$date))) +
        facet_wrap(~ tday, nrow = 5) +
        theme_minimal() +
        theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
        theme(legend.position = "bottom") +
        labs(x = "", y = "") +
        ggtitle(paste0(stock, " Trend Prediction ", min(predDf$date), " : ", max(predDf$date)))

###################################
### PLOTLY ###

densityCurveClose %>% ggplotly()

densityCurveCl_N_diff %>% ggplotly()

densityCurveCl_percent_change %>% ggplotly()

evalChart %>% ggplotly()

MA_chart %>% ggplotly()

predChart  # vertical dashlines would disappear if we convert it into a plotly object, thus not recommend

###################################
### END ###
# stop the cluster
parallel::stopCluster(cl)
```








