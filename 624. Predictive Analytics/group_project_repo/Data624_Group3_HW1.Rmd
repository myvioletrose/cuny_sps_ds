---
title: "Data 624 HW1"
author: "Group 3: Alain Kuiete Tchoupou, Jeff Littlejohn, Samriti Malhotra, Rajwant Mishra, Jimmy Ng"
date: "Due: 6/20/2020"
output: word_document
always_allow_html: true
  # html_document:
  #   toc: true
---

First, we load the requisite packages.  

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE)
packages <- c("tidyverse", "fpp2", "forecast", "kableExtra", "broom", "ggplot2", "caret", "e1071", "knitr", "GGally", "VIM", "mlbench", "car", "corrplot", "mice", "seasonal", "fma", "latex2exp","gridExtra")
pacman::p_load(char = packages)
```

## HA 2

### 2.1 

#### Use the help function to explore what the series gold, woolyrnq and gas represent.

Each is a dataset contained within the forecast package. The gold dataset contains time series data of daily morning gold prices in US dollars from 1/1/1985 to 3/31/1989. The woolyrnq dataset includes quarterly woolen yarn production from Australia from 2Q 1965 to 3Q 1994. Finally, the gas dataset consists of time series data of Australian monthly gas production from 1956 to 1995.

#### a. Use autoplot() to plot each of these in separate plots. 
  
```{r}
autoplot(gold) +
  ggtitle("Daily morning gold prices") +
  xlab("Days since 1/1/1985") +
  ylab("US dollars")
```
```{r}
autoplot(woolyrnq) +
  ggtitle("Quarterly woollen yarn production in Australia") +
  xlab("Year") +
  ylab("Tonnes")
```
 
```{r}
autoplot(gas) +
  ggtitle("Australian monthly gas production") +
  xlab("Year") +
  ylab("Unknown units")
```
  
#### b. What is the frequency of each series? Hint: apply the frequency() function. 

```{r}
frequency(gold)
```
Gold - daily
 
```{r}
frequency(woolyrnq)
```
Woolyrnq - quarterly
 
```{r}
frequency(gas)
```
Gas - monthly
  
#### c. Use which.max() to spot the outlier in the gold series. Which observation was it? 
  
```{r}
which.max(gold)
```
This function gives us 770, which corresponds with the single giant upward spike in prices. 770 signifies the number of days since the beginning of the time series, which would put the spike in Feb. 1987.

```{r}
gold[770]
```
A cursory google search revealed no details about such a spike. We want my want to review for the possibility of a data-entry error.


### 2.3

#### Download some monthly Australian retail data from the book website. These represent retail sales in various categories for different Australian states, and are stored in a MS-Excel file.

[Manually downloaded to R working directory]

*You can read the data into R with the following script:*

```{r}
retaildata <- readxl::read_excel("retail.xlsx", skip=1)
```

*The second argument (skip=1) is required because the Excel sheet has two header rows.*

*Select one of the time series as follows (but replace the column name with your own chosen column):*

```{r}
myts <- ts(retaildata[,"A3349398A"],
  frequency=12, start=c(1982,4))
```

#### Explore your chosen retail time series using the following functions:

*autoplot(), ggseasonplot(), ggsubseriesplot(), gglagplot(), ggAcf()*

```{r}
autoplot(myts)
```

```{r}
ggseasonplot(myts)
```

```{r}
ggsubseriesplot(myts)
```

```{r}
gglagplot(myts)
```

```{r}
ggAcf(myts)
```


*Can you spot any seasonality, cyclicity and trend? What do you learn about the series?*

Starting with the autoplot, we see both an obvious and increasing trend along with a clear seasonal pattern that increases in nominal size over time. The seasonal plot shows highest monthly sales generally occur in December, with the biggest dip arriving in February, and the seasonal subseries visualization only confirms this. 

Neither the lag nor the autocorrelation plots add significant value to our analysis that was not already conveyed in the earlier visualizations. They back up the trend of increasing sales over time with seasonal variation that's likely tied to holiday shopping.


## HA 6

### Exercise 6.2

The `plastics` data set consists of the monthly sales (in thousands) of product A for a plastics manufacturer for five years.

```{r}
plastics
```


#### A. Plot the time series of sales of product A. Can you identify seasonal fluctuations and/or a trend-cycle?

```{r}
ggseasonplot(plastics, polar=TRUE) +
  ylab("$ Thousands") +
  ggtitle("Polar seasonal plot: Plastic consumption")
autoplot(plastics) + ggtitle("Annual plastic consumption ") + xlab("Year") + ylab("Plastic Sale of Product A")
#
# # With Moving average of 3 data point 
# autoplot(plastics, series="Data") +
#   autolayer(ma(plastics,3), series="3-MA") +
#   xlab("Year") + ylab("GWh") +
#   ggtitle("Annual electricity sales: South Australia") +
#   scale_colour_manual(values=c("Data"="grey50","3-MA"="red"),
#                       breaks=c("Data","3-MA"))
# 
# #  Classical Decomposition : Deafult is additive
# dec_plastic <- decompose(plastics)
# autoplot(dec_plastic)
# plastics %>% stl(s.window=6) %>% autoplot() +xlab("Year") + ylab("GWh") +
#   ggtitle("Annual electricity sales: South Australia")
# 
# # X11 Decomposition
# plastics %>% seas() %>% autoplot() + xlab("Year") + ylab("GWh") +
#   ggtitle("X11: Annual plastic consumptio")
# 
# 
# elecequip
# 
# as.ts(plastics,start=c(1959),frequency = 12)
```

Above plot of data very clearly shows that there is some pattern each year. This is the seasonality present in the data each year, which shows downward trend in the beginning of the year and peak in the mid of the year with decline again at the end of the year. 
Polar seasonal plot also suggest decline at its peak in the month of Jan and Feb and then picking up from there.


#### B. Use a classical multiplicative decomposition to calculate the trend-cycle and seasonal indices.
```{r}
 # Classical Decomposition : Deafult is additive
dec_plastic_mul <- decompose(plastics,type = "multiplicative")
autoplot(dec_plastic_mul)
# autoplot(stl(plastics,s.window = "periodic"))
print("Trend Cycle:")
trendcycle(dec_plastic_mul)
print("seasonal indices")
print(dec_plastic_mul$figure)
```

Classical decomposition methods assume that the seasonal component repeats from year to year. For many series, this is a reasonable assumption, but for some longer series it is not. 
Here third plot shows the trend with m= 12, as it shows yearly increasing trend.As data suggest around the end we see some high ressidual suddden drop in demand, which our trend-cycle estimate is not able to capture.


####C.Do the results support the graphical interpretation from part a?
```{r}
```

Yes, yearly trend suggested from A. is very clearly seen in above plots. We can also see that some decline in number at the end of 5th year in last 6 months.


####D. Compute and plot the seasonally adjusted data.
```{r}
# Decomposing time series data with multiplicative seasonal component
dec_plastic_mul <-  decompose(plastics,type = "multiplicative")
autoplot(plastics, series="Data") +
  autolayer(trendcycle(dec_plastic_mul), series="Trend") +
  autolayer(seasadj(dec_plastic_mul), series="Seasonally Adjusted") +
  xlab("Year") + ylab("Plastic Sale of Product A") +
  ggtitle("Plastic Sale of Product A") +
  scale_colour_manual(values=c("orange","blue","red"),
             breaks=c("Data","Seasonally Adjusted","Trend"))
```

Blue line in the above plot shows Seasonally adjusted data. As we can see that there are not much seasonality fluctuation in the data any more after we have decomposed the data , only linear trend can be seen.


#### E. Change one observation to be an outlier (e.g., add 500 to one observation), and recompute the seasonally adjusted data. What is the effect of the outlier?
```{r}
# Function to use for checking Outlier effect.
plot_outlier_trend <- function(c_index) {
  c_plastics <- plastics
  print(paste("Ref Value:" ,plastics[c_index], "at ", c_index))
  c_plastics[c_index] <- c_plastics[c_index] + 500
  # Decomposing time series data with multiplicative seasonal component
  c_dec_plastic_mul <-  decompose(c_plastics,type = "multiplicative")
  autoplot(plastics, series="Data") +
  autolayer(c_plastics, series="Data Outlier") +
  autolayer(trendcycle(dec_plastic_mul), series="Trend") +
  autolayer(trendcycle(c_dec_plastic_mul), series="Trend with Outlier") +
  xlab("Year") + ylab("Plastic Sale of Product A") +
  ggtitle("Plastic Sale of Product A") +
  scale_colour_manual(values=c("orange","gray","red","blue"),
             breaks=c("Data","Data Outlier","Trend","Trend with Outlier"))
}
plot_outlier_seasadj <- function(c_index) {
  c_plastics <- plastics
  print(paste("Ref Value:" ,plastics[c_index], "at ", c_index))
c_plastics[c_index] <- c_plastics[c_index] + 500
# Decomposing time series data with multiplicative seasonal component
c_dec_plastic_mul <-  decompose(c_plastics,type = "multiplicative")
autoplot(plastics, series="Data") +
  autolayer(seasadj(c_dec_plastic_mul), series="Seasonally Adjusted with Outlier") +
  autolayer(seasadj(dec_plastic_mul), series="Seasonally Adjusted") +
  xlab("Year") + ylab("Plastic Sale of Product A") +
  ggtitle("Plastic Sale of Product A") +
  scale_colour_manual(values=c("orange","blue","red"),
             breaks=c("Data","Seasonally Adjusted","Seasonally Adjusted with Outlier"))
  
}
# Choose some index and update the value with +500
set.seed(421)
c_index <- sample(1:59, 1)
plot_outlier_seasadj(c_index)
plot_outlier_trend(c_index)
```

Addition of outliers shows some sharp variation in the data at the same season in other year, but `trend line` seem to have no effect except in the year outliers are noted.
The spike in the seasonally adjusted data is due to th big ressidual at the ponit of outlier.


#### F. Does it make any difference if the outlier is near the end rather than in the middle of the time series?
```{r}
# Lets set the outlier in the begning and end and see how it impacts 
plot_outlier_seasadj(1)
plot_outlier_trend(1)
plot_outlier_seasadj(60)
plot_outlier_trend(60)
```

Outliers at the beginning or at the end have very little effect in the same season's data but its impact is negligible  in other season. Possible reason could be not knowing the  Trend Cycle  for the first 6 and last 6 months of the data in the multiplicative seasonal decomposition.



## KJ 3

### 3.1

#### The UC Irvine Machine Learning Repository6 contains a data set related to glass identification. The data consist of 214 glass samples labeled as one of seven class categories. There are nine predictors, including the refractive index and percentages of eight elements: Na, Mg, Al, Si, K, Ca, Ba, and Fe. The data can be accessed via:
```{r}
data(Glass)
str(Glass)
```

The structure of the Glass data shows that all the predictors are numeric. The dependent variable is factor of 6 levels.

```{r}
head(Glass)
```

The head function displays the first values of each variables.



 
 
#### (a) Using visualizations, explore the predictor variables to understand their distributions as well as the relationships between predictors.

We remove the target variable
```{r}
glass.predictors <- Glass[,-10]
```

Since all variables are numerical, we can use the skewness function of e1071 to estimated the predictors to represent.
```{r}
skewValues <- apply(glass.predictors, 2, skewness)
skewValues
```

```{r}
par(mfrow = c(3,3))
hist(x = glass.predictors$RI)
hist(x = glass.predictors$Na)
hist(x = glass.predictors$Mg)
hist(x = glass.predictors$Al)
hist(x = glass.predictors$Si)
hist(x = glass.predictors$K)
hist(x = glass.predictors$Ca)
hist(x = glass.predictors$Ba)
hist(x = glass.predictors$Fe)
```

The predictors RI, Na, Al, Si and Ca are normally distributed.
The predictors K, Ba, and Fe are rigtht skewed. we can aplied the log function on those variable to normalise or Boxcox to centralise, scale and transform.

The Mg predictor need to be centralise and scale. It is neither normal, nor skewed.


#### (b) Do there appear to be any outliers in the data? Are any predictors skewed?

Looking for outliers:
```{r}
par(mfrow = c(3,3))
boxplot(x = glass.predictors$RI, main = "RI")
boxplot(x = glass.predictors$Na, main = "Na")
boxplot(x = glass.predictors$Mg, main = "Mg")
boxplot(x = glass.predictors$Al, main = "Al")
boxplot(x = glass.predictors$Si, main = "Si")
boxplot(x = glass.predictors$K, main = "K")
boxplot(x = glass.predictors$Ca, main = "Ca")
boxplot(x = glass.predictors$Ba, main = "Ba")
boxplot(x = glass.predictors$Fe, main = "Fe")
```

The boxplot graphs shows some outliers with the predictors RI, Na, Al, Si, K, Ca, Ba, and Fe. The outlier of Ba and K are extreme.

```{r}
summary(glass.predictors)
```



To visualize the correlation between predictors, we use the corrplot function in the package of the same name.

```{r}
correlations <- cor(glass.predictors)
correlations
```

```{r}
corrplot(correlations, order = "hclust")
```


```{r}
GGally::ggpairs(as.data.frame(glass.predictors))
```

The only notable correlation is between RI and Ca.

#### (c) Are there any relevant transformations of one or more predictors that might improve the classification model?

We use the powerTransform of the car package that calculates the Box-Cox transformation.
The Box-Cox transformation uses the maximum likelihood approach and returns information on the estimated values along with convenient rounded values that are within 1.96 standard deviations of the maximum likelihood estimate.

```{r}
summary(powerTransform(Glass[,1:9], family="yjPower"))$result[,1:2]
```

 The suggested transformations are:

No transformation for RI, Na, Si, and K since lambda=1.
Log transformations for Mg, K, Ba, and Fe since lambda =0.
Square root transformation for Ca since lambda = 0.5.

### 3.2

#### The soybean data can also be found at the UC Irvine Machine Learning Repository. Data were collected to predict disease in 683 soybeans. The 35 predictors are mostly categorical and include information on the environmental conditions (e.g., temperature, precipitation) and plant conditions (e.g., left spots, mold growth). The outcome labels consist of 19 distinct classes.

The data can be loaded via:

```{r}
data(Soybean)
## See ?Soybean for details
str(Soybean)
```
 All variables are factors or ordered factors.
 
 
```{r}
head(Soybean)
```

#### (a) Investigate the frequency distributions for the categorical predictors. Are any of the distributions degenerate in the ways discussed earlier in this chapter?
```{r}
nearZeroVar(Soybean,saveMetric=TRUE)
```

The predictors that correspond respectively to the position of variables  19, 26, 28 in the datafarme Soybean  which are degenerate, are leaf.mild, mycelium and sclerotia.


```{r}
summary(Soybean)
```

Using the summary of Soybean, the fraction of unique values over the sample size of the predictors is low. There are 2, 3,0r 4 unique values over 683 observations.

The predictors leaf.mild, mycelium and sclerotia have the ratio of  the frequency the most prevalent value to the frequency of the second most prevalent very large.


```{r}
imbalance.leaf.mild = 535/20
imbalance.leaf.mild
```


```{r}
imbalance.mycelium = 639/6
imbalance.mycelium
```

```{r}
imbalance.sclerotia = 625/20
imbalance.sclerotia
```

The three predictors have a very strong imbalance. These are near-zero variance predictors

We can observe these large imbalance between uniques values in the plots below.
```{r}
par(mfrow = c(3,3))
plot(x = Soybean$leaves) + title(main = 'leaves')
plot(x = Soybean$leaf.malf) + title(main = 'leaf.malf')
plot(x = Soybean$leaf.mild) + title(main = 'leaf.mild')
plot(x = Soybean$lodging) + title(main = 'lodging')
plot(x = Soybean$mycelium) + title(main = 'mycelium')
plot(x = Soybean$int.discolor)+ title(main = 'int.discolor')
plot(x = Soybean$sclerotia) + title(main = 'sclerotia')
plot(x = Soybean$seed.size) + title(main = 'seed.size')
plot(x = Soybean$shriveling) + title(main = 'shriveling')
```



#### (b) Roughly 18% of the data are missing. Are there particular predictors that are more likely to be missing? Is the pattern of missing data related to the classes?

Observation of missing values
```{r}
colSums(is.na(Soybean))
```


The aggr function in the VIM package plots and calculates the amount of missing values in each variable. The dply function is useful for wrangling data into aggregate summaries and is used to find the pattern of missing data related to the classes.
```{r}
aggr(Soybean, prop = c(TRUE, TRUE), bars=TRUE, numbers=TRUE, sortVars=TRUE)
```
The table above and the histograms show that the predictors hail, sever, seed.tmt, and lodging have around 18% of missing data. Other variables that are more likely to be missing are germ(16% of missing values), leaf.mild(16%),fruiting.bodies(15%), fruits.spots(15%), seed.discolor(15%), and shriveling(15%).

The grid shows the combination of all with 82% of data not missing in accordance with the problem description (18% missing). The remainder of the grid shows missing data for variable combinations with each row highlighting the missing values for the group of variables detailed in the x-axis. The non-graphical output of the function shows on top the exact proportion of missing values per variable.


Looking for pattern in missing data by classes
```{r}
Soybean %>%
  mutate(Total = n()) %>% 
  filter(!complete.cases(.)) %>%
  group_by(Class) %>%
  mutate(Missing = n(), Proportion=Missing/Total) %>%
  select(Class, Missing, Proportion) %>%
  unique()
```

Checking if a pattern of missing data related to the classes exists is done by checking if some classes hold most of the incomplete cases. This is accomplished by filtering, grouping, and mutating the data with dplyr. The majority of the missing values are in the phytophthora-rot class which has nearly 10% incomplete cases. The are only four more, out of the eighteen other, variables with incomplete cases. The pattern of missing data is related to the classes. Mostly the phytophthora-rot class however since the other four variables only have between 1% and 2% incomplete cases.



#### (c) Develop a strategy for handling missing data, either by eliminating predictors or imputation.

The strategy to handle missing data is by using the predictive mean matching method of the mice function to impute data.
Next, we create a complete dataset with the function complete()
We can previous the new dataset for missing values with aggr from VIM package


```{r}
MICE <- mice(Soybean, method="pmm", printFlag=FALSE, seed=6)
aggr(complete(MICE), prop = c(TRUE, TRUE), bars=TRUE, numbers=TRUE, sortVars=TRUE)
```

The strategy we use to deal with missing data is the simple imputation method that uses predictive mean matching (pmm) and “imputes missing values by means of the nearest-neighbor donor with distance based on the expected values of the missing variables conditional on the observed covariates.”
 
After applying the mice function, we realise that there are no missing values in any variable.
 

#### KJ 3 References

https://www.otexts.org/fpp/

https://rpubs.com/josezuniga/358605

https://rpubs.com/josezuniga/253955

https://rpubs.com/josezuniga/269297

http://appliedpredictivemodeling.com/


## HA 7

### 7.1

Consider the `pigs` series — the number of pigs slaughtered in Victoria each month.

a. Use the `ses()` function in R to find the optimal values of $\alpha$ and $\ell_0$, and generate forecasts for the next four months.

b. Compute a 95% prediction interval for the first forecast using $\hat{y}\pm1.96s$ where $s$ is the standard deviation of the residuals. Compare your interval with the interval produced by R.

#### a. ses() function
```{r echo = TRUE, collapse = TRUE, fig.width = 8, fig.height = 10}
# take a look of the ts object
pigs <- read.csv("pigs.ts.csv") %>% ts()
str(pigs)
broom::glance(pigs %>% data.frame)
pigs %>% forecast::autoplot()
# forecast for the next 4 months
pigs_forecast4 <- forecast::ses(pigs, h = 4)
# tidy forecast summary
broom::tidy(pigs_forecast4 %>% summary)
# visualize output
pigs_forecast4_fitted <- fitted(pigs_forecast4)
pigs_forecast4 %>%
        autoplot() +
        autolayer(pigs_forecast4_fitted, series = "Fitted") +
        theme_classic() +
        theme(legend.position = "none") +
        xlab("Year") +
        ylab("Pigs Slaughtered")
```

#### b. 95% prediction interval
```{r echo = TRUE, collapse = TRUE, fig.width = 8, fig.height = 10}
# 95% prediction interval by hand
lo.interval <- pigs_forecast4$mean[1] - 1.96 * sd(pigs_forecast4$residuals)
up.interval <- pigs_forecast4$mean[1] + 1.96 * sd(pigs_forecast4$residuals)
# R model output
lo.interval.model <- pigs_forecast4$lower[1, 2]
up.interval.model <- pigs_forecast4$upper[1, 2]
# comparison  
type <- paste0(c("calculated by hand", "calculated by hand", "run by R", "run by R"))
bound <- paste0(c("lower", "upper", "lower", "upper"))
interval <- c(lo.interval, up.interval, lo.interval.model, up.interval.model)
data.frame(type, bound, interval) %>%
        tidyr::spread(., type, interval) %>%
        dplyr::mutate(`diff (%)` = round((`calculated by hand` - `run by R`) / `calculated by hand` * 100, 2)) %>%
        kable() %>%
        kable_styling()
```

### 7.3 {.tabset .tabset-fade .tabset-pills}

Modify your function from the previous exercise to return the sum of squared errors rather than the forecast of the next observation. Then use the `optim()` function to find the optimal values of $\alpha$ and  $\ell_0$. Do you get the same values as the `ses()` function? 

```{r echo = TRUE, collapse = TRUE}
# custom function
custom_fun <- function(fn, ts){
        a = fn[1]
        b = fn[2]
        n = length(ts)
        yhat = c(b, 0 * (2:n))
        for (i in 1:(n-1)){
                yhat[i +1] <- a * ts[i] + (1-a) * yhat[i]
        }
        res = ts - yhat
        sse = sum(res^2) / (n-1)
        
        return(sse)
}
# output parameters
parameters <- optim(par = c(0.1, 10000), fn = custom_fun, ts = pigs)
a <- parameters$par[1]
b <- parameters$par[2]
# comparison
model_a <- pigs_forecast4$model$fit$par[1]
model_b <- pigs_forecast4$model$fit$par[2]
# comparison  
type <- paste0(c("calculated by hand", "calculated by hand", "run by R", "run by R"))
parameter <- paste0(c("alpha", "level", "alpha", "level"))
value <- c(a, b, model_a, model_b)
data.frame(type, parameter, value) %>%
        dplyr::mutate(value = round(value, 3)) %>%
        tidyr::spread(., type, value) %>%
        kable() %>%
        kable_styling()
```

The result is almost identical. 


## HA 8

### 8.1

#### Figure 8.31 shows the ACFs for 36 random numbers, 360 random numbers and 1,000 random numbers.

#### a. Explain the differences among these figures. Do they all indicate that the data are white noise?

![Left: ACF for a white noise series of 36 numbers. Middle: ACF for a white noise series of 360 numbers. Right: ACF for a white noise series of 1,000 numbers.](https://raw.githubusercontent.com/Rajwantmishra/DATA_624/master/Data624_Group3_HW1_HA8_1aGraph.png)

The Auto Correlation Function (ACF) conveys serial correlation between lagged values in time series data.(Along with the Partial Auto Correlation Functions (PACF) plots, they also help us determine good values for *p* and *q*.)

As white noise is truly randomly generated, there should be no correlation between lagging values. Our three examples show few if any correlation values that exceed the 95% confidence intervals, so we do probably do not detect significant correlation between values. This fits our expectation of white noise -  that it's stationary. 

Based on our confidence intervals beings set at 95%, we would expect a small percentage of our white noise values to exceed the intervals. We might take a closer look at lag 12 series x1 with Ljung-Box test given a lag of 12 could indicate annual seasonality, but with a sample of 36 time series values, we would expect the occassional outlier.


#### b. Why are the critical values at different distances from the mean of zero? Why are the autocorrelations different in each figure when they each refer to white noise?

As sample size grows, the interval narrows. Note the x- and y-axes are the same. Our formula for the confidence interval here is:

$$\pm\frac{\sqrt{2}}{T}$$

It's clear that as T grows, the interval shrinks. This seems intuitive, as smaller samples may erroneously show autocorrelation based on a few fluke outliers. 


### 8.2

#### 2. A classic example of a non-stationary series is the daily closing IBM stock price series (data set ibmclose). Use R to plot the daily closing prices for IBM stock and the ACF and PACF. Explain how each plot shows that the series is non-stationary and should be differenced.

```{r}
#https://www.rdocumentation.org/packages/forecast/versions/8.12/topics/ggtsdisplay
ggtsdisplay(ibmclose,main="IBM Closing Stock Price",xlab="Days",ylab="Closing Stock Price")
```
In the regular time series plot, we see changing, non-seasonal trends in the data over the year.The daily closing price is not a stationary time series. The ACF plot shows clear and significant autocorrelation to a lag of 25. The PACF plot shows a significant spike at one, which could indicate that all higher order autocorrelation is explained by that first lag autocorrelation. (We also know that the first partial autocorrelation coefficient will be identical to that of the first autocorrelation.)

Given the above, taking the differences of the closing stock price data could produce a stationary series.

```{r}
ggtsdisplay(diff(ibmclose),main="Difference in IBM Closing Stock Price",xlab="Days",ylab="Diff Closing Stock Price")
```
We might more correlation values that exceed the confidence interval than one would expect with a stationary dataset. Let's conduct the Ljung-Box test to verify:

```{r}
#https://stat.ethz.ch/R-manual/R-devel/library/stats/html/box.test.html
Box.test(diff(ibmclose), type = c("Ljung-Box"))
```

For the test, smaller p values means there's more autocorrelation - not stationary. Taking one difference gets us over the hump, and the data can now be called stationary - a random walk.


### 8.6

#### 6. Use R to simulate and plot some data from simple ARIMA models.

#### a. Use the following R code to generate data from an AR(1) model with $\phi_1 = 0.6$ and $\sigma^2=1$. The process starts with $y_1 = 0$.

```{r}
y <- ts(numeric(100))
e <- rnorm(100)
for(i in 2:100)
y[i] <- 0.6*y[i-1] + e[i]
```


#### b. Produce a time plot for the series. How does the plot change as you change $\phi_1$? 

```{r}
#phi1 = 0
y0 <- ts(numeric(100))
e0 <- rnorm(100)
for(i in 2:100)
y0[i] <- 0*y0[i-1] + e0[i]
p0 = autoplot(y0) + ggtitle(TeX("$\\phi_1 = 0$"))
#phi1 = .1
y_1 <- ts(numeric(100))
e_1 <- rnorm(100)
for(i in 2:100)
y_1[i] <- 0.1*y_1[i-1] + e_1[i]
p_1 = autoplot(y_1) + ggtitle(TeX("$\\phi_1 = 0.1$"))
#phi1 = .6
p_6 = autoplot(y) + ggtitle(TeX("$\\phi_1 = 0.6$"))
#phi1 = 1
y1 <- ts(numeric(100))
e1 <- rnorm(100)
for(i in 2:100)
y1[i] <- 1*y1[i-1] + e1[i]
p1 = autoplot(y1) + ggtitle(TeX("$\\phi_1 = 1$"))
#phi1 = -.5
yn_5 <- ts(numeric(100))
en_5 <- rnorm(100)
for(i in 2:100)
yn_5[i] <- -.5*yn_5[i-1] + en_5[i]
pn_5 = autoplot(yn_5) + ggtitle(TeX("$\\phi_1 = -0.5$"))
grid.arrange(p0,p_1,p_6,p1,pn_5)
```

When $\phi_1$ approaches 0, $y_t$ represents white noise. As$\phi_1$ approaches 1, we see greater variation and likely autocorrelation, as expected. Let's verify by reviewing ACF plots.

```{r}
acf0 = ggAcf(y0) + ggtitle(TeX("$\\phi_1 = 0$"))
acf_1 = ggAcf(y_1) + ggtitle(TeX("$\\phi_1 = .1$"))
acf_6 = ggAcf(y) + ggtitle(TeX("$\\phi_1 = .6$"))
acf1 = ggAcf(y1) + ggtitle(TeX("$\\phi_1 = 1$"))
acfn_5 = ggAcf(yn_5) + ggtitle(TeX("$\\phi_1 = -0.5$"))
grid.arrange(acf0,acf_1,acf_6,acf1,acfn_5)
```
We see that as $\phi_1$ increases, the correlations of more lag values exceed the 95% confidence interval, indicating that the data is/are autocorrelated. A negative $\phi_1$ value causes the correlations of the lags to jump back and forth around 30.


#### c. Write your own code to generate data from an MA(1) model with $\theta_{1} = 0.6$ and $\sigma^2=1$.

```{r}
y_ma_6 <- ts(numeric(100))
e_ma_6 <- rnorm(100,sd=1)
for(i in 2:100)
y_ma_6[i] <- 0.6*e_ma_6[i-1] + e_ma_6[i]
```


#### d. Produce a time plot for the series. How does the plot change as you change $\theta_1$?

```{r}
#theta1 = 0
y_ma0 <- ts(numeric(100))
e_ma0 <- rnorm(100,sd=1)
for(i in 2:100)
y_ma0[i] <- 0*e_ma0[i-1] + e_ma0[i]
t0 = autoplot(y_ma0) + ggtitle(TeX("$\\theta_{1} = 0$"))
#theta1 = .1
y_ma_1 <- ts(numeric(100))
e_ma_1 <- rnorm(100,sd=1)
for(i in 2:100)
y_ma_1[i] <- .1*e_ma_1[i-1] + e_ma_1[i]
t_1 = autoplot(y_ma_1) + ggtitle(TeX("$\\theta_{1} = 0.1$"))
#theta1 = .6
t_6 = autoplot(y_ma_6) + ggtitle(TeX("$\\theta_{1} = 0.6$"))
#theta1 = 1
y_ma1 <- ts(numeric(100))
e_ma1 <- rnorm(100,sd=1)
for(i in 2:100)
y_ma1[i] <- 1*e_ma1[i-1] + e_ma1[i]
t1 = autoplot(y_ma1) + ggtitle(TeX("$\\theta_{1} = 1$"))
grid.arrange(t0,t_1,t_6,t1)
```
```{r}
acft0 = ggAcf(y_ma0) + ggtitle(TeX("$\\theta_{1} = 0$"))
acft_1 = ggAcf(y_ma_1) + ggtitle(TeX("$\\theta_{1} = .1$"))
acft_6 = ggAcf(y_ma_6) + ggtitle(TeX("$\\theta_{1} = .6$"))
acft1 = ggAcf(y_ma1) + ggtitle(TeX("$\\theta_{1} = 1$"))
grid.arrange(acft0,acft_1,acft_6,acft1)
```
Generally, the MA(1) model shows more variation as $\theta_1$ increases. However, the effect appears to be less pronounced than with the AR(1) model.  


#### e. Generate data from an ARMA(1,1) model with $\phi_{1} = 0.6$, $\theta_{1} = 0.6$ and $\sigma^2=1$. 

```{r}
ya11 <- ts(numeric(100))
ea11 <- rnorm(100, sd=1)
for(i in 2:100)
  ya11[i] <- 0.6*ya11[i-1] + 0.6*ea11[i-1] + ea11[i]
autoplot(ya11) +
  ggtitle(TeX("$\\theta_{1} = .6$ and $\\phi_{1} = .6$: ARIMA(1,1) Model"))
```


#### f. Generate data from an AR(2) model with $\phi_{1} =-0.8$, $\phi_{2} = 0.3$ and $\sigma^2=1$. (Note that these parameters will give a non-stationary series.)

```{r}
yar2 <- ts(numeric(100))
ear2 <- rnorm(100, sd=1)
for(i in 3:100)
  yar2[i] <- -0.8*yar2[i-1] + 0.3*yar2[i-2] + ear2[i]
autoplot(yar2) +
  ggtitle(TeX("$\\phi_{1} = -0.8$ and $\\phi_{2} = .3$: AR(2) Model"))
```


#### g. Graph the latter two series and compare them.

```{r}
autoplot(ya11, series = "ARMA(1,1), Theta1 = .6, Phi1 = .6") +
  autolayer(yar2, series = "AR(2), Phi1 = -.8, Phi2 = .3") +
  xlab("y")
```

Setting phi1 and ph2 at such values in the AR(2) model creates clear autocorrelation that will grow over time. It's a non-stationary, sinusoidal pattern. The ARMA(1,1) model is stationary. 


### 8.8

#### 8. Consider austa, the total international visitors to Australia (in millions) for the period 1980-2015.

#### a. Use auto.arima() to find an appropriate ARIMA model. What model was selected. Check that the residuals look like white noise. Plot forecasts for the next 10 periods.

```{r}
mausta <- auto.arima(austa)
mausta
```

We see that ARIMA(0,1,1) with drift is selected as the best model to convey pattern in our Australian visitor data. 

```{r}
checkresiduals(mausta)
```
ACF plot shows stationary data, and residual distribution looks fine. Ljung-Box test values appear copacetic. Let's plot those 10 additional years of visitors.

```{r}
mausta %>% forecast(h=10) %>% autoplot()
```

#### b. Plot forecasts from an ARIMA(0,1,1) model with no drift and compare these to part a. Remove the MA term and plot again.

```{r}
mausta_nodrift <- Arima(austa,c(0,1,1),include.drift=FALSE)
mausta_nodrift %>% forecast(h=10) %>% autoplot()
```

Without drift, forecast values are constant.

```{r}
mausta_nodrift_noma <- Arima(austa,c(0,1,0),include.drift=FALSE)
mausta_nodrift_noma %>% forecast(h=10) %>% autoplot()
```
Without the MA term, the curve is slightly less smooth.


#### c. Plot forecasts from an ARIMA(2,1,3) model with drift. Remove the constant and see what happens.

```{r}
mausta_213_drift <- Arima(austa,c(2,1,3),include.drift=TRUE)
mausta_213_drift %>% forecast(h=10) %>% autoplot()
```

```{r}
#get an error when trying to exclude constant without changing method
mausta_213_drift_noc <- Arima(austa,c(2,1,3),include.constant=FALSE, method="CSS")
mausta_213_drift_noc %>% forecast(h=10) %>% autoplot()
```


I don't know what the method change does in order to get excluding the constant to work. Based on the plots, excluding the constant increases the confidence intervals of the forecast.


#### d. Plot forecasts from an ARIMA(0,0,1) model with a constant. Remove the MA term and plot again.

```{r}
mausta_001 <- Arima(austa,c(0,0,1))
mausta_001 %>% forecast(h=10) %>% autoplot()
```

```{r}
mausta_000 <- Arima(austa,c(0,0,0))
mausta_000 %>% forecast(h=10) %>% autoplot()
```

Without the MA term, the forecast is constant. With it, we see a drop early in the forecasted periods.


#### e. Plot forecasts from an ARIMA(0,2,1) model with no constant.

```{r}
mausta_021 <- Arima(austa,c(0,2,1),include.constant=FALSE)
mausta_021 %>% forecast(h=10) %>% autoplot()
```
