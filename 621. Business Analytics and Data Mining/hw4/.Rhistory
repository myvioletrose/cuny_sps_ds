knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(broom)
library(Hmisc)
library(tidyr)
library(corrplot)
library(mice)
library(MASS)
library(caret)
# set working directory
setwd("~/school_of_professional_studies/621. Business Analytics and Data Mining/hw1")
# read data
dfTrain <- read.csv("moneyball-training-data.csv")
dfEval <- read.csv("moneyball-evaluation-data.csv")
# head, dim
head(dfTrain); dim(dfTrain)
head(dfEval); dim(dfEval)
# what is the difference between the two data.frames?
paste0(names(dfTrain)[names(dfTrain) %nin% names(dfEval)], " is not found in the evaluation data set")
# data structure
str(dfTrain)
# descriptive statistics
summary(dfTrain)
# length of unique value for each variable
sapply(dfTrain, function(x) unique(length(x)))
# any missing
colSums(is.na(dfTrain))
### Visualization of data distribution, spread, outliners and correlations among variables ###
# boxplot
dfTrain %>%
tidyr::gather(key, value, -INDEX) %>%
ggplot(aes(x = key, y = value, fill = key)) +
geom_boxplot() +
# scale_y_continuous(labels = scales::dollar) +
geom_boxplot(outlier.colour = "red") +
theme(legend.position = "none",
axis.title.y = element_blank()) +
coord_flip()
# corrplot
dfTrain %>%
complete.cases() %>%
dfTrain[., ] %>%
dplyr::select(-INDEX) %>%
cor() %>%
corrplot(method = "number")
# let's look at number of missing values again
colSums(is.na(dfTrain))
# let's impute missing values using the mice package
dfImpute <- dfTrain %>%
mice::mice(m = 1,  # number of imputed data set
maxit = 10,  # number of iterations to impute missing values
method = "pmm",  # method used in imputation, and we choose "predictive mean matching" for all our numeric variables
seed = 1234)
dfClean <- mice::complete(dfImpute)
str(dfClean)
# check again
colSums(is.na(dfClean))
# split into train, test
set.seed(1234)
index <- sample(1:nrow(dfClean), size = nrow(dfClean) * 0.7)
train <- dfClean[index, ]
test <- dfClean[-index, ]
# build full model
fullModel <- lm(TARGET_WINS ~., data = train %>% dplyr::select(-INDEX))
broom::glance(fullModel)
# build full model - sqrt transformation for taking care of outliners
DV <- names(dfClean)[names(dfClean) %nin% c("INDEX", "TARGET_WINS")]
fullModelSqrt <- lm(TARGET_WINS ~ ., data = train %>% dplyr::select(-INDEX) %>% dplyr::mutate_at(vars(DV), sqrt))
broom::glance(fullModelSqrt)
# stepwise regression - direction default to both
step <- MASS::stepAIC(fullModel, trace = FALSE)
step$anova
# 2 degree of interactions - full model
step2 <- MASS::stepAIC(fullModel, ~ .^2, trace = FALSE)
step2$anova
# 2 degree of interactions - sqrt transformation
step2.1 <- MASS::stepAIC(fullModelSqrt, ~.^2, trace = FALSE)
step2.1$anova
# final model (based on the sqrt transformation)
finalModelSqrt <- lm(
TARGET_WINS ~ TEAM_BATTING_H + TEAM_BATTING_2B + TEAM_BATTING_3B +
TEAM_BATTING_HR + TEAM_BATTING_BB + TEAM_BATTING_SO + TEAM_BASERUN_SB +
TEAM_BASERUN_CS + TEAM_BATTING_HBP + TEAM_PITCHING_H + TEAM_PITCHING_HR +
TEAM_PITCHING_BB + TEAM_PITCHING_SO + TEAM_FIELDING_E + TEAM_FIELDING_DP +
TEAM_BATTING_BB:TEAM_FIELDING_DP + TEAM_BASERUN_SB:TEAM_PITCHING_HR +
TEAM_BATTING_SO:TEAM_BASERUN_SB + TEAM_BATTING_2B:TEAM_FIELDING_DP +
TEAM_BATTING_HBP:TEAM_PITCHING_H + TEAM_BATTING_H:TEAM_FIELDING_E +
TEAM_BATTING_HR:TEAM_PITCHING_H + TEAM_PITCHING_H:TEAM_PITCHING_SO +
TEAM_BATTING_2B:TEAM_BATTING_HR + TEAM_BATTING_3B:TEAM_PITCHING_SO +
TEAM_BATTING_H:TEAM_BATTING_HR + TEAM_BASERUN_SB:TEAM_PITCHING_H +
TEAM_PITCHING_SO:TEAM_FIELDING_E + TEAM_BATTING_BB:TEAM_FIELDING_E +
TEAM_BATTING_SO:TEAM_PITCHING_SO + TEAM_BATTING_3B:TEAM_PITCHING_H +
TEAM_BATTING_SO:TEAM_PITCHING_H + TEAM_PITCHING_H:TEAM_PITCHING_BB +
TEAM_BATTING_H:TEAM_BATTING_3B + TEAM_BATTING_HR:TEAM_BASERUN_SB +
TEAM_BATTING_BB:TEAM_BASERUN_SB + TEAM_BATTING_H:TEAM_PITCHING_SO +
TEAM_BASERUN_CS:TEAM_PITCHING_BB + TEAM_BATTING_3B:TEAM_FIELDING_E +
TEAM_BATTING_3B:TEAM_BASERUN_SB + TEAM_BATTING_HBP:TEAM_FIELDING_E +
TEAM_FIELDING_E:TEAM_FIELDING_DP + TEAM_BASERUN_SB:TEAM_PITCHING_BB +
TEAM_BATTING_SO:TEAM_FIELDING_E + TEAM_PITCHING_BB:TEAM_FIELDING_E +
TEAM_BATTING_HR:TEAM_BATTING_BB + TEAM_BATTING_2B:TEAM_BATTING_HBP +
TEAM_BATTING_2B:TEAM_BASERUN_SB + TEAM_BASERUN_SB:TEAM_PITCHING_SO +
TEAM_PITCHING_HR:TEAM_FIELDING_DP + TEAM_BASERUN_SB:TEAM_BASERUN_CS +
TEAM_BASERUN_CS:TEAM_PITCHING_HR + TEAM_BASERUN_SB:TEAM_FIELDING_DP,
data = train %>% dplyr::select(-INDEX) %>% dplyr::mutate_at(vars(DV), sqrt)
)
broom::glance(finalModelSqrt)
broom::tidy(finalModelSqrt) %>% arrange(p.value)
# diagnostic
par(mfrow = c(2, 2))
plot(finalModel)
# diagnostic
par(mfrow = c(2, 2))
plot(finalModelSqrt)
# plot predicted outcomes with actuals with confidence intervals
testFinal <- test %>%
dplyr::mutate(predictedOutcome = round(predict(finalModelSqrt,
newdata = test %>% dplyr::select(-INDEX) %>% dplyr::mutate_at(vars(DV), sqrt),
type = "response")))
test.pred <- predict(finalModelSqrt,
newdata = test %>% dplyr::select(-INDEX) %>% dplyr::mutate_at(vars(DV), sqrt),
interval = "prediction")
testCombined <- cbind(testFinal, test.pred) %>%
dplyr::select(-fit)
test.subset <- testCombined %>%
dplyr::select(target_wins = TARGET_WINS,
predicted_wins = predictedOutcome,
lwr,
upr) %>%
dplyr::filter(predicted_wins >0)
p <- test.subset%>%
ggplot(aes(target_wins, predicted_wins)) +
geom_point()
p + geom_point(aes(y = lwr), col = "red") +
geom_point(aes(y = upr), col = "green") +
geom_abline(intercept = 0, slope = 1)
# what is the correlation betweent predicted and actuals?
cor.test(test.subset$target_wins, test.subset$predicted_wins)
# what is the RMSE of this final model (sqrt transformed)?
caret::RMSE(pred = test.subset$predicted_wins, obs = test.subset$target_wins)
# finally, let's apply it to our evaluation set
colSums(is.na(dfEval))
# let's impute missing values using the mice package
dfImpute2 <- dfEval %>%
mice::mice(m = 1,  # number of imputed data set
maxit = 10,  # number of iterations to impute missing values
method = "pmm",  # method used in imputation, and we choose "predictive mean matching" for all our numeric variables
seed = 1234)
dfEvalClean <- mice::complete(dfImpute2)
dfEval$evalPredictedOutcome = round(predict(finalModelSqrt,
newdata = dfEvalClean %>% dplyr::select(-INDEX) %>% dplyr::mutate_at(vars(DV), sqrt),
type = "response"))
head(dfEval)
today <- Sys.Date()
yesterday <- Sys.Date() - 1
one_year <- yesterday - 365
one_year_month <- lubridate::floor_date(one_year, unit = "month")
start_of_month <- lubridate::floor_date(yesterday, unit = "month")
end_of_month <- lubridate::ceiling_date(yesterday, unit = "month") - 1
yesterday_day <- format(yesterday, "%d")
eopm <- yesterday - day(yesterday)
sopm <- eopm - day(eopm) + 1
last_weekday <- as.Date(ifelse(weekdays(today) == "Monday", Sys.Date() - 3, yesterday))
last_weekday_day <- format(last_weekday, "%d")
yesterday_weekday <- weekdays(yesterday)
month_days <- format(end_of_month, "%d")
month_pacing <- percent(round(as.numeric(yesterday_day) / as.numeric(month_days), 2))
month_character <- months(yesterday)
year_character <- as.numeric(format(yesterday, "%Y"))
start_of_year <- lubridate::floor_date(yesterday, unit = "year")
rm(list = ls())
pacman::p_load(
anytime,
bigrquery,
dplyr,
glue,
googleAuthR,
htmlTable,
httr,
jsonlite,
kableExtra,
lubridate,
readr,
sendmailR,
scales,
tidyr,
zoo
)
today <- Sys.Date()
yesterday <- Sys.Date() - 1
one_year <- yesterday - 365
one_year_month <- lubridate::floor_date(one_year, unit = "month")
start_of_month <- lubridate::floor_date(yesterday, unit = "month")
end_of_month <- lubridate::ceiling_date(yesterday, unit = "month") - 1
yesterday_day <- format(yesterday, "%d")
eopm <- yesterday - day(yesterday)
sopm <- eopm - day(eopm) + 1
last_weekday <- as.Date(ifelse(weekdays(today) == "Monday", Sys.Date() - 3, yesterday))
last_weekday_day <- format(last_weekday, "%d")
yesterday_weekday <- weekdays(yesterday)
month_days <- format(end_of_month, "%d")
month_pacing <- percent(round(as.numeric(yesterday_day) / as.numeric(month_days), 2))
month_character <- months(yesterday)
year_character <- as.numeric(format(yesterday, "%Y"))
start_of_year <- lubridate::floor_date(yesterday, unit = "year")
today
ls()
one_year
yesterday
one_year_month
start_of_month
end_of_month
yesterday_day
eopm
eopm
sopm
eopm
sopm
last_weekday
last_weekday_day
yesterday_weekday
month_days
month_days
month_pacing <- percent(round(as.numeric(yesterday_day) / as.numeric(month_days), 2))
month_pacing
yesterday_day
month_days
month_pacing
month_character
year_character
start_of_year
rm(list = ls())
knitr::opts_chunk$set(echo = TRUE)
# load packages
if(!require(pacman)){install.packages("pacman"); require(pacman)}
packages <- c('tidyverse', 'sqldf', 'broom', 'survival', 'survminer', 'kableExtra')
pacman::p_load(char = packages)
# head, dim
df <- read.csv("df.csv")
head(df)
dim(df)
# check missing
colSums(is.na(df))
# billing periods
unique(df$billing_period)
# billing periods x status
ftable(df$billing_period, df$status)
# average number of payment received by billing period
aggregate(lifetime ~ billing_period, data = df, FUN = mean)
# nest data, i.e. data = c(status, lifetime)
dfNest <- df %>%
dplyr::select(billing_period, status, lifetime) %>%
dplyr::mutate(status = dplyr::case_when(status == 'Active' ~ 1, TRUE ~ 2)) %>%
nest(., data = c(status, lifetime)) %>%
arrange(billing_period)
dfNest
# build model for overall survival - (estimate lifetime for new subscriptions)
overall_survival <- lapply(dfNest$data, function(x) survfit(Surv(lifetime, status) ~ 1, data = x))
names(overall_survival) <- dfNest$billing_period
# tidy result output
overall_survival_result <- lapply(1:length(overall_survival),
function(x) tidy(overall_survival[[x]]) %>%
dplyr::mutate(billing_period = names(overall_survival[x])))
# combine results (from list) into one dataframe
overall_survival_result <- overall_survival_result %>%
dplyr::bind_rows(.) %>%
dplyr::select(billing_period, lifetime = time, everything())
overall_survival_result %>% kable()
ggsurvplot(
fit = survfit(Surv(lifetime, status) ~ billing_period, data = unnest(dfNest)),
xlab = "Payments",
ylab = "Overall survival probability")
# option 1
option_1 <- overall_survival_result %>% group_by(billing_period) %>% summarise(lifetime = sum(estimate))
option_1 <- option_1 %>% dplyr::mutate(lifetime = floor(lifetime))
option_1
# option 2
# create 'expectancy' table - a fixed number of outcomes (possible number of payments) for each payment cohort
billing_periods_list <- list("Annual", "Month", "Semi_Annual", "Two_Years")
# set the maximum number of payment equal to 4 years
lifetime_list <- list(Annual = c(0:4), Month = c(0:48), Semi_Annual = c(0:8), Two_Years = c(0:2))
expectancy <- purrr::map2(billing_periods_list, lifetime_list, function(x, y) expand.grid(x, y)) %>%
dplyr::bind_rows(.) %>%
dplyr::select(billing_period = Var1,
lifetime = Var2)
# left join with the above model result output
modelOutput <- expectancy %>%
dplyr::left_join(., overall_survival_result, by = c("billing_period" = "billing_period", "lifetime", "lifetime")) %>%
dplyr::select(billing_period, lifetime, estimate) %>%
arrange(billing_period, lifetime)
modelOutput %>% kable()
# decay function
for(i in 1:nrow(modelOutput)){
if(!is.na(modelOutput$estimate[i])) {
next
} else {
modelOutput$estimate[i] <- dplyr::lag(modelOutput$estimate, 1)[i] / dplyr::lag(modelOutput$estimate, 2)[i] * dplyr::lag(modelOutput$estimate, 1)[i]
}
}
modelOutput %>% kable()
# sum up the estimate to get E(x)
option_2 <- modelOutput %>%
dplyr::mutate(estimate = dplyr::case_when(is.nan(estimate) ~ 0,
TRUE ~ estimate)) %>%
group_by(billing_period) %>%
summarise(periods_remaining = sum(estimate)) %>%
dplyr::mutate(periods_remaining = floor(periods_remaining))
# comparison between option 1 and 2
option_1
option_2
rm(list = ls())
setwd("~/school_of_professional_studies/621. Business Analytics and Data Mining/hw4")
knitr::opts_chunk$set(echo = TRUE)
# load packages
if(!require(pacman)){install.packages("pacman"); require(pacman)}
packages <- c('tidyverse', 'sqldf', 'broom', 'caret', 'kableExtra', 'janitor', 'Hmisc')
pacman::p_load(char = packages)
# read data
dfTrain <- read.csv("insurance_training_data.csv", header = TRUE)
dfEval <- read.csv("insurance-evaluation-data.csv", header = TRUE)
# check dim
dim(dfTrain); dim(dfEval)
# are they compatible?
if(!any(names(dfTrain) == names(dfEval))){print("the two data sets are different, please check for consistency")}
# clean names
dfTrain <- dfTrain %>% janitor::clean_names()
dfEval <- dfEval %>% janitor::clean_names()
# head
head(dfTrain) %>% kable()
# target flag - proportion
with(dfTrain, prop.table(ftable(target_flag), 1))
# target amt - distribution
hist(dfTrain$target_amt)
# target amt - log transformed
boxplot(log(dfTrain$target_amt) ~ dfTrain$target_flag)
# length of unique values of each variable from dfTrain
sapply(dfTrain, function(x) length(unique(x)))
# quickly glance at the class of each variable
fields <- data.frame(fields = names(dfTrain),
class = sapply(dfTrain, class) %>% unlist %>% as.vector)
fields
# mutate target_flag
# mutate these 4 factors - "income", "home_val", "bluebook", "oldclaim"
# these should be numeric variables, not factor
dfTrain <- dfTrain %>%
dplyr::mutate(target_flag = dplyr::case_when(target_flag == 1 ~ "Y",
TRUE ~ "N") %>%
factor(., levels = c("Y", "N"), labels = c("Yes", "No")),
income = as.numeric(income),
home_val = as.numeric(home_val),
bluebook = as.numeric(bluebook),
oldclaim = as.numeric(oldclaim))
dfEval <- dfEval %>%
dplyr::mutate(target_flag = dplyr::case_when(target_flag == 1 ~ "Y",
TRUE ~ "N") %>%
factor(., levels = c("Y", "N"), labels = c("Yes", "No")),
income = as.numeric(income),
home_val = as.numeric(home_val),
bluebook = as.numeric(bluebook),
oldclaim = as.numeric(oldclaim))
# set variables
binary_var <- "target_flag"
quant_var <- "target_amt"
variables <- names(dfTrain)[names(dfTrain) %nin% c(binary_var, quant_var, "index")]
# check missing
colSums(is.na(dfTrain))
colSums(is.na(dfEval))
# fix missing - train
preProcValuesTrain <- caret::preProcess(dfTrain[, variables], method = c("medianImpute", "center", "scale"))
dfTrain <- predict(preProcValuesTrain, dfTrain)
colSums(is.na(dfTrain))
# fix missing - eval
preProcValuesEval <- caret::preProcess(dfEval[, variables], method = c("medianImpute", "center", "scale"))
dfEval <- predict(preProcValuesEval, dfEval)
colSums(is.na(dfEval))
# split data into train (70%), test (30%) sets
set.seed(1234)
index <- caret::createDataPartition(dfTrain$target_flag, p = 0.7, list = FALSE)
trainSet <- dfTrain[index, ]
testSet <- dfTrain[-index, ]
# trainSetControl - let's do 10-fold cross-validation
trainSet.control <- caret::trainControl(method = "cv", number = 10, savePredictions = 'final', classProbs = TRUE)
# count time - start
start <- Sys.time()
# methods
baseTrainMethods <- c("glm", "rf", "nb")
topTrainMethods <- c("rf", "nb", "gbm")
# output
modelSummaryList <- vector(mode = "list")
# train base layer
for(baseLayer in baseTrainMethods){
# set parameters
ml = baseLayer
model = paste0("model_", ml)
OOF_prediction = paste0("OOF_pred_", ml)
prediction = paste0("pred_", ml)
result = paste0("result_", ml)
# model
assign(bquote(.(model)), caret::train(trainSet[, variables], trainSet[, binary_var],
method = ml,
trControl = trainSet.control))
# Out-Of-Fold probability predictions - trainSet
if(ml == "glm"){trainSet$OOF_pred_glm = eval(sym(model))$pred$Y[order(eval(sym(model))$pred$rowIndex)]}
if(ml == "rf"){trainSet$OOF_pred_rf = eval(sym(model))$pred$Y[order(eval(sym(model))$pred$rowIndex)]}
if(ml == "nb"){trainSet$OOF_pred_nb = eval(sym(model))$pred$Y[order(eval(sym(model))$pred$rowIndex)]}
# Out-Of-Fold probability predictions - testSet
assign(bquote(.(OOF_prediction)), predict(eval(sym(model)), testSet[, variables], type = "prob")$Y)
if(ml == "glm"){testSet$OOF_pred_glm = eval(sym(OOF_prediction))}
if(ml == "rf"){testSet$OOF_pred_rf = eval(sym(OOF_prediction))}
if(ml == "nb"){testSet$OOF_pred_nb = eval(sym(OOF_prediction))}
# Y/N predictions for Confusion Matrix - testSet
assign(bquote(.(prediction)), predict(eval(sym(model)), testSet[, variables]))
if(ml == "glm"){testSet$pred_glm = eval(sym(prediction))}
if(ml == "rf"){testSet$pred_rf = eval(sym(prediction))}
if(ml == "nb"){testSet$pred_nb = eval(sym(prediction))}
# output
assign(bquote(.(result)), broom::tidy(caret::confusionMatrix(testSet[, prediction], testSet[, binary_var])) %>%
dplyr::mutate(trainMethod = ml) %>%
dplyr::select(trainMethod, everything()))
# store output into a list
tempModelList <- list(eval(sym(result)))
modelSummaryList <<- c(modelSummaryList, tempModelList)
}
# train top layer
for(topLayer in topTrainMethods){
# set parameters
ml = topLayer
model = paste0("model_", ml)
OOF_predictors_top = c("OOF_pred_glm", "OOF_pred_rf", "OOF_pred_nb")
OOF_prediction_top = paste0("OOF_pred_top_", ml)
prediction_top = paste0("pred_top_", ml)
result = paste0("result_top_", ml)
# model
assign(bquote(.(model)), caret::train(trainSet[, OOF_predictors_top], trainSet[, binary_var],
method = ml,
trControl = trainSet.control))
# Out-Of-Fold probability predictions - testSet
assign(bquote(.(OOF_prediction_top)), predict(eval(sym(model)), testSet[, OOF_predictors_top], type = "prob")$Y)
if(ml == "glm"){testSet$OOF_pred_top_gbm = eval(sym(OOF_prediction_top))}
if(ml == "nb"){testSet$OOF_pred_top_nb = eval(sym(OOF_prediction_top))}
if(ml == "gbm"){testSet$OOF_pred_top_glm = eval(sym(OOF_prediction_top))}
# Y/N predictions for Confusion Matrix - testSet
assign(bquote(.(prediction_top)), predict(eval(sym(model)), testSet[, OOF_predictors_top]))
if(ml == "glm"){testSet$pred_top_gbm = eval(sym(prediction_top))}
if(ml == "nb"){testSet$pred_top_nb = eval(sym(prediction_top))}
if(ml == "gbm"){testSet$pred_top_glm = eval(sym(prediction_top))}
# output
assign(bquote(.(result)), broom::tidy(caret::confusionMatrix(testSet[, prediction_top], testSet[, binary_var])) %>%
dplyr::mutate(trainMethod = paste0(ml, " - top layer")) %>%
dplyr::select(trainMethod, everything()))
# store output into a list
tempModelList <- list(eval(sym(result)))
modelSummaryList <<- c(modelSummaryList, tempModelList)
}
