last_weekday <- as.Date(ifelse(weekdays(today) == "Monday", Sys.Date() - 3, yesterday))
last_weekday_day <- format(last_weekday, "%d")
yesterday_weekday <- weekdays(yesterday)
month_days <- format(end_of_month, "%d")
month_pacing <- percent(round(as.numeric(yesterday_day) / as.numeric(month_days), 2))
month_character <- months(yesterday)
year_character <- as.numeric(format(yesterday, "%Y"))
start_of_year <- lubridate::floor_date(yesterday, unit = "year")
rm(list = ls())
pacman::p_load(
anytime,
bigrquery,
dplyr,
glue,
googleAuthR,
htmlTable,
httr,
jsonlite,
kableExtra,
lubridate,
readr,
sendmailR,
scales,
tidyr,
zoo
)
today <- Sys.Date()
yesterday <- Sys.Date() - 1
one_year <- yesterday - 365
one_year_month <- lubridate::floor_date(one_year, unit = "month")
start_of_month <- lubridate::floor_date(yesterday, unit = "month")
end_of_month <- lubridate::ceiling_date(yesterday, unit = "month") - 1
yesterday_day <- format(yesterday, "%d")
eopm <- yesterday - day(yesterday)
sopm <- eopm - day(eopm) + 1
last_weekday <- as.Date(ifelse(weekdays(today) == "Monday", Sys.Date() - 3, yesterday))
last_weekday_day <- format(last_weekday, "%d")
yesterday_weekday <- weekdays(yesterday)
month_days <- format(end_of_month, "%d")
month_pacing <- percent(round(as.numeric(yesterday_day) / as.numeric(month_days), 2))
month_character <- months(yesterday)
year_character <- as.numeric(format(yesterday, "%Y"))
start_of_year <- lubridate::floor_date(yesterday, unit = "year")
today
ls()
one_year
yesterday
one_year_month
start_of_month
end_of_month
yesterday_day
eopm
eopm
sopm
eopm
sopm
last_weekday
last_weekday_day
yesterday_weekday
month_days
month_days
month_pacing <- percent(round(as.numeric(yesterday_day) / as.numeric(month_days), 2))
month_pacing
yesterday_day
month_days
month_pacing
month_character
year_character
start_of_year
rm(list = ls())
knitr::opts_chunk$set(echo = TRUE)
# load packages
if(!require(pacman)){install.packages("pacman"); require(pacman)}
packages <- c('tidyverse', 'sqldf', 'broom', 'survival', 'survminer', 'kableExtra')
pacman::p_load(char = packages)
# head, dim
df <- read.csv("df.csv")
head(df)
dim(df)
# check missing
colSums(is.na(df))
# billing periods
unique(df$billing_period)
# billing periods x status
ftable(df$billing_period, df$status)
# average number of payment received by billing period
aggregate(lifetime ~ billing_period, data = df, FUN = mean)
# nest data, i.e. data = c(status, lifetime)
dfNest <- df %>%
dplyr::select(billing_period, status, lifetime) %>%
dplyr::mutate(status = dplyr::case_when(status == 'Active' ~ 1, TRUE ~ 2)) %>%
nest(., data = c(status, lifetime)) %>%
arrange(billing_period)
dfNest
# build model for overall survival - (estimate lifetime for new subscriptions)
overall_survival <- lapply(dfNest$data, function(x) survfit(Surv(lifetime, status) ~ 1, data = x))
names(overall_survival) <- dfNest$billing_period
# tidy result output
overall_survival_result <- lapply(1:length(overall_survival),
function(x) tidy(overall_survival[[x]]) %>%
dplyr::mutate(billing_period = names(overall_survival[x])))
# combine results (from list) into one dataframe
overall_survival_result <- overall_survival_result %>%
dplyr::bind_rows(.) %>%
dplyr::select(billing_period, lifetime = time, everything())
overall_survival_result %>% kable()
ggsurvplot(
fit = survfit(Surv(lifetime, status) ~ billing_period, data = unnest(dfNest)),
xlab = "Payments",
ylab = "Overall survival probability")
# option 1
option_1 <- overall_survival_result %>% group_by(billing_period) %>% summarise(lifetime = sum(estimate))
option_1 <- option_1 %>% dplyr::mutate(lifetime = floor(lifetime))
option_1
# option 2
# create 'expectancy' table - a fixed number of outcomes (possible number of payments) for each payment cohort
billing_periods_list <- list("Annual", "Month", "Semi_Annual", "Two_Years")
# set the maximum number of payment equal to 4 years
lifetime_list <- list(Annual = c(0:4), Month = c(0:48), Semi_Annual = c(0:8), Two_Years = c(0:2))
expectancy <- purrr::map2(billing_periods_list, lifetime_list, function(x, y) expand.grid(x, y)) %>%
dplyr::bind_rows(.) %>%
dplyr::select(billing_period = Var1,
lifetime = Var2)
# left join with the above model result output
modelOutput <- expectancy %>%
dplyr::left_join(., overall_survival_result, by = c("billing_period" = "billing_period", "lifetime", "lifetime")) %>%
dplyr::select(billing_period, lifetime, estimate) %>%
arrange(billing_period, lifetime)
modelOutput %>% kable()
# decay function
for(i in 1:nrow(modelOutput)){
if(!is.na(modelOutput$estimate[i])) {
next
} else {
modelOutput$estimate[i] <- dplyr::lag(modelOutput$estimate, 1)[i] / dplyr::lag(modelOutput$estimate, 2)[i] * dplyr::lag(modelOutput$estimate, 1)[i]
}
}
modelOutput %>% kable()
# sum up the estimate to get E(x)
option_2 <- modelOutput %>%
dplyr::mutate(estimate = dplyr::case_when(is.nan(estimate) ~ 0,
TRUE ~ estimate)) %>%
group_by(billing_period) %>%
summarise(periods_remaining = sum(estimate)) %>%
dplyr::mutate(periods_remaining = floor(periods_remaining))
# comparison between option 1 and 2
option_1
option_2
rm(list = ls())
knitr::opts_chunk$set(echo = TRUE)
# load packages
if(!require(pacman)){install.packages("pacman"); require(pacman)}
packages <- c('tidyverse', 'caret', 'broom', 'Hmisc', 'kableExtra')
pacman::p_load(char = packages)
# load packages
if(!require(pacman)){install.packages("pacman"); require(pacman)}
packages <- c('tidyverse', 'caret', 'broom', 'Hmisc', 'kableExtra')
pacman::p_load(char = packages)
# read data
df <- read.csv(url('https://datahack-prod.s3.ap-south-1.amazonaws.com/train_file/train_u6lujuX_CVtuZ9i.csv'))
# check missing
tempDf <- data.frame(fields = names(df),
missing = colSums(is.na(df)))
# impute dataset
preProcValues <- preProcess(df, method = c("medianImpute", "center", "scale"))
dfProcessed <- predict(preProcValues, df)
# check again
colSums(is.na(dfProcessed))
# clean names
dfProcessed <- janitor::clean_names(dfProcessed)
# read data
df <- read.csv(url('https://datahack-prod.s3.ap-south-1.amazonaws.com/train_file/train_u6lujuX_CVtuZ9i.csv'))
# check missing
tempDf <- data.frame(fields = names(df),
missing = colSums(is.na(df)))
tempDf
# impute dataset
preProcValues <- preProcess(df, method = c("medianImpute", "center", "scale"))
dfProcessed <- predict(preProcValues, df)
# check again
tempDf2 <- data.frame(fields = names(dfProcessed),
missing = colSums(is.na(dfProcessed)))
tempDf2
# clean names
dfProcessed <- janitor::clean_names(dfProcessed)
names(dfProcessed)
# check distribution
broom::glance(dfProcessed)
table(dfProcessed$loan_status)
prop.table(ftable(dfProcessed$loan_status)) %>% round(., 1)
# head df
head(dfProcessed) %>% kable(.)
# str(), dim()
str(dfProcessed)
dim(dfProcessed)
# make changes to the "loan_status" level, label
levels(dfProcessed$loan_status)
dfProcessed <- dfProcessed %>%
dplyr::mutate(loan_status = loan_status %>%
factor(., levels = c("Y", "N"), labels = c("Yes", "No")))
levels(dfProcessed$loan_status)
# set seed
set.seed(1234)
# split data
index <- caret::createDataPartition(dfProcessed$loan_status, p = 0.7, list = FALSE)
trainSet <- df_processed[index, ]
# set seed
set.seed(1234)
# split data
index <- caret::createDataPartition(dfProcessed$loan_status, p = 0.7, list = FALSE)
trainSet <- dfProcessed[index, ]
testSet <- dfProcessed[-index, ]
# define the training control - let's do 10-fold cross-validation
train.control <- caret::trainControl(
method = "cv",
number = 10,
savePredictions = 'final',
classProbs = TRUE)
# define IVs, DV
IV <- names(dfProcessed)[names(dfProcessed) %nin% c("loan_id", "loan_status")]
DV <- "loan_status"
dim(trainSet)
# set seed
set.seed(1234)
# methods
baseTrainMethods <- c("glm", "nb", "rf")
topTrainMethods <- c("glm", "nb", "gbm")
# output
modelSummaryList <- vector(mode = "list")
# train base layer
for(baseLayer in baseTrainMethods){
# set parameters
ml = baseLayer
model = paste0("model_base_", ml)
OOF_prediction = paste0("OOF_pred_", ml)
prediction = paste0("pred_", ml)
result = paste0("result_", ml)
# model
assign(bquote(.(model)), caret::train(trainSet[, IV], trainSet[, DV],
method = ml,
trControl = trainSet.control,
trace = FALSE))
# Out-Of-Fold probability predictions - trainSet
if(ml == "glm"){trainSet$OOF_pred_glm = eval(sym(model))$pred$Y[order(eval(sym(model))$pred$rowIndex)]}
if(ml == "nb"){trainSet$OOF_pred_nb = eval(sym(model))$pred$Y[order(eval(sym(model))$pred$rowIndex)]}
if(ml == "rf"){trainSet$OOF_pred_rf = eval(sym(model))$pred$Y[order(eval(sym(model))$pred$rowIndex)]}
# Out-Of-Fold probability predictions - testSet
assign(bquote(.(OOF_prediction)), predict(eval(sym(model)), testSet[, IV], type = "prob")$Y)
if(ml == "glm"){testSet$OOF_pred_glm = eval(sym(OOF_prediction))}
if(ml == "nb"){testSet$OOF_pred_nb = eval(sym(OOF_prediction))}
if(ml == "rf"){testSet$OOF_pred_rf = eval(sym(OOF_prediction))}
# Y/N predictions for Confusion Matrix - testSet
assign(bquote(.(prediction)), predict(eval(sym(model)), testSet[, IV]))
if(ml == "glm"){testSet$pred_glm = eval(sym(prediction))}
if(ml == "nb"){testSet$pred_nb = eval(sym(prediction))}
if(ml == "rf"){testSet$pred_rf = eval(sym(prediction))}
# output
assign(bquote(.(result)), broom::tidy(caret::confusionMatrix(testSet[, prediction], testSet[, DV])) %>%
dplyr::mutate(trainMethod = ml) %>%
dplyr::select(trainMethod, everything()))
# store output into a list
tempModelList <- list(eval(sym(result)))
modelSummaryList <<- c(modelSummaryList, tempModelList)
}
# set seed
set.seed(1234)
# methods
baseTrainMethods <- c("glm", "nb", "rf")
topTrainMethods <- c("glm", "nb", "gbm")
# output
modelSummaryList <- vector(mode = "list")
# train base layer
for(baseLayer in baseTrainMethods){
# set parameters
ml = baseLayer
model = paste0("model_base_", ml)
OOF_prediction = paste0("OOF_pred_", ml)
prediction = paste0("pred_", ml)
result = paste0("result_", ml)
# model
assign(bquote(.(model)), caret::train(trainSet[, IV], trainSet[, DV],
method = ml,
trControl = train.control,
trace = FALSE))
# Out-Of-Fold probability predictions - trainSet
if(ml == "glm"){trainSet$OOF_pred_glm = eval(sym(model))$pred$Y[order(eval(sym(model))$pred$rowIndex)]}
if(ml == "nb"){trainSet$OOF_pred_nb = eval(sym(model))$pred$Y[order(eval(sym(model))$pred$rowIndex)]}
if(ml == "rf"){trainSet$OOF_pred_rf = eval(sym(model))$pred$Y[order(eval(sym(model))$pred$rowIndex)]}
# Out-Of-Fold probability predictions - testSet
assign(bquote(.(OOF_prediction)), predict(eval(sym(model)), testSet[, IV], type = "prob")$Y)
if(ml == "glm"){testSet$OOF_pred_glm = eval(sym(OOF_prediction))}
if(ml == "nb"){testSet$OOF_pred_nb = eval(sym(OOF_prediction))}
if(ml == "rf"){testSet$OOF_pred_rf = eval(sym(OOF_prediction))}
# Y/N predictions for Confusion Matrix - testSet
assign(bquote(.(prediction)), predict(eval(sym(model)), testSet[, IV]))
if(ml == "glm"){testSet$pred_glm = eval(sym(prediction))}
if(ml == "nb"){testSet$pred_nb = eval(sym(prediction))}
if(ml == "rf"){testSet$pred_rf = eval(sym(prediction))}
# output
assign(bquote(.(result)), broom::tidy(caret::confusionMatrix(testSet[, prediction], testSet[, DV])) %>%
dplyr::mutate(trainMethod = ml) %>%
dplyr::select(trainMethod, everything()))
# store output into a list
tempModelList <- list(eval(sym(result)))
modelSummaryList <<- c(modelSummaryList, tempModelList)
}
# train top layer
for(topLayer in topTrainMethods){
# set parameters
ml = topLayer
model = paste0("model_top_", ml)
OOF_predictors_top = c("OOF_pred_glm", "OOF_pred_nb", "OOF_pred_rf")
OOF_prediction_top = paste0("OOF_pred_top_", ml)
prediction_top = paste0("pred_top_", ml)
result = paste0("result_top_", ml)
# model
assign(bquote(.(model)), caret::train(trainSet[, OOF_predictors_top], trainSet[, DV],
method = ml,
trControl = train.control))
# Out-Of-Fold probability predictions - testSet
assign(bquote(.(OOF_prediction_top)), predict(eval(sym(model)), testSet[, OOF_predictors_top], type = "prob")$Y)
if(ml == "glm"){testSet$OOF_pred_top_glm = eval(sym(OOF_prediction_top))}
if(ml == "nb"){testSet$OOF_pred_top_nb = eval(sym(OOF_prediction_top))}
if(ml == "gbm"){testSet$OOF_pred_top_gbm = eval(sym(OOF_prediction_top))}
# Y/N predictions for Confusion Matrix - testSet
assign(bquote(.(prediction_top)), predict(eval(sym(model)), testSet[, OOF_predictors_top]))
if(ml == "glm"){testSet$pred_top_glm = eval(sym(prediction_top))}
if(ml == "nb"){testSet$pred_top_nb = eval(sym(prediction_top))}
if(ml == "gbm"){testSet$pred_top_gbm = eval(sym(prediction_top))}
# output
assign(bquote(.(result)), broom::tidy(caret::confusionMatrix(testSet[, prediction_top], testSet[, DV])) %>%
dplyr::mutate(trainMethod = paste0(ml, " - top layer")) %>%
dplyr::select(trainMethod, everything()))
# store output into a list
tempModelList <- list(eval(sym(result)))
modelSummaryList <<- c(modelSummaryList, tempModelList)
}
# put together - averaging the OOF prediction probability
testSet <- testSet %>%
dplyr::mutate(pred_final_prob_avg = (OOF_pred_top_glm + OOF_pred_top_nb + OOF_pred_top_gbm) / length(topTrainMethods),
pred_final = ifelse(pred_final_prob_avg > 0.5, "Y", "N") %>%
factor(., levels = c("Y", "N"), labels = c("Yes", "No")))
finalResult <- broom::tidy(caret::confusionMatrix(testSet$pred_final, testSet$loan_status)) %>%
dplyr::mutate(trainMethod = "final - averaging") %>%
dplyr::select(trainMethod, everything())
# store finalResult output into a list
tempModelList <- list(finalResult)
modelSummaryList <<- c(modelSummaryList, tempModelList)
modelSummaryList
# let's look at the result
modelSummaryDf <- modelSummaryList %>%
dplyr::bind_rows() %>%
dplyr::select(trainMethod, term, estimate) %>%
tidyr::spread(term, estimate) %>%
arrange(desc(f1))
modelSummaryDf %>% kable()
# let's look at the result
modelSummaryDf <- modelSummaryList %>%
dplyr::bind_rows() %>%
dplyr::select(trainMethod, term, estimate) %>%
tidyr::spread(term, estimate) %>%
arrange(desc(f1))
modelSummaryDf %>% kable()
knitr::opts_chunk$set(echo = TRUE)
# load packages
if(!require(pacman)){install.packages("pacman"); require(pacman)}
packages <- c('tidyverse', 'caret', 'broom', 'Hmisc', 'kableExtra', 'janitor')
pacman::p_load(char = packages)
# read data
df <- read.csv(url('https://datahack-prod.s3.ap-south-1.amazonaws.com/train_file/train_u6lujuX_CVtuZ9i.csv'))
# check missing
tempDf <- data.frame(fields = names(df),
missing = colSums(is.na(df)))
tempDf
# impute dataset
preProcValues <- preProcess(df, method = c("medianImpute", "center", "scale"))
dfProcessed <- predict(preProcValues, df)
# check again
tempDf2 <- data.frame(fields = names(dfProcessed),
missing = colSums(is.na(dfProcessed)))
tempDf2
# clean names
dfProcessed <- janitor::clean_names(dfProcessed)
names(dfProcessed)
# check distribution
broom::glance(dfProcessed)
table(dfProcessed$loan_status)
prop.table(ftable(dfProcessed$loan_status)) %>% round(., 1)
# head df
head(dfProcessed) %>% kable(.)
# str(), dim()
str(dfProcessed)
dim(dfProcessed)
# make changes to the "loan_status" level, label
levels(dfProcessed$loan_status)
dfProcessed <- dfProcessed %>%
dplyr::mutate(loan_status = loan_status %>%
factor(., levels = c("Y", "N"), labels = c("Yes", "No")))
levels(dfProcessed$loan_status)
# set seed
set.seed(1234)
# split data
index <- caret::createDataPartition(dfProcessed$loan_status, p = 0.7, list = FALSE)
trainSet <- dfProcessed[index, ]
testSet <- dfProcessed[-index, ]
# define the training control - let's do 10-fold cross-validation
train.control <- caret::trainControl(
method = "cv",
number = 10,
savePredictions = 'final',
classProbs = TRUE)
# define IVs, DV
IV <- names(dfProcessed)[names(dfProcessed) %nin% c("loan_id", "loan_status")]; IV
DV <- "loan_status"
# set seed
set.seed(1234)
# methods
baseTrainMethods <- c("glm", "nb", "rf")
topTrainMethods <- c("glm", "nb", "gbm")
# output
modelSummaryList <- vector(mode = "list")
# train base layer
for(baseLayer in baseTrainMethods){
# set parameters
ml = baseLayer
model = paste0("model_base_", ml)
OOF_prediction = paste0("OOF_pred_", ml)
prediction = paste0("pred_", ml)
result = paste0("result_", ml)
# model
assign(bquote(.(model)), caret::train(trainSet[, IV], trainSet[, DV],
method = ml,
trControl = train.control,
trace = FALSE))
# Out-Of-Fold probability predictions - trainSet
if(ml == "glm"){trainSet$OOF_pred_glm = eval(sym(model))$pred$Y[order(eval(sym(model))$pred$rowIndex)]}
if(ml == "nb"){trainSet$OOF_pred_nb = eval(sym(model))$pred$Y[order(eval(sym(model))$pred$rowIndex)]}
if(ml == "rf"){trainSet$OOF_pred_rf = eval(sym(model))$pred$Y[order(eval(sym(model))$pred$rowIndex)]}
# Out-Of-Fold probability predictions - testSet
assign(bquote(.(OOF_prediction)), predict(eval(sym(model)), testSet[, IV], type = "prob")$Y)
if(ml == "glm"){testSet$OOF_pred_glm = eval(sym(OOF_prediction))}
if(ml == "nb"){testSet$OOF_pred_nb = eval(sym(OOF_prediction))}
if(ml == "rf"){testSet$OOF_pred_rf = eval(sym(OOF_prediction))}
# Y/N predictions for Confusion Matrix - testSet
assign(bquote(.(prediction)), predict(eval(sym(model)), testSet[, IV]))
if(ml == "glm"){testSet$pred_glm = eval(sym(prediction))}
if(ml == "nb"){testSet$pred_nb = eval(sym(prediction))}
if(ml == "rf"){testSet$pred_rf = eval(sym(prediction))}
# output
assign(bquote(.(result)), broom::tidy(caret::confusionMatrix(testSet[, prediction], testSet[, DV])) %>%
dplyr::mutate(trainMethod = ml) %>%
dplyr::select(trainMethod, everything()))
# store output into a list
tempModelList <- list(eval(sym(result)))
modelSummaryList <<- c(modelSummaryList, tempModelList)
}
# train top layer
for(topLayer in topTrainMethods){
# set parameters
ml = topLayer
model = paste0("model_top_", ml)
OOF_predictors_top = c("OOF_pred_glm", "OOF_pred_nb", "OOF_pred_rf")
OOF_prediction_top = paste0("OOF_pred_top_", ml)
prediction_top = paste0("pred_top_", ml)
result = paste0("result_top_", ml)
# model
assign(bquote(.(model)), caret::train(trainSet[, OOF_predictors_top], trainSet[, DV],
method = ml,
trControl = train.control))
# Out-Of-Fold probability predictions - testSet
assign(bquote(.(OOF_prediction_top)), predict(eval(sym(model)), testSet[, OOF_predictors_top], type = "prob")$Y)
if(ml == "glm"){testSet$OOF_pred_top_glm = eval(sym(OOF_prediction_top))}
if(ml == "nb"){testSet$OOF_pred_top_nb = eval(sym(OOF_prediction_top))}
if(ml == "gbm"){testSet$OOF_pred_top_gbm = eval(sym(OOF_prediction_top))}
# Y/N predictions for Confusion Matrix - testSet
assign(bquote(.(prediction_top)), predict(eval(sym(model)), testSet[, OOF_predictors_top]))
if(ml == "glm"){testSet$pred_top_glm = eval(sym(prediction_top))}
if(ml == "nb"){testSet$pred_top_nb = eval(sym(prediction_top))}
if(ml == "gbm"){testSet$pred_top_gbm = eval(sym(prediction_top))}
# output
assign(bquote(.(result)), broom::tidy(caret::confusionMatrix(testSet[, prediction_top], testSet[, DV])) %>%
dplyr::mutate(trainMethod = paste0(ml, " - top layer")) %>%
dplyr::select(trainMethod, everything()))
# store output into a list
tempModelList <- list(eval(sym(result)))
modelSummaryList <<- c(modelSummaryList, tempModelList)
}
# put together - averaging the OOF prediction probability
testSet <- testSet %>%
dplyr::mutate(pred_final_prob_avg = (OOF_pred_top_glm + OOF_pred_top_nb + OOF_pred_top_gbm) / length(topTrainMethods),
pred_final = ifelse(pred_final_prob_avg > 0.5, "Y", "N") %>%
factor(., levels = c("Y", "N"), labels = c("Yes", "No")))
finalResult <- broom::tidy(caret::confusionMatrix(testSet$pred_final, testSet$loan_status)) %>%
dplyr::mutate(trainMethod = "final - averaging") %>%
dplyr::select(trainMethod, everything())
# store finalResult output into a list
tempModelList <- list(finalResult)
modelSummaryList <<- c(modelSummaryList, tempModelList)
# let's look at the result
modelSummaryDf <- modelSummaryList %>%
dplyr::bind_rows() %>%
dplyr::select(trainMethod, term, estimate) %>%
tidyr::spread(term, estimate) %>%
arrange(desc(f1))
modelSummaryDf %>% kable()
update.packages("knitr")
update.packages("rmarkdown")
rm(list = ls())
update.packages("knitr")
update.packages("rmarkdown")
setwd("~/school_of_professional_studies/621. Business Analytics and Data Mining/blog/4. time series analysis")
rm(list = ls())
