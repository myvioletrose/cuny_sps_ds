# sen <- InformationValue::sensitivity(test$target, predictions[i], threshold = optCutOff)
# spe <- InformationValue::specificity(test$target, predictions[i], threshold = optCutOff)
# # diagnostic output
# tempList <- list(
#         data.frame(model = names(modelResultList)[i],
#                    misClass = misCE,
#                    auroc = auroc,
#                    precision = pre,
#                    sensitivity = sen,
#                    specificity = spe)
# )
#
# # put together
# modelResultList <- c(modelResultList, tempList)
}
misCE
class(predict(fullModel, test, type = "response"))
class(predictions[[2]])
modelResultList <- vector(mode = "list", length = 3)
names(modelResultList) <- c("full", "full - sqrt transformed", "stepwise")
for(i in 1:length(modelResultList)){
# metrics
auroc <- InformationValue::AUROC(test$target, predictions[[i]] %>% unlist)
misCE <- InformationValue::misClassError(test$target, predictions[[i]] %>% unlist, threshold = optCutOff)
pre <- InformationValue::precision(test$target, predictions[[i]] %>% unlist, threshold = optCutOff)
sen <- InformationValue::sensitivity(test$target, predictions[[i]] %>% unlist, threshold = optCutOff)
spe <- InformationValue::specificity(test$target, predictions[[i]] %>% unlist, threshold = optCutOff)
# diagnostic output
tempList <- list(
data.frame(model = names(modelResultList)[i],
misClass = misCE,
auroc = auroc,
precision = pre,
sensitivity = sen,
specificity = spe)
)
# put together
modelResultList <- c(modelResultList, tempList)
}
modelResultDf <- modelResultList %>% dplyr::bind_rows()
modelResultDf
# set threshold
optCutOff <- 0.5
# prediction for each model
predicted_1 <- list(predict(fullModel, test, type = "response"))
predicted_2 <- list(predict(fullModelSqrt, test, type = "response"))
predicted_3 <- list(predict(stepModel, test, type = "response"))
predicted_4 <- list(predict(kFoldModel, test, type = "raw"))
predictions <- list(predicted_1, predicted_2, predicted_3, predicted_4)
# diagnostic - Confusion Matrix, i.e. the columns are actuals, while rows are predicteds
cm_1 <- InformationValue::confusionMatrix(test$target, predicted = predicted_1 %>% unlist, threshold = optCutOff)
cm_2 <- InformationValue::confusionMatrix(test$target, predicted = predicted_2 %>% unlist, threshold = optCutOff)
cm_3 <- InformationValue::confusionMatrix(test$target, predicted = predicted_3 %>% unlist, threshold = optCutOff)
cm_4 <- ftable(predicted_4 %>% unlist, test$target)
cm_1
cm_2
cm_3
cm_4
# diagnostic - AUROC, misClassification, precision, sensitivity, specificity
modelResultList <- vector(mode = "list", length = 3)
names(modelResultList) <- c("full", "full - sqrt transformed", "stepwise")
for(i in 1:length(modelResultList)){
# metrics
auroc <- InformationValue::AUROC(test$target, predictions[[i]] %>% unlist)
misCE <- InformationValue::misClassError(test$target, predictions[[i]] %>% unlist, threshold = optCutOff)
pre <- InformationValue::precision(test$target, predictions[[i]] %>% unlist, threshold = optCutOff)
sen <- InformationValue::sensitivity(test$target, predictions[[i]] %>% unlist, threshold = optCutOff)
spe <- InformationValue::specificity(test$target, predictions[[i]] %>% unlist, threshold = optCutOff)
# diagnostic output
tempList <- list(
data.frame(model = names(modelResultList)[i],
auroc = round(auroc, 2)
misClass = round(misCE, 2)
precision = round(pre, 2)
sensitivity = round(sen, 2)
specificity = round(spe, 2))
)
# put together
modelResultList <- c(modelResultList, tempList)
}
modelResultDf <- modelResultList %>% dplyr::bind_rows()
# set threshold
optCutOff <- 0.5
# prediction for each model
predicted_1 <- list(predict(fullModel, test, type = "response"))
predicted_2 <- list(predict(fullModelSqrt, test, type = "response"))
predicted_3 <- list(predict(stepModel, test, type = "response"))
predicted_4 <- list(predict(kFoldModel, test, type = "raw"))
predictions <- list(predicted_1, predicted_2, predicted_3, predicted_4)
# diagnostic - Confusion Matrix, i.e. the columns are actuals, while rows are predicteds
cm_1 <- InformationValue::confusionMatrix(test$target, predicted = predicted_1 %>% unlist, threshold = optCutOff)
cm_2 <- InformationValue::confusionMatrix(test$target, predicted = predicted_2 %>% unlist, threshold = optCutOff)
cm_3 <- InformationValue::confusionMatrix(test$target, predicted = predicted_3 %>% unlist, threshold = optCutOff)
cm_4 <- ftable(predicted_4 %>% unlist, test$target)
cm_1
cm_2
cm_3
cm_4
# diagnostic - AUROC, misClassification, precision, sensitivity, specificity
modelResultList <- vector(mode = "list", length = 3)
names(modelResultList) <- c("full", "full - sqrt transformed", "stepwise")
for(i in 1:length(modelResultList)){
# metrics
auroc <- InformationValue::AUROC(test$target, predictions[[i]] %>% unlist)
misCE <- InformationValue::misClassError(test$target, predictions[[i]] %>% unlist, threshold = optCutOff)
pre <- InformationValue::precision(test$target, predictions[[i]] %>% unlist, threshold = optCutOff)
sen <- InformationValue::sensitivity(test$target, predictions[[i]] %>% unlist, threshold = optCutOff)
spe <- InformationValue::specificity(test$target, predictions[[i]] %>% unlist, threshold = optCutOff)
# diagnostic output
tempList <- list(
data.frame(model = names(modelResultList)[i],
auroc = round(auroc, 2),
misClass = round(misCE, 2),
precision = round(pre, 2),
sensitivity = round(sen, 2),
specificity = round(spe, 2))
)
# put together
modelResultList <- c(modelResultList, tempList)
}
modelResultDf <- modelResultList %>% dplyr::bind_rows()
modelResultDf
?accuracy
summary(fullModel)
names(fullModel)
?performance
library(ROCR)
install.packages("ROCR")
library(ROCR)
performance(predicted_1 %>% unlist, "f")
ls(pos = "package:InformationValue")
# set threshold
optCutOff <- 0.5
# prediction for each model
predicted_1 <- list(predict(fullModel, test, type = "response"))
predicted_2 <- list(predict(fullModelSqrt, test, type = "response"))
predicted_3 <- list(predict(stepModel, test, type = "response"))
predicted_4 <- list(predict(kFoldModel, test, type = "raw"))
predictions <- list(predicted_1, predicted_2, predicted_3, predicted_4)
# diagnostic - Confusion Matrix, i.e. the columns are actuals, while rows are predicteds
cm_1 <- InformationValue::confusionMatrix(test$target, predicted = predicted_1 %>% unlist, threshold = optCutOff)
cm_2 <- InformationValue::confusionMatrix(test$target, predicted = predicted_2 %>% unlist, threshold = optCutOff)
cm_3 <- InformationValue::confusionMatrix(test$target, predicted = predicted_3 %>% unlist, threshold = optCutOff)
cm_4 <- ftable(predicted_4 %>% unlist, test$target)
cm_1
cm_2
cm_3
cm_4
# diagnostic - AUROC, misClassification, precision, sensitivity, specificity
modelResultList <- vector(mode = "list", length = 3)
names(modelResultList) <- c("full", "full - sqrt transformed", "stepwise")
for(i in 1:length(modelResultList)){
# metrics
auroc <- InformationValue::AUROC(test$target, predictions[[i]] %>% unlist)
misCE <- InformationValue::misClassError(test$target, predictions[[i]] %>% unlist, threshold = optCutOff)
pre <- InformationValue::precision(test$target, predictions[[i]] %>% unlist, threshold = optCutOff)
sen <- InformationValue::sensitivity(test$target, predictions[[i]] %>% unlist, threshold = optCutOff)
spe <- InformationValue::specificity(test$target, predictions[[i]] %>% unlist, threshold = optCutOff)
cm <- InformationValue::confusionMatrix(test$target, predictions[[i]] %>% unlist, threshold = optCutOff)
TN <- cm[1, 1]
FN <- cm[1, 2]
FP <- cm[2, 1]
TP <- cm[2, 2]
# diagnostic output
tempList <- list(
data.frame(model = names(modelResultList)[i],
auroc = round(auroc, 2),
misClass = round(misCE, 2),
precision = round(pre, 2),
sensitivity = round(sen, 2),
specificity = round(spe, 2),
`True Positive` = TP,
`True Negative` = TN,
`False Positive` = FP,
`False Negative` = FN,
accuracy = (TP + TN) / (TP + TN + FP + FN),
F1 = (2 * precision * sensitivity) / (precision + sensitivity))
)
# put together
modelResultList <- c(modelResultList, tempList)
}
modelResultDf <- modelResultList %>% dplyr::bind_rows()
modelResultDf
# diagnostic - AUROC, misClassification, precision, sensitivity, specificity
modelResultList <- vector(mode = "list", length = 3)
names(modelResultList) <- c("full", "full - sqrt transformed", "stepwise")
for(i in 1:length(modelResultList)){
# metrics
auroc <- InformationValue::AUROC(test$target, predictions[[i]] %>% unlist)
misCE <- InformationValue::misClassError(test$target, predictions[[i]] %>% unlist, threshold = optCutOff)
pre <- InformationValue::precision(test$target, predictions[[i]] %>% unlist, threshold = optCutOff)
sen <- InformationValue::sensitivity(test$target, predictions[[i]] %>% unlist, threshold = optCutOff)
spe <- InformationValue::specificity(test$target, predictions[[i]] %>% unlist, threshold = optCutOff)
cm <- InformationValue::confusionMatrix(test$target, predictions[[i]] %>% unlist, threshold = optCutOff)
TN <- cm[1, 1]
FN <- cm[1, 2]
FP <- cm[2, 1]
TP <- cm[2, 2]
# diagnostic output
tempList <- list(
data.frame(model = names(modelResultList)[i],
auroc = round(auroc, 2),
misClass = round(misCE, 2),
precision = round(pre, 2),
sensitivity = round(sen, 2),
specificity = round(spe, 2),
`True Positive` = TP,
`True Negative` = TN,
`False Positive` = FP,
`False Negative` = FN,
accuracy = (TP + TN) / (TP + TN + FP + FN),
F1 = 2 * ((precision * sensitivity) / (precision + sensitivity)))
)
# put together
modelResultList <- c(modelResultList, tempList)
}
modelResultDf <- modelResultList %>% dplyr::bind_rows()
modelResultDf
# set threshold
optCutOff <- 0.5
# prediction for each model
predicted_1 <- list(predict(fullModel, test, type = "response"))
predicted_2 <- list(predict(fullModelSqrt, test, type = "response"))
predicted_3 <- list(predict(stepModel, test, type = "response"))
predicted_4 <- list(predict(kFoldModel, test, type = "raw"))
predictions <- list(predicted_1, predicted_2, predicted_3, predicted_4)
# diagnostic - Confusion Matrix, i.e. the columns are actuals, while rows are predicteds
cm_1 <- InformationValue::confusionMatrix(test$target, predicted = predicted_1 %>% unlist, threshold = optCutOff)
cm_2 <- InformationValue::confusionMatrix(test$target, predicted = predicted_2 %>% unlist, threshold = optCutOff)
cm_3 <- InformationValue::confusionMatrix(test$target, predicted = predicted_3 %>% unlist, threshold = optCutOff)
cm_4 <- ftable(predicted_4 %>% unlist, test$target)
cm_1
cm_2
cm_3
cm_4
# diagnostic - AUROC, misClassification, precision, sensitivity, specificity
modelResultList <- vector(mode = "list", length = 3)
names(modelResultList) <- c("full", "full - sqrt transformed", "stepwise")
for(i in 1:length(modelResultList)){
# metrics
auroc <- InformationValue::AUROC(test$target, predictions[[i]] %>% unlist)
misCE <- InformationValue::misClassError(test$target, predictions[[i]] %>% unlist, threshold = optCutOff)
pre <- InformationValue::precision(test$target, predictions[[i]] %>% unlist, threshold = optCutOff)
sen <- InformationValue::sensitivity(test$target, predictions[[i]] %>% unlist, threshold = optCutOff)
spe <- InformationValue::specificity(test$target, predictions[[i]] %>% unlist, threshold = optCutOff)
cm <- InformationValue::confusionMatrix(test$target, predictions[[i]] %>% unlist, threshold = optCutOff)
TN <- cm[1, 1]
FN <- cm[1, 2]
FP <- cm[2, 1]
TP <- cm[2, 2]
# diagnostic output
tempList <- list(
data.frame(model = names(modelResultList)[i],
auroc = round(auroc, 2),
misClass = round(misCE, 2),
precision = round(pre, 2),
sensitivity = round(sen, 2),
specificity = round(spe, 2),
`True Positive` = TP,
`True Negative` = TN,
`False Positive` = FP,
`False Negative` = FN,
accuracy = round((TP + TN) / (TP + TN + FP + FN), 2),
F1 = round(2 * ((precision * sensitivity) / (precision + sensitivity)), 2))
)
# put together
modelResultList <- c(modelResultList, tempList)
}
modelResultDf <- modelResultList %>% dplyr::bind_rows()
modelResultDf %>% kableExtra::kable()
install.packages("MLeval")
library(MLeval)
library(MLeval)
res <- evalm(list(fullModel, fullModelSqrt, stepModel),
gnames = c("full", "full - sqrt transformed", "stepwise"))
res <- evalm(list(fullModel, fullModelSqrt, stepModel))
?evalm
fullModelSqrt
fullModel
stepModel
pred <- prediction(test$target, predicted_1 %>% unlist)
perf <- performance(pred, "tpr", "fpr")
plot(perf, colorize=TRUE)
pred <- prediction(test$target, predicted_1 %>% unlist)
?prediction
pred <- prediction(predicted_1 %>% unlist, test$target)
perf <- performance(pred, "tpr", "fpr")
plot(perf, colorize=TRUE)
pred <- prediction(predicted_2 %>% unlist, test$target)
perf <- performance(pred, "tpr", "fpr")
plot(perf, colorize = TRUE, main = "Full Model - Sqrt")
windows()
par(mfrow = c(1, 3))
pred <- prediction(predicted_1 %>% unlist, test$target)
perf <- performance(pred, "tpr", "fpr")
plot(perf, colorize = TRUE, main = "Full Model")
pred <- prediction(predicted_2 %>% unlist, test$target)
perf <- performance(pred, "tpr", "fpr")
plot(perf, colorize = TRUE, main = "Full Model - Sqrt")
pred <- prediction(predicted_3 %>% unlist, test$target)
perf <- performance(pred, "tpr", "fpr")
plot(perf, colorize = TRUE, main = "Step Model")
dfEval$prediction = ifelse(predicted_3 %>% unlist > 0.5, 1, 0)
dfEval <- dfEval %>%
dplyr::mutate(prediction = predict(stepModel, dfEval, type = "response"),
prediction = ifelse(prediction > optCutOff, 1, 0))
table(dfEval$prediction)
modelResultDf
stepModel
tidy(stepModel)
# set threshold
optCutOff <- 0.5
# prediction for each model
predicted_1 <- list(predict(fullModel, test, type = "response"))
predicted_2 <- list(predict(fullModelSqrt, test, type = "response"))
predicted_3 <- list(predict(stepModel, test, type = "response"))
predicted_4 <- list(predict(kFoldModel, test, type = "raw"))
predictions <- list(predicted_1, predicted_2, predicted_3, predicted_4)
# diagnostic - Confusion Matrix, i.e. the columns are actuals, while rows are predicteds
cm_1 <- InformationValue::confusionMatrix(test$target, predicted = predicted_1 %>% unlist, threshold = optCutOff)
cm_2 <- InformationValue::confusionMatrix(test$target, predicted = predicted_2 %>% unlist, threshold = optCutOff)
cm_3 <- InformationValue::confusionMatrix(test$target, predicted = predicted_3 %>% unlist, threshold = optCutOff)
cm_4 <- ftable(predicted_4 %>% unlist, test$target)
cm_1
cm_2
cm_3
cm_4
# diagnostic - AUROC, misClassification, precision, sensitivity, specificity
modelResultList <- vector(mode = "list", length = 3)
names(modelResultList) <- c("full", "full - sqrt transformed", "stepwise")
for(i in 1:length(modelResultList)){
# metrics
auroc <- InformationValue::AUROC(test$target, predictions[[i]] %>% unlist)
misCE <- InformationValue::misClassError(test$target, predictions[[i]] %>% unlist, threshold = optCutOff)
pre <- InformationValue::precision(test$target, predictions[[i]] %>% unlist, threshold = optCutOff)
sen <- InformationValue::sensitivity(test$target, predictions[[i]] %>% unlist, threshold = optCutOff)
spe <- InformationValue::specificity(test$target, predictions[[i]] %>% unlist, threshold = optCutOff)
cm <- InformationValue::confusionMatrix(test$target, predictions[[i]] %>% unlist, threshold = optCutOff)
TN <- cm[1, 1]
FN <- cm[1, 2]
FP <- cm[2, 1]
TP <- cm[2, 2]
# diagnostic output
tempList <- list(
data.frame(model = names(modelResultList)[i],
auroc = round(auroc, 2),
misClass = round(misCE, 2),
precision = round(pre, 2),
sensitivity = round(sen, 2),
specificity = round(spe, 2),
`True Positive` = TP,
`True Negative` = TN,
`False Positive` = FP,
`False Negative` = FN,
accuracy = round((TP + TN) / (TP + TN + FP + FN), 2),
F1 = round(2 * ((precision * sensitivity) / (precision + sensitivity)), 2))
)
# put together
modelResultList <- c(modelResultList, tempList)
}
modelResultDf <- modelResultList %>% dplyr::bind_rows()
modelResultDf %>% kableExtra::kable()
# ROC Curve
par(mfrow = c(1, 3))
pred <- prediction(predicted_1 %>% unlist, test$target)
perf <- performance(pred, "tpr", "fpr")
plot(perf, colorize = TRUE, main = "Full Model")
pred <- prediction(predicted_2 %>% unlist, test$target)
perf <- performance(pred, "tpr", "fpr")
plot(perf, colorize = TRUE, main = "Full Model - Sqrt")
pred <- prediction(predicted_3 %>% unlist, test$target)
perf <- performance(pred, "tpr", "fpr")
plot(perf, colorize = TRUE, main = "Step Model")
# Final print of the stepModel
tidy(stepModel)
# Prediction for Evaluation Set
dfEval <- dfEval %>%
dplyr::mutate(prediction = predict(stepModel, dfEval, type = "response"),
prediction = ifelse(prediction > optCutOff, 1, 0))
table(dfEval$prediction)
rm(list = ls())
knitr::opts_chunk$set(echo = TRUE)
# load packages
if(!require(pacman)){install.packages("pacman"); require(pacman)}
packages <- c('tidyverse', 'glue', 'broom', 'MASS', 'caret', 'InformationValue', 'Hmisc', 'kableExtra', 'corrplot', 'ROCR')
pacman::p_load(char = packages)
# get data
dfTrain <- read_csv("crime-training-data_modified.csv")
dfEval <- read_csv("crime-evaluation-data_modified.csv")
dim(dfTrain)
head(dfTrain)
summary(dfTrain)
str(dfTrain)
# number of unique values for each variable
sapply(dfTrain, function(x) unique(x) %>% length)
# any missing
colSums(is.na(dfTrain))
# let's look at distribution of each variable by target variable
categorical_vars <- c("chas", "rad")
quantitative_vars <- names(dfTrain)[names(dfTrain) %nin% categorical_vars]
dfGather <- dfTrain %>%
dplyr::select(all_of(quantitative_vars)) %>%
dplyr::mutate(target = as.factor(target)) %>%
tidyr::gather(key, value, -target)
ggplot(dfGather, aes(value, color = target)) +
geom_density() +
geom_vline(data = aggregate(value ~ target + key, dfGather, median),
aes(xintercept = value,
color = target),
linetype = "dashed") +
facet_wrap(~ key, nrow = 5, scales = "free")
# let's look at other categorical variables
with(dfTrain, ftable(target) %>% prop.table()) %>% round(., 2)
with(dfTrain, ftable(target, chas) %>% prop.table(., margin = 2) %>% round(., 2))
with(dfTrain, ftable(target, rad) %>% prop.table(., margin = 2) %>% round(., 2))
dfTrain %>%
cor() %>%
corrplot(method = "number", type = "upper", order = "hclust")
# split dfTrain into train and test sets
set.seed(1234)
index <- sample(1:nrow(dfTrain), nrow(dfTrain) * 0.7)
train <- dfTrain[index, ]
test <- dfTrain[-index, ]
# model 1 - full model
fullModel <- glm(target ~ ., data = train, family = binomial(link = "logit"))
broom::glance(fullModel)
# model 2 - sqrt transformation for taking care of outliners
DV <- names(dfTrain)[names(train) %nin% c(categorical_vars, 'target')]
fullModelSqrt <- glm(target ~ ., data = train %>% dplyr::mutate_at(vars(DV), sqrt), family = binomial(link = "logit"))
broom::glance(fullModelSqrt)
# model 3 - stepwise (direction default to both)
step <- MASS::stepAIC(fullModel, trace = FALSE)
step$anova
stepModel <- glm(target ~ zn + nox + dis + rad + tax + ptratio + lstat + medv, data = train, family = binomial(link = "logit"))
broom::glance(stepModel)
# model 4 - repeated, k-fold cross-validation
set.seed(1234)
train.control <- caret::trainControl(method = "repeatedcv", number = 10, repeats = 10)
index <- sample(1:nrow(dfTrain), size = nrow(dfTrain) * 0.7)
kFoldModel <- caret::train(as.factor(target) ~ zn + nox + dis + rad + tax + ptratio + lstat + medv,
data = train, method = "glm", trControl = train.control)
summary(kFoldModel)
# set threshold
optCutOff <- 0.5
# prediction for each model
predicted_1 <- list(predict(fullModel, test, type = "response"))
predicted_2 <- list(predict(fullModelSqrt, test, type = "response"))
predicted_3 <- list(predict(stepModel, test, type = "response"))
predicted_4 <- list(predict(kFoldModel, test, type = "raw"))
predictions <- list(predicted_1, predicted_2, predicted_3, predicted_4)
# diagnostic - Confusion Matrix, i.e. the columns are actuals, while rows are predicteds
cm_1 <- InformationValue::confusionMatrix(test$target, predicted = predicted_1 %>% unlist, threshold = optCutOff)
cm_2 <- InformationValue::confusionMatrix(test$target, predicted = predicted_2 %>% unlist, threshold = optCutOff)
cm_3 <- InformationValue::confusionMatrix(test$target, predicted = predicted_3 %>% unlist, threshold = optCutOff)
cm_4 <- ftable(predicted_4 %>% unlist, test$target)
cm_1
cm_2
cm_3
cm_4
# diagnostic - AUROC, misClassification, precision, sensitivity, specificity
modelResultList <- vector(mode = "list", length = 3)
names(modelResultList) <- c("full", "full - sqrt transformed", "stepwise")
for(i in 1:length(modelResultList)){
# metrics
auroc <- InformationValue::AUROC(test$target, predictions[[i]] %>% unlist)
misCE <- InformationValue::misClassError(test$target, predictions[[i]] %>% unlist, threshold = optCutOff)
pre <- InformationValue::precision(test$target, predictions[[i]] %>% unlist, threshold = optCutOff)
sen <- InformationValue::sensitivity(test$target, predictions[[i]] %>% unlist, threshold = optCutOff)
spe <- InformationValue::specificity(test$target, predictions[[i]] %>% unlist, threshold = optCutOff)
cm <- InformationValue::confusionMatrix(test$target, predictions[[i]] %>% unlist, threshold = optCutOff)
TN <- cm[1, 1]
FN <- cm[1, 2]
FP <- cm[2, 1]
TP <- cm[2, 2]
# diagnostic output
tempList <- list(
data.frame(model = names(modelResultList)[i],
auroc = round(auroc, 2),
misClass = round(misCE, 2),
precision = round(pre, 2),
sensitivity = round(sen, 2),
specificity = round(spe, 2),
`True Positive` = TP,
`True Negative` = TN,
`False Positive` = FP,
`False Negative` = FN,
accuracy = round((TP + TN) / (TP + TN + FP + FN), 2),
F1 = round(2 * ((pre * sen) / (pre + sen)), 2))
)
# put together
modelResultList <- c(modelResultList, tempList)
}
modelResultDf <- modelResultList %>% dplyr::bind_rows()
modelResultDf %>% kableExtra::kable()
# ROC Curve
par(mfrow = c(1, 3))
pred <- prediction(predicted_1 %>% unlist, test$target)
perf <- performance(pred, "tpr", "fpr")
plot(perf, colorize = TRUE, main = "Full Model")
pred <- prediction(predicted_2 %>% unlist, test$target)
perf <- performance(pred, "tpr", "fpr")
plot(perf, colorize = TRUE, main = "Full Model - Sqrt")
pred <- prediction(predicted_3 %>% unlist, test$target)
perf <- performance(pred, "tpr", "fpr")
plot(perf, colorize = TRUE, main = "Step Model")
# Final print of the stepModel
tidy(stepModel)
# Prediction for Evaluation Set
dfEval <- dfEval %>%
dplyr::mutate(prediction = predict(stepModel, dfEval, type = "response"),
prediction = ifelse(prediction > optCutOff, 1, 0))
table(dfEval$prediction)
