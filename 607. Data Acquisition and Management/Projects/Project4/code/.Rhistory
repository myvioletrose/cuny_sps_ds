tidyr::gather(., "measure", "outcome", -classification) %>%
tidyr::spread(measure, outcome)
data.frame(classification = c("Support Vector Machine", "Naive Bayes", "Logistic Regression"),
sensitivity = round(c(sensitivity_svm, sensitivity_nb, sensitivity_lr), 3),
specificity = round(c(specificity_svm, specificity_nb, specificity_lr), 3),
precision = round(c(precision_svm, precision_nb, precision_lr), 3),
total_accuracy = round(c(total_accuracy_svm, total_accuracy_nb, total_accuracy_lr), 3),
stringsAsFactors = F) %>%
tidyr::gather(., "measure", "outcome", -classification)
data.frame(classification = c("Support Vector Machine", "Naive Bayes", "Logistic Regression"),
sensitivity = round(c(sensitivity_svm, sensitivity_nb, sensitivity_lr), 3),
specificity = round(c(specificity_svm, specificity_nb, specificity_lr), 3),
precision = round(c(precision_svm, precision_nb, precision_lr), 3),
total_accuracy = round(c(total_accuracy_svm, total_accuracy_nb, total_accuracy_lr), 3),
stringsAsFactors = F)
##########################################
data.frame(classification = c("Support Vector Machine", "Naive Bayes", "Logistic Regression"),
sensitivity = round(c(sensitivity_svm, sensitivity_nb, sensitivity_lr), 3),
specificity = round(c(specificity_svm, specificity_nb, specificity_lr), 3),
precision = round(c(precision_svm, precision_nb, precision_lr), 3),
total_accuracy = round(c(total_accuracy_svm, total_accuracy_nb, total_accuracy_lr), 3),
stringsAsFactors = F) %>%
tidyr::gather(., "measure", "outcome", -classification) %>%
tidyr::spread(classification, outcome)
##########################################
data.frame(classification = c("Support Vector Machine", "Naive Bayes", "Logistic Regression"),
sensitivity = round(c(sensitivity_svm, sensitivity_nb, sensitivity_lr), 3),
specificity = round(c(specificity_svm, specificity_nb, specificity_lr), 3),
precision = round(c(precision_svm, precision_nb, precision_lr), 3),
total_accuracy = round(c(total_accuracy_svm, total_accuracy_nb, total_accuracy_lr), 3),
stringsAsFactors = F) %>%
tidyr::gather(., "measure", "outcome", -classification) %>%
tidyr::spread(classification, outcome)
head(corpus)
unique(corpus$handle)
reg <- "([^A-Za-z\\d#@']|'(?![A-Za-z\\d#@]))"
clinton <- corpus %>%
dplyr::filter(handle == "HillaryClinton") %>%
dplyr::mutate(text = str_replace_all(text, "https://t.co/[A-Za-z\\d]+|&amp;", "")) %>%
tidytext::unnest_tokens(word, text, token = "regex", pattern = reg) %>%
filter(!word %in% stop_words$word, str_detect(word, "[a-z]"))
head(clinton)
clinton <- corpus %>%
dplyr::filter(handle == "HillaryClinton") %>%
dplyr::select(text) %>%
dplyr::mutate(text = str_replace_all(text, "https://t.co/[A-Za-z\\d]+|&amp;", "")) %>%
tidytext::unnest_tokens(word, text, token = "regex", pattern = reg) %>%
filter(!word %in% stop_words$word, str_detect(word, "[a-z]"))
head(clinton)
plyr::count(clinton$word)
clinton <- corpus %>%
dplyr::filter(handle == "HillaryClinton") %>%
dplyr::select(text) %>%
dplyr::mutate(text = str_replace_all(text, "https://t.co/[A-Za-z\\d]+|&amp;", "")) %>%
tidytext::unnest_tokens(word, text, token = "regex", pattern = reg) %>%
filter(!word %in% stop_words$word, str_detect(word, "[a-z]")) %>%
plyr::count(.$word)
clinton <- corpus %>%
dplyr::filter(handle == "HillaryClinton") %>%
dplyr::select(text) %>%
dplyr::mutate(text = str_replace_all(text, "https://t.co/[A-Za-z\\d]+|&amp;", "")) %>%
tidytext::unnest_tokens(word, text, token = "regex", pattern = reg) %>%
filter(!word %in% stop_words$word, str_detect(word, "[a-z]")) %.>%
plyr::count(.$word)
clinton <- corpus %>%
dplyr::filter(handle == "HillaryClinton") %>%
dplyr::select(text) %>%
dplyr::mutate(text = str_replace_all(text, "https://t.co/[A-Za-z\\d]+|&amp;", "")) %>%
tidytext::unnest_tokens(word, text, token = "regex", pattern = reg) %>%
filter(!word %in% stop_words$word, str_detect(word, "[a-z]")) %.>%
plyr::count(.$word) %>%
arrange(desc(freq)) %>%
top_n(100)
View(clinton)
clinton <- corpus %>%
dplyr::filter(handle == "HillaryClinton") %>%
dplyr::select(text) %>%
dplyr::mutate(text = str_replace_all(text, "https://t.co/[A-Za-z\\d]+|&amp;", "")) %>%
tidytext::unnest_tokens(word, text, token = "regex", pattern = reg) %>%
filter(!word %in% stop_words$word, str_detect(word, "[a-z]")) %.>%
plyr::count(.$word) %>%
arrange(desc(freq)) %>%
top_n(200)
head(clinton)
clinton <- corpus %>%
dplyr::filter(handle == "HillaryClinton") %>%
dplyr::select(text) %>%
dplyr::mutate(text = str_replace_all(text, "https://t.co/[A-Za-z\\d]+|&amp;", "")) %>%
tidytext::unnest_tokens(word, text, token = "regex", pattern = reg) %>%
filter(!word %in% stop_words$word, str_detect(word, "[a-z]")) %.>%
plyr::count(.$word) %>%
arrange(desc(freq)) %>%
top_n(200) %>%
rename("word" = x)
head(clinton)
library(wordcloud)
windows()
clinton <- corpus %>%
dplyr::filter(handle == "HillaryClinton") %>%
dplyr::select(text) %>%
dplyr::mutate(text = str_replace_all(text, "https://t.co/[A-Za-z\\d]+|&amp;", "")) %>%
tidytext::unnest_tokens(word, text, token = "regex", pattern = reg) %>%
filter(!word %in% stop_words$word, str_detect(word, "[a-z]")) %.>%
plyr::count(.$word) %>%
arrange(desc(freq)) %>%
top_n(300) %>%
rename("word" = x) %>%
wordcloud(., colors = brewer.pal(8, "RdBu"), scale = c(4, .1), rot.per = .2)
clinton <- corpus %>%
dplyr::filter(handle == "HillaryClinton") %>%
dplyr::select(text) %>%
dplyr::mutate(text = str_replace_all(text, "https://t.co/[A-Za-z\\d]+|&amp;", "")) %>%
tidytext::unnest_tokens(word, text, token = "regex", pattern = reg) %>%
filter(!word %in% stop_words$word, str_detect(word, "[a-z]")) %.>%
plyr::count(.$word) %>%
arrange(desc(freq)) %>%
top_n(300) %>%
rename("word" = x)
corpus %>%
dplyr::filter(handle == "HillaryClinton") %>%
dplyr::select(text) %>%
dplyr::mutate(text = str_replace_all(text, "https://t.co/[A-Za-z\\d]+|&amp;", "")) %>%
tidytext::unnest_tokens(word, text, token = "regex", pattern = reg) %>%
filter(!word %in% stop_words$word, str_detect(word, "[a-z]")) %.>%
plyr::count(.$word) %>%
arrange(desc(freq)) %>%
top_n(300) %>%
rename("word" = x) %.>%
wordcloud(word = .$word, freq = .$freq,
colors = brewer.pal(8, "RdBu"), scale = c(4, .1), rot.per = .2)
corpus %>%
dplyr::filter(handle == "HillaryClinton") %>%
dplyr::select(text) %>%
dplyr::mutate(text = str_replace_all(text, "https://t.co/[A-Za-z\\d]+|&amp;", "")) %>%
tidytext::unnest_tokens(word, text, token = "regex", pattern = reg) %>%
filter(!word %in% (stop_words$word, c(c("trump", "hillary", "donald", "trump's", "hillary's"),
str_detect(word, "[a-z]")) %.>%
plyr::count(.$word) %>%
arrange(desc(freq)) %>%
top_n(300) %>%
rename("word" = x) %.>%
wordcloud(word = .$word, freq = .$freq,
colors = brewer.pal(8, "RdBu"), scale = c(4, .1), rot.per = .2)
corpus %>%
dplyr::filter(handle == "HillaryClinton") %>%
dplyr::select(text) %>%
dplyr::mutate(text = str_replace_all(text, "https://t.co/[A-Za-z\\d]+|&amp;", "")) %>%
tidytext::unnest_tokens(word, text, token = "regex", pattern = reg) %>%
filter(!word %in% (stop_words$word, c("trump", "hillary", "donald", "trump's", "hillary's")),
str_detect(word, "[a-z]")) %.>%
plyr::count(.$word) %>%
arrange(desc(freq)) %>%
top_n(300) %>%
rename("word" = x) %.>%
wordcloud(word = .$word, freq = .$freq,
colors = brewer.pal(8, "RdBu"), scale = c(4, .1), rot.per = .2)
corpus %>%
dplyr::filter(handle == "HillaryClinton") %>%
dplyr::select(text) %>%
dplyr::mutate(text = str_replace_all(text, "https://t.co/[A-Za-z\\d]+|&amp;", "")) %>%
tidytext::unnest_tokens(word, text, token = "regex", pattern = reg) %>%
filter(!word %in% (stop_words$word,
c("trump", "hillary", "donald", "trump\\'s", "hillary\\'s")),
str_detect(word, "[a-z]")) %.>%
plyr::count(.$word) %>%
arrange(desc(freq)) %>%
top_n(300) %>%
rename("word" = x) %.>%
wordcloud(word = .$word, freq = .$freq,
colors = brewer.pal(8, "RdBu"), scale = c(4, .1), rot.per = .2)
corpus %>%
dplyr::filter(handle == "HillaryClinton") %>%
dplyr::select(text) %>%
dplyr::mutate(text = str_replace_all(text, "https://t.co/[A-Za-z\\d]+|&amp;", "")) %>%
tidytext::unnest_tokens(word, text, token = "regex", pattern = reg) %>%
filter(!word %in% (stop_words$word,
c("trump", "hillary", "donald", "trump\\'s", "hillary\\'s")),
str_detect(word, "[a-z]")) %.>%
plyr::count(.$word) %>%
arrange(desc(freq)) %>%
top_n(300) %>%
rename("word" = x)
clinton <- corpus %>%
dplyr::filter(handle == "HillaryClinton") %>%
dplyr::select(text) %>%
dplyr::mutate(text = str_replace_all(text, "https://t.co/[A-Za-z\\d]+|&amp;", "")) %>%
tidytext::unnest_tokens(word, text, token = "regex", pattern = reg) %>%
filter(!word %in% (stop_words$word,
c("trump", "hillary", "donald", "clinton")),
str_detect(word, "[a-z]")) %.>%
plyr::count(.$word) %>%
arrange(desc(freq)) %>%
top_n(300) %>%
rename("word" = x)
clinton <- corpus %>%
dplyr::filter(handle == "HillaryClinton") %>%
dplyr::select(text) %>%
dplyr::mutate(text = str_replace_all(text, "https://t.co/[A-Za-z\\d]+|&amp;", "")) %>%
tidytext::unnest_tokens(word, text, token = "regex", pattern = reg) %>%
filter(!word %in% (stop_words$word,
c("trump", "hillary", "donald", "clinton")),
str_detect(word, "[a-z]")) %.>%
plyr::count(.$word) %>%
arrange(desc(freq)) %>%
top_n(300) %>%
rename("word" = x)
clinton <- corpus %>%
dplyr::filter(handle == "HillaryClinton") %>%
dplyr::select(text) %>%
dplyr::mutate(text = str_replace_all(text, "https://t.co/[A-Za-z\\d]+|&amp;", "")) %>%
tidytext::unnest_tokens(word, text, token = "regex", pattern = reg) %>%
filter(!word %in% (stop_words$word,
c("trump", "hillary", "donald", "clinton")),
str_detect(word, "[a-z]")) %.>%
plyr::count(.$word) %>%
arrange(desc(freq)) %>%
top_n(300) %>%
rename("word" = x)
clinton <- corpus %>%
dplyr::filter(handle == "HillaryClinton") %>%
dplyr::select(text) %>%
dplyr::mutate(text = str_replace_all(text, "https://t.co/[A-Za-z\\d]+|&amp;", "")) %>%
tidytext::unnest_tokens(word, text, token = "regex", pattern = reg) %>%
filter(!word %in% c(stop_words$word,
c("trump", "hillary", "donald", "clinton", "trump's")),
str_detect(word, "[a-z]")) %.>%
plyr::count(.$word) %>%
arrange(desc(freq)) %>%
top_n(300) %>%
rename("word" = x)
View(clinton)
clinton <- corpus %>%
dplyr::filter(handle == "HillaryClinton") %>%
dplyr::select(text) %>%
dplyr::mutate(text = str_replace_all(text, "https://t.co/[A-Za-z\\d]+|&amp;", "")) %>%
tidytext::unnest_tokens(word, text, token = "regex", pattern = reg) %>%
filter(!word %in% c(stop_words$word,
c("trump", "hillary", "donald", "clinton", "trump's", "hillary's")),
str_detect(word, "[a-z]")) %.>%
plyr::count(.$word) %>%
arrange(desc(freq)) %>%
top_n(300) %>%
rename("word" = x)
View(clinton)
reg <- "([^A-Za-z\\d#@']|'(?![A-Za-z\\d#@]))"
clinton <- corpus %>%
dplyr::filter(handle == "HillaryClinton") %>%
dplyr::select(text) %>%
dplyr::mutate(text = str_replace_all(text, "https://t.co/[A-Za-z\\d]+|&amp;", "")) %>%
tidytext::unnest_tokens(word, text, token = "regex", pattern = reg) %>%
filter(!word %in% c(stop_words$word,
c("trump", "hillary", "donald", "clinton", "trump's", "hillary's")),
str_detect(word, "[a-z]")) %.>%
plyr::count(.$word) %>%
arrange(desc(freq)) %>%
top_n(300) %>%
rename("word" = x) %.>%
wordcloud(word = .$word, freq = .$freq,
colors = brewer.pal(8, "RdBu"), scale = c(4, .1), rot.per = .2)
corpus %>%
dplyr::filter(handle == "HillaryClinton") %>%
dplyr::select(text) %>%
dplyr::mutate(text = str_replace_all(text, "https://t.co/[A-Za-z\\d]+|&amp;", "")) %>%
tidytext::unnest_tokens(word, text, token = "regex", pattern = reg) %>%
filter(!word %in% c(stop_words$word,
c("trump", "hillary", "donald", "clinton", "trump's", "hillary's")),
str_detect(word, "[a-z]")) %.>%
plyr::count(.$word) %>%
arrange(desc(freq)) %>%
top_n(100) %>%
rename("word" = x) %.>%
wordcloud(word = .$word, freq = .$freq,
colors = brewer.pal(8, "RdBu"), scale = c(4, .1), rot.per = .2)
windows()
clinton <- corpus %>%
dplyr::filter(handle == "HillaryClinton") %>%
dplyr::select(text) %>%
dplyr::mutate(text = str_replace_all(text, "https://t.co/[A-Za-z\\d]+|&amp;", "")) %>%
tidytext::unnest_tokens(word, text, token = "regex", pattern = reg) %>%
filter(!word %in% c(stop_words$word,
c("trump", "hillary", "donald", "clinton", "trump's", "hillary's")),
str_detect(word, "[a-z]")) %.>%
plyr::count(.$word) %>%
arrange(desc(freq)) %>%
top_n(100) %>%
rename("word" = x)
trump <- corpus %>%
dplyr::filter(handle == "realDonaldTrump") %>%
dplyr::select(text) %>%
dplyr::mutate(text = str_replace_all(text, "https://t.co/[A-Za-z\\d]+|&amp;", "")) %>%
tidytext::unnest_tokens(word, text, token = "regex", pattern = reg) %>%
filter(!word %in% c(stop_words$word,
c("trump", "hillary", "donald", "clinton", "trump's", "hillary's")),
str_detect(word, "[a-z]")) %.>%
plyr::count(.$word) %>%
arrange(desc(freq)) %>%
top_n(100) %>%
rename("word" = x)
trump
windows()
par(mfrow = c(1, 2))
par(mfrow = c(1, 2))
wordcloud(word = clinton$word, freq = clinton$freq,
colors = brewer.pal(8, "RdBu"), scale = c(4, .1), rot.per = .2)
wordcloud(word = trump$word, freq = trump$freq,
colors = brewer.pal(8, "RdBu"), scale = c(4, .1), rot.per = .2)
reg <- "([^A-Za-z\\d#@']|'(?![A-Za-z\\d#@]))"
clinton <- corpus %>%
dplyr::filter(handle == "HillaryClinton") %>%
dplyr::select(text) %>%
dplyr::mutate(text = str_replace_all(text, "https://t.co/[A-Za-z\\d]+|&amp;", "")) %>%
tidytext::unnest_tokens(word, text, token = "regex", pattern = reg) %>%
filter(!word %in% c(stop_words$word,
c("trump", "hillary", "donald", "clinton", "trump's", "hillary's")),
str_detect(word, "[a-z]")) %.>%
plyr::count(.$word) %>%
arrange(desc(freq)) %>%
top_n(200) %>%
rename("word" = x)
trump <- corpus %>%
dplyr::filter(handle == "realDonaldTrump") %>%
dplyr::select(text) %>%
dplyr::mutate(text = str_replace_all(text, "https://t.co/[A-Za-z\\d]+|&amp;", "")) %>%
tidytext::unnest_tokens(word, text, token = "regex", pattern = reg) %>%
filter(!word %in% c(stop_words$word,
c("trump", "hillary", "donald", "clinton", "trump's", "hillary's")),
str_detect(word, "[a-z]")) %.>%
plyr::count(.$word) %>%
arrange(desc(freq)) %>%
top_n(200) %>%
rename("word" = x)
set.seed(1234)
par(mfrow = c(1, 2))
wordcloud(word = clinton$word, freq = clinton$freq,
colors = brewer.pal(8, "RdBu"), scale = c(4, .1), rot.per = .2)
wordcloud(word = trump$word, freq = trump$freq,
colors = brewer.pal(8, "RdBu"), scale = c(4, .1), rot.per = .2)
set.seed(1117)
par(mfrow = c(1, 2))
wordcloud(word = clinton$word, freq = clinton$freq,
colors = brewer.pal(8, "RdBu"), scale = c(4, .1), rot.per = .2)
wordcloud(word = trump$word, freq = trump$freq,
colors = brewer.pal(8, "RdBu"), scale = c(4, .1), rot.per = .2)
set.seed(2016)
par(mfrow = c(1, 2))
wordcloud(word = clinton$word, freq = clinton$freq,
colors = brewer.pal(8, "RdBu"), scale = c(4, .1), rot.per = .2)
wordcloud(word = trump$word, freq = trump$freq,
colors = brewer.pal(8, "RdBu"), scale = c(4, .1), rot.per = .2)
set.seed(20161108)
par(mfrow = c(1, 2))
wordcloud(word = clinton$word, freq = clinton$freq,
colors = brewer.pal(8, "RdBu"), scale = c(4, .1), rot.per = .2)
wordcloud(word = trump$word, freq = trump$freq,
colors = brewer.pal(8, "RdBu"), scale = c(4, .1), rot.per = .2)
rm(list = ls())
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(sqldf)
library(wrapr)
library(readit)
library(tidytext)
library(tm)
library(e1071)
library(ROCR)
library(wordcloud)
# load data
# the tweets were manually collected from various sources, i.e. Kaggle, Github
corpus <- readit("../data/tweets.txt")
# %>% mutate(handle = dplyr::case_when(handle == 'HillaryClinton' ~ 0, TRUE ~ 1))
dim(corpus)
# check counts
table(corpus$handle)
### cleanup steps ###
# first put the corpus in tm format
corpusClean <- Corpus(VectorSource(corpus$text))
# clean up
corpusClean <- tm_map(corpusClean, content_transformer(tolower))
corpusClean <- tm_map(corpusClean, removeWords, stopwords())
corpusClean <- tm_map(corpusClean, stripWhitespace)
corpusClean <- tm_map(corpusClean, removePunctuation)
# convert it into a dtm (row per document, column per word)
dtm <- DocumentTermMatrix(corpusClean)
inspect(dtm)
# set index
set.seed(1234)
index <- sample(1:dim(corpus)[1], .7 * dim(corpus)[1])  # split it by 70% vs 30%
# split original corpus into train and test sets, each set contains the "handle" (dependent variable)
train_step_1 <- corpus[index, ]
test_step_1 <- corpus[-index, ]
# set frequency filter, i.e. only include words that appear f or more times in the whole corpus
f = 5
features <- findFreqTerms(dtm, f)
# split clean corpus
# turn it into dtm
# set features filter - this is a very important step; if not applied, some sparse terms built in the training set would not be found in the test set and the model will fail
# convert value into 1 or 0 for classification, e.g. Naive Bayes
# turn it into a data.frame
system.time(
train_step_2 <- corpusClean[index] %>%
DocumentTermMatrix(., list(global = c(2, Inf), dictionary = features)) %>%
apply(MARGIN = 2, function(x) x <- ifelse(x >0, 1, 0)) %>%
as.data.frame
)
system.time(
test_step_2 <- corpusClean[-index] %>%
DocumentTermMatrix(., list(global = c(2, Inf), dictionary = features)) %>%
apply(MARGIN = 2, function(x) x <- ifelse(x >0, 1, 0)) %>%
as.data.frame
)
# final step, put step 1 and 2 together
train <- cbind(handle = factor(train_step_1$handle), train_step_2) %>% as.data.frame
test <- cbind(handle = factor(test_step_1$handle), test_step_2) %>% as.data.frame
##########################
# Support-Vector-Machine #
##########################
system.time( fit_svm <- e1071::svm(handle~., train) )
# summary(fit_svm)
# fit a prediction
system.time( fit_svm_pred <- predict(fit_svm, na.omit(test)) )
# classification outcome
ftable(fit_svm_pred, test$handle,
dnn = c("Predicted", "Actual")) -> table_svm
table_svm
table_svm %>% prop.table(., margin = 2)*100 -> accuracy_svm
round(accuracy_svm, 1)
tp_svm <- 756
fp_svm <- 156
tn_svm <- 377
fn_svm <- 11
sensitivity_svm <- tp_svm / (tp_svm + fn_svm)  # equivalent to recall
specificity_svm <- tn_svm / (tn_svm + fp_svm)
precision_svm <- tp_svm / (tp_svm + fp_svm)
total_accuracy_svm <- (tp_svm + tn_svm) / sum(tp_svm, fp_svm, tn_svm, fn_svm)
system.time( fit_nb <- e1071::naiveBayes(handle ~., train) )
# summary(fit_nb)
# fit a prediction
system.time( fit_nb_pred <- predict(fit_nb, na.omit(test)) )
# classification outcome
ftable(fit_nb_pred, test$handle,
dnn = c("Predicted", "Actual")) -> table_nb
table_nb
table_nb %>% prop.table(., margin = 2)*100 -> accuracy_nb
round(accuracy_nb, 1)
tp_nb <- 195
fp_nb <- 49
tn_nb <- 484
fn_nb <- 572
sensitivity_nb <- tp_nb / (tp_nb + fn_nb)  # equivalent to recall
specificity_nb <- tn_nb / (tn_nb + fp_nb)
precision_nb <- tp_nb / (tp_nb + fp_nb)
total_accuracy_nb <- (tp_nb + tn_nb) / sum(tp_nb, fp_nb, tn_nb, fn_nb)
system.time( fit_lr <- glm(handle ~., train, family = "binomial") )
# summary(fit_lr)
# fit a prediction
system.time( fit_lr_pred <- predict(fit_lr, newdata = test[, -1], type = "response") )
# classification outcome
ftable(test$handle, fit_lr_pred > 0.5) -> table_lr
table_lr
table_lr %>% prop.table(., margin = 1)*100 -> accuracy_lr
round(accuracy_lr, 1)
tp_lr <- 175
fp_lr <- 151
tn_lr <- 382
fn_lr <- 592
sensitivity_lr <- tp_lr / (tp_lr + fn_lr)  # equivalent to recall
specificity_lr <- tn_lr / (tn_lr + fp_lr)
precision_lr <- tp_lr / (tp_lr + fp_lr)
total_accuracy_lr <- (tp_lr + tn_lr) / sum(tp_lr, fp_lr, tn_lr, fn_lr)
data.frame(classification = c("Support Vector Machine", "Naive Bayes", "Logistic Regression"),
sensitivity = round(c(sensitivity_svm, sensitivity_nb, sensitivity_lr), 3),
specificity = round(c(specificity_svm, specificity_nb, specificity_lr), 3),
precision = round(c(precision_svm, precision_nb, precision_lr), 3),
total_accuracy = round(c(total_accuracy_svm, total_accuracy_nb, total_accuracy_lr), 3),
stringsAsFactors = F) %>%
tidyr::gather(., "measure", "outcome", -classification) %>%
tidyr::spread(classification, outcome)
# Let's summarize what they said by using word clouds (top 200 words by frequency)
reg <- "([^A-Za-z\\d#@']|'(?![A-Za-z\\d#@]))"
clinton <- corpus %>%
dplyr::filter(handle == "HillaryClinton") %>%
dplyr::select(text) %>%
dplyr::mutate(text = str_replace_all(text, "https://t.co/[A-Za-z\\d]+|&amp;", "")) %>%
tidytext::unnest_tokens(word, text, token = "regex", pattern = reg) %>%
filter(!word %in% c(stop_words$word,
c("trump", "hillary", "donald", "clinton", "trump's", "hillary's")),
str_detect(word, "[a-z]")) %.>%
plyr::count(.$word) %>%
arrange(desc(freq)) %>%
top_n(200) %>%
rename("word" = x)
# clinton
trump <- corpus %>%
dplyr::filter(handle == "realDonaldTrump") %>%
dplyr::select(text) %>%
dplyr::mutate(text = str_replace_all(text, "https://t.co/[A-Za-z\\d]+|&amp;", "")) %>%
tidytext::unnest_tokens(word, text, token = "regex", pattern = reg) %>%
filter(!word %in% c(stop_words$word,
c("trump", "hillary", "donald", "clinton", "trump's", "hillary's")),
str_detect(word, "[a-z]")) %.>%
plyr::count(.$word) %>%
arrange(desc(freq)) %>%
top_n(200) %>%
rename("word" = x)
# trump
set.seed(20161108)
par(mfrow = c(1, 2))
wordcloud(word = clinton$word, freq = clinton$freq,
colors = brewer.pal(8, "RdBu"), scale = c(4, .1), rot.per = .2)
wordcloud(word = trump$word, freq = trump$freq,
colors = brewer.pal(8, "RdBu"), scale = c(4, .1), rot.per = .2)
quit()
