---
title: "Group 4 Assignment 4"
author: "Ajay Arora, Romerl Elizes, Jimmy Ng, Joshua Registe, Adam Rich"
date: "April 16, 2021"
output:
  rmdformats::readthedown:
    self_contained: yes
    thumbnails: yes
    lightbox: yes
    gallery: no
    highlight: tango
    code_folding: hide
editor_options: 
  chunk_output_type: console
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  echo = TRUE, 
  warning = FALSE, 
  message = FALSE,
  cache = TRUE)

packages <- c(
    'tidyverse', 
    'corrplot', 
    'palmerpenguins',
    'class',
    'kableExtra',
    'naniar',
    'DataExplorer',
    'caret',
    'tidymodels',
    'rsample',
    'themis',
    'randomForest',
    'car',
    'xgboost',
    'broom',
    "readr",
    "MachineShop",
    "readit",
    "janitor",
    "recipes",
    "factoextra",
    "wrapr",
    "gridExtra"
)

for (pkg in packages) {
  suppressPackageStartupMessages(suppressWarnings(
    library(
      pkg, character.only = TRUE, 
      warn.conflicts = FALSE, quietly = TRUE)
  ))
}

# A ggplot2 function
defaulttheme <- theme(
  panel.background = element_blank(),
  panel.border = element_rect(color = "black", fill = NA))
```

# Data Exploration

The first thing we will do for this assignment is explore the dataset provided. This dataset is a mental health dataset from a real-life research project. All identifying information of the dataset was removed and the data dictionary provides an explanation of each of the variables. The data contains 54 variables with 175 observations per variable. Every variable in the dataset is imported in as numeric features with the exception of `Initial` which will not be used as part of any exploration or analysis.

```{r}

ADHD_data<-read_csv("ADHD_data.csv")
names(ADHD_data)<-str_replace_all(names(ADHD_data)," +","_")
names(ADHD_data)<-str_replace_all(names(ADHD_data),"-","_")

psych::describe(ADHD_data) %>% 
  kable() %>% 
  kable_styling() %>% 
  scroll_box(height = "200px")

dfMeta <- data.frame(missing = colSums(is.na(ADHD_data)),
                     dtype = sapply(ADHD_data, class),
                     num_of_unique = sapply(ADHD_data, function(x) length(unique(x)))) %>%
    tibble::rownames_to_column() %>%
    dplyr::select(columns = rowname, dtype, missing, num_of_unique)

dfMeta %>% 
  kable() %>%
  kable_styling() %>%
  scroll_box(height = "200px")
```

After observing the data's structure, the next step was to plot the data to assess how this data is portrayed and what observations we can make about the distributions involved in the dataset. Based on the histrograms plotted below, we can note that there are many observations although numeric, behave as categorical features and this will need to be assessed when performing the kmeans clustering analysis. There does not seem to be any clear distinguishable outliers however there does seem to be some features that experience low variance such `Stimulants` where majority of the recorded observations are 0.

```{r}
plot_histogram(ADHD_data)+theme_bw()

```

Assessing correlations will be important for the models to come because Kmeans, and PCA are particularly sensitive to multi-colinearity and in order to reduce noise and complexity of the dataset prior to modeling, multi-colinearity (if it exists) should be addressed. The dataset was assessed for pairwise spearman correlations which measures data for it's monotonicity providing a regression coefficient that defines both linear and non-linear trends. the table below shows we do exhibit correlations particularly amongst features that are directly related such as the `ADHDQ#` to the `ADHD_Total`. Highest measured spearman rank was around 0.79.

```{r}
NumericADHD<-ADHD_data %>% 
  select(-Initial)

ADHD_cors<-
  bind_rows(
    #pearson correlation of numeric features
    NumericADHD %>% 
      cor(method = "pearson", use = "pairwise.complete.obs") %>% as.data.frame() %>% 
      rownames_to_column(var = "x") %>% 
      pivot_longer(cols = -x, names_to = "y", values_to = "correlation") %>% 
      mutate(cor_type = "pearson"),
    #spearman (monotonic) correlations of numeric features
    NumericADHD %>% 
      cor(method = "spearman", use = "pairwise.complete.obs") %>% as.data.frame() %>% 
      rownames_to_column(var = "x") %>% 
      pivot_longer(cols = -x, names_to = "y", values_to = "correlation") %>% 
      mutate(cor_type = "spearman")
  )

ADHD_cors %>% 
  filter(!(x ==y)) %>% 
  filter(cor_type=="spearman") %>% 
  distinct(correlation,.keep_all = T) %>% 
  arrange(-correlation) %>% #top_n(10, correlation) %>% 
  distinct(x, .keep_all = T) %>% 
  kable() %>% 
  kable_styling(position = "center") %>% 
  scroll_box(height = "200px")

  
```

Next, we want to assess any missing data within the dataset. The models presented here are also susceptible to missing data and this must be treated accordingly. The figure below displays a plot of missing data by percentage of observations in the dataset. It is clear that the feature `Psych_meds.` has a significant amount of features that are inappropriate to impute or include and thus this feature will be removed from the dataset prior to any models.

```{r}

ADHD_data %>%  naniar::miss_var_summary() %>% 
  slice_max(n_miss, n = 10) %>% 
  ggplot(aes( x = fct_reorder(variable, n_miss), y = pct_miss))+
  coord_flip()+
  geom_col(alpha = 0.5, fill = "skyblue3", color = "darkblue")+
  defaulttheme+
  labs(title = "Top Features with Missing Data",
       y = "Percent Missing",
       x = "Feature")



```

# Question 1 K-Means Clustering

K-means clustering is a method used of vector quantization. It is an unsupervised learning algorithm where the predicted outcomes are unlabeled and unknown. The goal of these algorithms help to reduce data dimensionality or cluster groups based on unlabeled data for purposes such as targeted marketing. The groups are defined by k clusters where each observation will belong to a cluster with the nearest mean or the cluster centroid, as such, these metrics are based on the euclidian distances.

## Data preparation for Q1

For the Data preparation for K-means Clustering algorithm the following data preparation steps were made and the reasons are provided below

-   **Remove `Initial`**: Character value that identifies patient with which will provide no bearing on the model output

-   **Removal `totalized features`**: Remove features that are summations of other features to reduce redundancy and noise in the dataset

-   **Removal `Psych_meds.`**: Removed due to large amount of missing data (\>60% observations missing)

-   **Imputation of missing data with KNN:** the remaining data was imputed with K-nearestneighbors (KNN) as a way to fill in missing gaps. alternative methods include median, mean, or bag imputations but it was determed that KNN provides the best results with minimal effect on computational effort.

-   **Numeric to Factor Conversions**: Several variables were with low distribution were converted into classes based on their categorical nature displayed in the histograms presented in the `Data Exploration` section. This conversion was made to all variables in the dataset except for `Age`, `ADHD.Total`, and `MD.Total`.

-   **Dummifying Variables**: Newly transformed categorical variables were binarized into 0/1. This is particularly important for k-means because k-means will not be able to distinguish the eucliiand distances properly between classes that span more than 2 categories. For example a feature with categories 1,2,3 will not properly be computed in k-means because 1,3 will measure greater distances than 1,2, thus binarizing each of these categories such that for example 3 would be its own column with 0/1 for presence/absence is absolutely necessary.

-   **Normalization**: Again, due to the euclidian nature of k-means clustering, features need to be normalized such that the distances they are centered and scaled the mean is 0 and the Stdev is 1, this scales all the data to allow kmeans to appropriately place centroids and observations at appropriate distances.

-   **Colinearity test**: Colinearity was tested and it was determined that there was not sufficient colinearity between any variables such that they needed to be removed for this reason alone.

-   **Removed low-variance features**: Removing any extremely low-variance data that will not provide useful data to the model and will only contribute to noise. At first glance, From `Data Exploration` section `Stimulants` seems like a low-variance variable with majority of categories recorded at 0. This will be confirmed statistically with tidymodels. Based on the model adjustment below, there were many features that were too sparse to provide valuable information to the model that including but not limited to: `Race_X3`, `Race_X6`, `ADHD_Q5_X5`, `Alcohol_X0.6` and more. The total amount of features used in model after removing sparse parameters went from 238 to 147 The model recipe is shown below.


```{r}

Q1_ADHD<-ADHD_data %>% recipe(~.) %>% 
  step_rm(Initial, Psych_meds.) %>% 
  step_rm(contains("total")) %>% 
  step_impute_knn(all_predictors()) %>% 
  step_mutate_at(-Age, fn = ~ as.factor(.)) %>% 
  step_dummy(all_nominal(), one_hot = T) %>% 
  step_normalize(all_predictors()) %>%
  step_nzv(all_predictors()) %>% 
  step_corr(all_predictors()) %>% 
  prep() #%>% 

Q1_ADHD 


```

After applying all those data transformations to the ADHD data, the following table was produced with 147 variables and 175 observations of those variables.

```{r}

Q1_ADHD<- Q1_ADHD %>%  bake(ADHD_data)

Q1_ADHD %>% kable() %>% 
  scroll_box(height = "200px", width = "800px")

```

We can now begin to explore our k-means modeling. K-means clustering will use the data and group them into "k" clusters. These clusters are undefined in the model hence the term unsupervised learning. In order to determine the proper k value, there are several tools we can deploy, one common tool is called the *Elbow method* which measures the sum of squared distances for stabilized k values. Another tool is the *Silhouette method* which provides a measure of how close each point in one cluster is to points in a neighboring cluster to determine optimal k. And lastly the *Gap Statistics method* which measures biggest jumps from within clusters to determine optimal k's.  These methods are tested along various k's iteratively to see where the elbow or shoulder of curve experiences an inflection point in which the increase/decrease in errors is no longer substantial. Based on the figure below, the optimal k seems looks to be around 2-4.



```{r, width = 5, height=6, fig.align='center'}
Clustering<-function(df, algorithm, method = "k-means"){
  optimalk_theme<-theme(panel.background = element_blank(),
                        panel.grid = element_blank(),
                        panel.grid.major.x = element_line(color = "grey90", linetype = 2))
  
  p1<-factoextra::fviz_nbclust(df, algorithm, method = "wss")+
    labs(x = "Centers (k)",
         y = "SSE Width",
         title = paste0(method," - Elbow Method for Determining Optimal k")) +
    optimalk_theme
  
  p2<-factoextra::fviz_nbclust(df, algorithm, method = "silhouette")+
    labs(x = "Centers (k)",
         y = "Silhouette Width",
         title = paste0(method," - silhouette Method for Determining Optimal k")) +
    optimalk_theme
  
    p3<-factoextra::fviz_nbclust(df, algorithm, method = "gap_stat")+
      labs(x = "Centers (k)",
           title = paste0(method," - Gap Statistic for Determining Optimal k"))
  
  grid.arrange(p1,p2,p3, ncol = 1)
}
Clustering(Q1_ADHD,kmeans, "K-means")

```


After looking at our expected appropriate k-values, a visualization test will be used to see what these clusters truly look like. Below shows several plots of k values from 0-9 and assessing how well the clusters are defined in each. In the figure below, we notice that the k-values of 2, 3, and 4 do well to separate our unknown features and can be used as an appropriately selected k-value.

```{r, fig.width=10, fig.height=10}

set.seed(2)
for(i in 1:9){
assign(paste0("A",i),
       factoextra::fviz_cluster(kmeans(Q1_ADHD, centers = i, iter.max = 100),Q1_ADHD,
                                ggtheme = defaulttheme,
                                geom = "point",
                                main = paste("Cluster Plot with K = ", i)))

}


gridExtra::grid.arrange(A1,A2,A3,A4,A5,A6,A7,A8,A9, ncol = 3)


```

# Question 2 Principal Component Analysis (PCA)

PCA uses orthogonal projection of highly correlated variables to a set of values of linearly uncorrelated variables called principal components. The number of principal components is less than or equal to the number of original variables, i.e. min(n - 1, p). This linear transformation is defined in such a way that the first principal component has the largest possible variance. It accounts for as much of the variability in the data as possible by considering highly correlated features. Each succeeding component in turn has the highest variance using the features that are less correlated with the first principal component and that are orthogonal to the preceding component. Features that are strongly correlated with each other are more suitable for PCA than those loosely related. Below **corrplot** using the Spearman correlation for the categorical variables demonstrate that the features within ADHD set are more strongly correlated than the MD set. In addition, there are too many missing values for the individual substance misuse, and therefore PCA is not performed on this set. For question 2, we conduct PCA on both ADHD and MD, but ADHD is demonstrated to be more suitable for the task.

## data prep for Q2 {.tabset .tabset-fade .tabset-pills}

```{r}
df = readit("ADHD_data.csv")
df = janitor::clean_names(df)
```

### Spearman correlation

```{r, fig.width=10, fig.height=10}
# Spearman correlation on ADHD, MD
dfDisorder <- df %>% 
    dplyr::select(matches("age|sex|race|adhd|md"))

cor(dfDisorder, method = "spearman") %>% corrplot(type = "upper")
```

### impute missing, scale and dummify variables

Applying the same methodology to impute missing and dummify variables like the above process in Q1; however, we decide to treat "education" as a numeric variable instead of categorical. We also remove "initial" (just an identifier) and "psych_meds" (too many missing values).

```{r}
numeric_var = c("age", "adhd_total", "md_total", "education")

dfReady <-df %>% recipe(~.) %>% 
    step_rm(initial, psych_meds) %>% 
    step_impute_knn(all_predictors()) %>% 
    step_mutate_at(-all_of(numeric_var), fn = ~ as.factor(.)) %>% 
    step_dummy(all_nominal(), one_hot = TRUE) %>% 
    step_normalize(all_predictors()) %>%
    step_nzv(all_predictors()) %>%
    step_corr(all_predictors()) %>%
    prep()

dfReady <- dfReady %>% 
    bake(df)

dfReady <- dfReady %>%
    dplyr::mutate_at(all_of(numeric_var), function(x) scale(x) %>% as.numeric)

print(paste0("After transformation, there are ", nrow(dfReady), " rows and ", ncol(dfReady), " columns."))

# check missing again
if(!any(colSums(is.na(dfReady)) >0)){print("No more missing variable.")}
```
## PCA - ADHD {.tabset .tabset-fade .tabset-pills}

### summary
Our components are sorted from largest to smallest with regard to their standard deviation. **Standard deviation** is simply the eigenvalues in our case since the data has been centered and scaled (standardized). **Proportion of Variance** is the amount of variance that the component accounts for in the data, i.e. PC1 accounts for roughly 14% of total variance in the data set. **Cumulative Proportion** is the accumulated amount of explained variance, e.g. the first 10 components account for roughly 47% of total variance in the data. Since an eigenvalues <1 would mean that the component actually explains less than a single explanatory variable, therefore we would like to discard those.

```{r}
# pca - ADHD
adhdPCA <- prcomp(dfReady %>% dplyr::select(contains("adhd_")))

sd <- adhdPCA$sdev
loadings <- adhdPCA$rotation
rownames(loadings) <- colnames(dfReady %>% dplyr::select(contains("adhd_")))
scores <- adhdPCA$x

summary(adhdPCA)
```
### scree, cumulative variance plots
A better way to understand and then choose the appropriate number of principal components is to visualize the results using scree and cumulative variance plot. The scree plot ("elbow" method) would indicate that the first 4 components are the most important ones, although they merely captured roughly 32% of the variance in the data. The first 29 components have the standard deviation above 1 and together they captured 77% of the variance. The number of dimensions is significantly reduced from 91 to 29 in this case.

```{r, fig.width=10, fig.height=10}
par(mfrow = c(1, 2))

# scree plot
screeplot(adhdPCA, type = "l", npcs = 30, main = "Screeplot of the first 30 PCs")
abline(h = 1, col = "red", lty = 5)
legend("topright", legend = c("Eigenvalue = 1"), col = c("red"), lty = 5, cex = 0.6)

# cumulative variance plot
cumpro <- cumsum(adhdPCA$sdev^2 / sum(adhdPCA$sdev^2))
plot(cumpro[0:30], xlab = "PC #", ylab = "Amount of explained variance", main = "Cumulative variance plot")
```

### loadings

**loadings** reveal how our variables contribute to each principal component. Positive loadings indicate a variable and a principal component are positively correlated: an increase in one results in an increase in the other. Negative loadings indicate a negative correlation. Large (either positive or negative) loadings indicate that a variable has a strong effect on that principal component. Because the sum of the squares of all loadings for an individual principal component must sum to one, we can calculate what the loadings would be if all variables contributed equally to that principal component. Any variable that has a larger loading than this value contributes more than one variableâ€™s worth of information and would be regarded as an important contributor to that principal component.

**adhd_total** is the most important contributor to the first principal component. PC1 is made up by majority of "_X4" variables, meaning the response of "very often" from the ADHD questions; whereas PC2 is made up by majority of "_X0" variables, meaning the response of "never" from the ADHD questions. As expected, PCA successfully extract components from features that are not strongly associated to each other. 

```{r}
cut_off <- sqrt(1/ncol(dfReady %>% dplyr::select(contains("adhd_")))) 

loadingsDf <- loadings %>% 
    as.data.frame() %>% 
    tibble::rownames_to_column() %>%
    dplyr::select(variables = rowname, everything())

pc1_important <- loadingsDf %>% 
    dplyr::filter(abs(PC1) > cut_off) %>%
    dplyr::select(variables, PC1) %>%
    arrange(desc(abs(PC1)))

pc1_important %>% 
  kable() %>%
  scroll_box()

pc2_important <- loadingsDf %>% 
    dplyr::filter(abs(PC2) > cut_off) %>%
    dplyr::select(variables, PC2) %>%
    arrange(desc(abs(PC2)))

pc2_important %>%
  kable() %>%
  scroll_box()
```

### biplot
The biplot displays the individuals and variables in the same plot featuring the first two principal components. It seems to suggest that the individuals can be visually clustered into 4 groups based on their responses to the ADHD questions.

```{r, fig.width=10, fig.height=10}
biplot(adhdPCA, scale = 0, cex = 0.5)
```

## PCA - MD {.tabset .tabset-fade .tabset-pills}

### summary
We repeat a similar exercise for the MD questions. The summary suggested that the first four components (standard deviation above 1) would account for 61% of the total variance in the data. 

```{r}
mdPCA <- prcomp(dfReady %>% dplyr::select(contains("md_")))
summary(mdPCA)

sd2 <- mdPCA$sdev
loadings2 <- mdPCA$rotation
rownames(loadings2) <- colnames(dfReady %>% dplyr::select(contains("md_")))
scores2 <- mdPCA$x
```

### scree, cumulative variance plots

```{r, fig.width=10, fig.height=10}
par(mfrow = c(1, 2))

# scree plot
screeplot(mdPCA, type = "l", npcs = 10, main = "Screeplot of the first 10 PCs")
abline(h = 1, col = "red", lty = 5)
legend("topright", legend = c("Eigenvalue = 1"), col = c("red"), lty = 5, cex = 0.6)

# cumulative variance plot
cumpro <- cumsum(mdPCA$sdev^2 / sum(mdPCA$sdev^2))
plot(cumpro[0:10], xlab = "PC #", ylab = "Amount of explained variance", main = "Cumulative variance plot")
```


### loadings

Similar to ADHD set, **md_total** is the most important contributor to the first principal component.  

```{r}
cut_off2 <- sqrt(1/ncol(dfReady %>% dplyr::select(contains("md_")))) 

loadingsDf2 <- loadings2 %>% 
    as.data.frame() %>% 
    tibble::rownames_to_column() %>%
    dplyr::select(variables = rowname, everything())

pc1_important2 <- loadingsDf2 %>% 
    dplyr::filter(abs(PC1) > cut_off2) %>%
    dplyr::select(variables, PC1) %>%
    arrange(desc(abs(PC1)))

pc1_important2 %>% kable() %>% scroll_box()

pc2_important2 <- loadingsDf2 %>% 
    dplyr::filter(abs(PC2) > cut_off2) %>%
    dplyr::select(variables, PC2) %>%
    arrange(desc(abs(PC2)))

pc2_important2 %>% kable() %>% scroll_box()
```

### biplot
The biplot does not suggest that the individuals can be as clearly defined into 4 groups as the ADHD set. 

```{r, fig.width=10, fig.height=10}
biplot(mdPCA, scale = 0, cex = 0.5)
```

# Question 3 Support Vector Machine (SVM)

We will apply the SVM to classify attempted suicide. We will conduct linear, non-linear (radial basis kernel, polynomial basis kernel) SVM. Since there's an imbalance distribution for the target variable (approximately 30% attempted suicide), we will oversample the minority class by 50% (replacement = TRUE) in order to see if that can boost up the model performance. Finally, we will apply PCA and then conduct the SVM on the subset (reduced dimensions from original data) to see if there's any improvement. Since there are 13 missing values from the suicide variable, we will remove those individuals and look at the complete set only.

## data prep
We will transform the target variable into factor before passing it to the SVM algorithm. We will create a different train set by oversampling the minority class (attempted suicide equal to "Yes") for comparison. Every model/confusion matrix denoted to "2" is from the oversampling train set.

```{r}
# let's remove the missing suicide data and look at the complete data set
dfReady2 <- dfReady[!is.na(df$suicide), ]

# baseline distribution
print("baseline distribution for target variable from entire data set:")
table(dfReady2 %>% 
          dplyr::mutate(suicide_X1 = dplyr::case_when(suicide_X1 >0 ~1, TRUE ~0) %>% 
                          factor(., levels = c("1", "0"), labels = c("Yes", "No"))) %>% 
          dplyr::select(suicide_X1))

# split data
set.seed(1234)
index <- floor(sample(1:nrow(dfReady2), nrow(dfReady2) * .75, replace = FALSE))
trainSet <- dfReady2[index, ] %>% dplyr::mutate(suicide_X1 = dplyr::case_when(suicide_X1 >0 ~1, TRUE ~0) %>% 
                                                   factor(., levels = c("1", "0"), labels = c("Yes", "No")))
testSet <- dfReady2[-index, ] %>% dplyr::mutate(suicide_X1 = dplyr::case_when(suicide_X1 >0 ~1, TRUE ~0) %>% 
                                                   factor(., levels = c("1", "0"), labels = c("Yes", "No")))

# oversampling the minority class
set.seed(1234)
oversampling_by = 1.5
oversample_index = sample(1:nrow(trainSet %>% dplyr::filter(suicide_X1 == "Yes")),
                          floor(nrow(trainSet %>% dplyr::filter(suicide_X1 == "Yes")) * oversampling_by), 
                          replace = TRUE)

trainSet2 = dplyr::bind_rows(
    trainSet %>% dplyr::filter(suicide_X1 == "Yes") %>% 
        .[oversample_index, ],
    trainSet %>% dplyr::filter(suicide_X1 == "No")
    )

# trainSetControl
trainSet.control <- caret::trainControl(method = "repeatedcv",
                                        number = 10,
                                        summaryFunction = twoClassSummary,
                                        classProb = TRUE)
#trainSet.control <- caret::trainControl(method = "cv", number = 5, savePredictions = "final", classProbs = TRUE)

# distribution from train, test sets
print("distribution of target variable from train set:")
table(trainSet$suicide_X1)

# distribution from train, test sets
print("distribution of target variable from oversample train set:")
table(trainSet2$suicide_X1)

print("distribution of target variable from test set:")
table(testSet$suicide_X1)
```
## SVM linear {.tabset .tabset-fade .tabset-pills}
By default, **caret** builds the SVM linear classifier using C = 1. This tuning parameter C, also known as **Cost**, determines the possible misclassification. Essentially, it imposes a penalty to the model for making an error, i.e. the higher value of C, the less likely it is that the SVM algorithm will misclassify a target.

### linear
```{r}
set.seed(12)
svm_lin <- caret::train(suicide_X1 ~., 
                        data = trainSet, 
                        method = "svmLinear", 
                        trControl = trainSet.control,
                        tuneGrid = expand.grid(C = seq(0.1, 2, length = 20)),
                        metric = "ROC")

svm_lin2 <- caret::train(suicide_X1 ~., 
                        data = trainSet2, 
                        method = "svmLinear", 
                        trControl = trainSet.control,
                        tuneGrid = expand.grid(C = seq(0.1, 2, length = 20)),
                        metric = "ROC")

# overview of the model
print("regular train set:")
svm_lin

print("oversample train set:")
svm_lin2
```

### confusion matrix: linear
```{r}
# testSet
cm_svm_lin = caret::confusionMatrix(predict(svm_lin, testSet, type = "raw"), testSet$suicide_X1)
cm_svm_lin2 = caret::confusionMatrix(predict(svm_lin2, testSet, type = "raw"), testSet$suicide_X1)

print("regular train set:")
cm_svm_lin

print("oversample train set:")
cm_svm_lin2
```

## SVM non-linear {.tabset .tabset-fade .tabset-pills}
To build a non-linear SVM classifier, we can use either radial kernel or polynomial kernel function. The caret package automatically chooses the optimal values for the model tuning parameters based on values that maximize the model accuracy.

### non-linear: radial kernel
```{r}
set.seed(12)
svm_k <- caret::train(suicide_X1 ~., 
                      data = trainSet, 
                      method = "svmRadial", 
                      trControl = trainSet.control,
                      # tuneGrid = expand.grid(sigma = 2^c(-25, -20, -15,-10, -5, 0), 
                      #                        C = 2^c(0:5))
                      tuneLength = 10,
                      metric = "ROC")

svm_k2 <- caret::train(suicide_X1 ~., 
                      data = trainSet2, 
                      method = "svmRadial", 
                      trControl = trainSet.control,
                      # tuneGrid = expand.grid(sigma = 2^c(-25, -20, -15,-10, -5, 0), 
                      #                        C = 2^c(0:5))
                      tuneLength = 10,
                      metric = "ROC")

# overview of the model
print("regular train set:")
svm_k

print("oversample train set:")
svm_k2
```

### confusion matrix: radial kernel

```{r}
cm_svm_k = caret::confusionMatrix(predict(svm_k, testSet, type = "raw"), testSet$suicide_X1)
cm_svm_k2 = caret::confusionMatrix(predict(svm_k2, testSet, type = "raw"), testSet$suicide_X1)

print("regular train set:")
cm_svm_k

print("oversamle train set:")
cm_svm_k2
```

### non-linear: polynomial kernel

```{r}
set.seed(12)
svm_p <- caret::train(suicide_X1 ~., 
                      data = trainSet, 
                      method = "svmPoly", 
                      trControl = trainSet.control,
                      tuneLength = 5,
                      metric = "ROC")

svm_p2 <- caret::train(suicide_X1 ~., 
                       data = trainSet2, 
                       method = "svmPoly", 
                       trControl = trainSet.control,
                       tuneLength = 5,
                       metric = "ROC")

# overview of the model
print("regular train set:")
svm_p

print("oversample train set:")
svm_p2
```

### confusion matrix: polynomial kernel

```{r}
cm_svm_p = caret::confusionMatrix(predict(svm_p, testSet, type = "raw"), testSet$suicide_X1)
cm_svm_p2 = caret::confusionMatrix(predict(svm_p2, testSet, type = "raw"), testSet$suicide_X1)

print("regular train set:")
cm_svm_p

print("oversample train set:")
cm_svm_p2
```

## SVM result (without PCA) 
First of all, the result of "oversample" strategy (denoted by "2") seems to work well only with linear method and in the case of polynomial kernel (where it brings precision to 100% but lower sensitivity to 8%). Second, the best performing model is the **SVM linear** model based on f1 score. Next, we will rebuild the SVM models without the "oversample" strategy and with the help of PCA to see if there is any improvement.

```{r}
cmResultList = list(cm_svm_lin, cm_svm_lin2, cm_svm_k, cm_svm_k2, cm_svm_p, cm_svm_p2)
names(cmResultList) <- wrapr::qc(cm_svm_lin, cm_svm_lin2, cm_svm_k, cm_svm_k2, cm_svm_p, cm_svm_p2)

cmResultDf = lapply(1:length(cmResultList), function(x) broom::tidy(cmResultList[[x]]) %>%
                        dplyr::mutate(type = names(cmResultList)[x]) %>%
                        dplyr::filter(term != "mcnemar") %>%
                        dplyr::select(type, term, estimate)) %>%
    dplyr::bind_rows() %>%
    tidyr::spread(type, estimate) %>%
    dplyr::select(term, cm_svm_lin, cm_svm_lin2, cm_svm_k, cm_svm_k2, cm_svm_p, cm_svm_p2)

cmResultDf %>% mutate_if(is.numeric, round, digits = 2) %>% kable()
```
## SVM with PCA {.tabset .tabset-fade .tabset-pills}

### PCA
Let's apply PCA on entire data set (minus the initial, suicide and psych_meds variables). Two things to bear in mind. First, we should not combine the train and test set to obtain PCA components of entire data set at once for building a model. This would violate the assumption of generalization since test data would be part of the training set. Second, we should not perform PCA on test and train data sets separately. It's because the resultant vectors from train and test PCA will have different directions due to unequal variance. We would end up comparing data registered on different axes. The resulting vectors from train and test data should have same axes.

Below, we perform PCA on entire data using the train set only. Subsequently, we use it to "predict" or generate the scores vector using the test set. We extract the first 42 PC only - the standard deviations above 1 and together they explain more than 82% of the total variance in the entire data.

In other words, we use these 42 principal components extracted from the original data set to generate new train and test set to classify attempted suicide.

```{r}
allPCA <- prcomp(trainSet %>% dplyr::select(-suicide_X1))
summary(allPCA)

# Creating new train, test set
new_trainSet = trainSet %>% 
  dplyr::select(class = suicide_X1) %>%
  dplyr::bind_cols(., allPCA$x[, 1:42] %>% as.data.frame())

new_testSet = dplyr::bind_cols(class = testSet$suicide_X1, 
                               predict(allPCA, newdata = testSet)[, 1:42] %>% as.data.frame())

head(new_trainSet) %>% kable() %>% scroll_box()
```

### linear
```{r}
set.seed(12)
svm_lin_new <- caret::train(class ~., 
                            data = new_trainSet, 
                            method = "svmLinear", 
                            trControl = trainSet.control,
                            tuneGrid = expand.grid(C = seq(0.1, 2, length = 20)),
                            metric = "ROC")
svm_lin_new

cm_svm_lin_new = caret::confusionMatrix(predict(svm_lin_new, new_testSet, type = "raw"), new_testSet$class)
cm_svm_lin_new
```

### radial kernel
```{r}
set.seed(12)
svm_k_new <- caret::train(class ~., 
                          data = new_trainSet, 
                          method = "svmRadial", 
                          trControl = trainSet.control,
                          # tuneGrid = expand.grid(sigma = 2^c(-25, -20, -15,-10, -5, 0), 
                          #                    C = 2^c(0:5)),
                          tuneLength = 5,
                          metric = "ROC")
svm_k_new

cm_svm_k_new = caret::confusionMatrix(predict(svm_k_new, new_testSet, type = "raw"), new_testSet$class)
cm_svm_k_new
```

### polynomial kernel
```{r}
set.seed(12)
svm_p_new <- caret::train(class ~., 
                          data = new_trainSet,
                          method = "svmPoly", 
                          trControl = trainSet.control,
                          tuneLength = 5,
                          metric = "ROC")
svm_p_new

cm_svm_p_new = caret::confusionMatrix(predict(svm_p_new, new_testSet, type = "raw"), new_testSet$class)
cm_svm_p_new
```

## SVM result final
Unfortunately, PCA does not seem to bring significant help. The **SVM linear** is still the best method among all based on f1 score. The **SVM polynomial** with "oversampling" strategy does help to bring precision to 100% but lower sensitivity to 8%. Overall, the linear model (especially with oversampling) achieves moderate success in terms of accuracy, sensitivity, and precision. 

```{r}
cmResultList2 = list(cm_svm_lin_new, cm_svm_k_new, cm_svm_p_new)
names(cmResultList2) <- wrapr::qc(cm_svm_lin_new, cm_svm_k_new, cm_svm_p_new)

cmResultDf2 = lapply(1:length(cmResultList2), function(x) broom::tidy(cmResultList2[[x]]) %>%
                        dplyr::mutate(type = names(cmResultList2)[x]) %>%
                        dplyr::filter(term != "mcnemar") %>%
                        dplyr::select(type, term, estimate)) %>%
    dplyr::bind_rows() %>%
    tidyr::spread(type, estimate) %>%
    dplyr::select(term, cm_svm_lin_new, cm_svm_k_new, cm_svm_p_new)


finalComparison = dplyr::inner_join(cmResultDf, cmResultDf2, by = "term") %>%
    dplyr::select(term, 
                  cm_svm_lin, cm_svm_lin2, cm_svm_lin_new,
                  cm_svm_k, cm_svm_k2, cm_svm_k_new,
                  cm_svm_p, cm_svm_p2, cm_svm_p_new)

finalComparison %>% mutate_if(is.numeric, round, digits = 2) %>% kable()
```